{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SHeBTW2CYM5w"
   },
   "source": [
    "# ニューラルネットワークの実装：回帰\n",
    "\n",
    "この章では、分類と同じく教師あり学習のトピックである回帰についてもChainerで実装する方法を学びます。\n",
    "\n",
    "## 問題設定\n",
    "\n",
    "基本的には、分類と同じように実装することができます。分類の実装方法と違いを確認しながら進めていきます。\n",
    "\n",
    "今回は、重回帰分析と同様の問題設定である家賃の予測に取り掛かります。\n",
    "\n",
    "\n",
    "### 必要なモジュールの読み込み\n",
    "\n",
    "下記の３つは解析の最初に読み込んでおくと便利ですので読み込んでおきます。\n",
    "\n",
    "- Numpy\n",
    "- Pandas\n",
    "- Matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G6hO-gBfYM5z"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aqR4AdwiYM53"
   },
   "source": [
    "### データの読み込み\n",
    "\n",
    "今回は[housing.csv]('data/housing.csv')を使用して回帰を実装します。ファイルをダウンロードし、**Colab Notebooks** という黄色のフォルダにの中に`data`というフォルダを作成し、その中にアップロードします。\n",
    "アップロードできたら、データを確認します。\n",
    "\n",
    "\n",
    "#### Driveのマウント"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3438,
     "status": "ok",
     "timestamp": 1547552495198,
     "user": {
      "displayName": "西沢衛",
      "photoUrl": "",
      "userId": "12011220225445512117"
     },
     "user_tz": -540
    },
    "id": "N7AjKnX2YYRU",
    "outputId": "005ebe60-6691-418e-c905-ac3564117a9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3452,
     "status": "ok",
     "timestamp": 1547552495196,
     "user": {
      "displayName": "西沢衛",
      "photoUrl": "",
      "userId": "12011220225445512117"
     },
     "user_tz": -540
    },
    "id": "1IlYHZ4ZZr9o",
    "outputId": "8e28830f-53b9-4532-bd03-e7730a619fcb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "housing.csv  wine_class.csv\n"
     ]
    }
   ],
   "source": [
    "!ls drive/'My Drive'/'Colab Notebooks/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vL357AjBYM54"
   },
   "outputs": [],
   "source": [
    "# CSVファイルの読み込み\n",
    "df = pd.read_csv(\"drive/My Drive/Colab Notebooks/data/housing.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 142
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 625,
     "status": "ok",
     "timestamp": 1547552550091,
     "user": {
      "displayName": "西沢衛",
      "photoUrl": "",
      "userId": "12011220225445512117"
     },
     "user_tz": -540
    },
    "id": "i1_0XI0mYM56",
    "outputId": "4f009c31-74dd-41df-f1d8-d9468c2d6c03"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "      <th>x7</th>\n",
       "      <th>x8</th>\n",
       "      <th>x9</th>\n",
       "      <th>x10</th>\n",
       "      <th>x11</th>\n",
       "      <th>x12</th>\n",
       "      <th>x13</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1</td>\n",
       "      <td>296</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        x1    x2    x3  x4     x5     x6    x7      x8  x9  x10   x11     x12  \\\n",
       "0  0.00632  18.0  2.31   0  0.538  6.575  65.2  4.0900   1  296  15.3  396.90   \n",
       "1  0.02731   0.0  7.07   0  0.469  6.421  78.9  4.9671   2  242  17.8  396.90   \n",
       "2  0.02729   0.0  7.07   0  0.469  7.185  61.1  4.9671   2  242  17.8  392.83   \n",
       "\n",
       "    x13     y  \n",
       "0  4.98  24.0  \n",
       "1  9.14  21.6  \n",
       "2  4.03  34.7  "
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "azK19ocpYM5_"
   },
   "source": [
    "データを確認したところ、今回は入力変数の数が13個であることがわかります。\n",
    "\n",
    "## データの前準備\n",
    "### 入力変数と教師データ（出力変数）に切り分ける\n",
    "\n",
    "それでは、入力変数と教師データを切り分けていきます。  \n",
    "Chainerで使用することも念頭に入れて、Pandasで部分抽出した後に、`.values`でNumpyの形式に変換します。  \n",
    "また、回帰では入力変数と教師データともに実数値で扱うため、`float32`とします。`int32`ではエラーがでるので注意してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Tq27qOpjYM6A"
   },
   "outputs": [],
   "source": [
    "x = df.iloc[:, :-1].values.astype('f')\n",
    "t = df.iloc[:, -1].values.astype('f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 788,
     "status": "ok",
     "timestamp": 1547552554433,
     "user": {
      "displayName": "西沢衛",
      "photoUrl": "",
      "userId": "12011220225445512117"
     },
     "user_tz": -540
    },
    "id": "qf2A_QjrYM6C",
    "outputId": "87bc2281-3918-4ad4-8dec-2651ff224857"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(506, 13)"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 660,
     "status": "ok",
     "timestamp": 1547552554435,
     "user": {
      "displayName": "西沢衛",
      "photoUrl": "",
      "userId": "12011220225445512117"
     },
     "user_tz": -540
    },
    "id": "kyENdiUVYM6G",
    "outputId": "42364dd8-c930-4442-c5e2-388894f39299"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(506,)"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7n56nJXUYM6J"
   },
   "source": [
    "ここで、回帰の際に注意しないといけないこととして、教師データのサイズが `(506,)` となっていますが、`(506, 1)` のように506行1変数ということを明確にできていないと`trainer.run()`のタイミングでエラーが出ます。  \n",
    "逆に分類の場合は、`(506,)` のような形式が正しいため、区別して覚えておくことが重要です。  \n",
    "\n",
    "正しく扱えるサイズにするため、Numpyの`reshape`を使用します。  \n",
    "`t = t.reshape(506, 1)` のように数値を直接指定しても良いのですが、今後の汎用性も考えると、いろいろなデータに対応できる形式で記述しておきます。  \n",
    "`len()` を用いてベクトルの長さを取得します。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 780,
     "status": "ok",
     "timestamp": 1547552558157,
     "user": {
      "displayName": "西沢衛",
      "photoUrl": "",
      "userId": "12011220225445512117"
     },
     "user_tz": -540
    },
    "id": "phwon-FZYM6J",
    "outputId": "0b6300ba-62f2-4dfb-cecc-9c21f1adb37d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "506"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ベクトルの長さを取得\n",
    "len(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fvmvzdVFYM6M"
   },
   "outputs": [],
   "source": [
    "t = t.reshape(len(t), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 640,
     "status": "ok",
     "timestamp": 1547552559366,
     "user": {
      "displayName": "西沢衛",
      "photoUrl": "",
      "userId": "12011220225445512117"
     },
     "user_tz": -540
    },
    "id": "ATl0CKB1YM6P",
    "outputId": "289f4911-1bfa-4948-d166-dccf7f2c7ca7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(506, 1)"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UYE1wGNPYM6R"
   },
   "source": [
    "こちらで望ましい形式へと変換することができました。\n",
    "\n",
    "### データセットの準備\n",
    "\n",
    "分類の場合でも紹介した方法で、Chainerで扱える形式へと変換していきます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Mrczk54WYM6T"
   },
   "outputs": [],
   "source": [
    "# Chainerで使用できるデータセットの形式\n",
    "dataset = list(zip(x, t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NbnQvnr2YM6V"
   },
   "source": [
    "### 訓練データ、検証データ、テストデータへ分割\n",
    "\n",
    "ここからはChainer内のモジュールも使用しながら進めていきます。\n",
    "基本的には、下記の３つを最初の段階で読み込んでおくと円滑に進められます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WIlaezRhYM6W"
   },
   "outputs": [],
   "source": [
    "import chainer\n",
    "import chainer.links as L\n",
    "import chainer.functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YHUIL2q1YM6Z"
   },
   "source": [
    "訓練データの数は全体の70%とできるように、`n_train_val` にその数を計算して格納しておき、Chainer側で用意されている `split_dataset_random` を使用して、全体からランダムに70%を訓練データ、残りの30%を検証データとします。  \n",
    "\n",
    "ここで、**ランダム**という言葉が出てきた際は**再現性の確保**ができているかに注意してください。  \n",
    "`split_dataset_random`でも任意の引数として、`seed`があるため、しっかり固定しておきます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YdMWNr65YM6a"
   },
   "outputs": [],
   "source": [
    "n_train_val = int(len(dataset) * 0.7)\n",
    "train_val, test = chainer.datasets.split_dataset_random(dataset, n_train_val, seed=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BTE0l8czYM6b"
   },
   "source": [
    "サンプル数を確認して、おかしな値が出ていないかは逐次確認します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 700,
     "status": "ok",
     "timestamp": 1547552611365,
     "user": {
      "displayName": "西沢衛",
      "photoUrl": "",
      "userId": "12011220225445512117"
     },
     "user_tz": -540
    },
    "id": "VRkpZZGQYM6d",
    "outputId": "89f2e548-4377-48ba-e36f-eca28b9f7384"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "354"
      ]
     },
     "execution_count": 24,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 425,
     "status": "ok",
     "timestamp": 1547552611366,
     "user": {
      "displayName": "西沢衛",
      "photoUrl": "",
      "userId": "12011220225445512117"
     },
     "user_tz": -540
    },
    "id": "utm6bGmhYM6g",
    "outputId": "d4f0b826-c24d-47b8-c458-1254fbe0d064"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "152"
      ]
     },
     "execution_count": 25,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gxAH1gOj4Czs"
   },
   "source": [
    "### 検証用（Validation）データの作成\n",
    "\n",
    "先程作成した`train_val`を訓練用（`train`）データと検証用（`valid`）データに分割します。  \n",
    "`train_valid`と`test`に分割したときと同様のやり方になります。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u43oqAQ33YrK"
   },
   "outputs": [],
   "source": [
    "n_train = int(len(train_val) * 0.7)\n",
    "train, valid = chainer.datasets.split_dataset_random(train_val, n_train, seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1810,
     "status": "ok",
     "timestamp": 1547552622098,
     "user": {
      "displayName": "西沢衛",
      "photoUrl": "",
      "userId": "12011220225445512117"
     },
     "user_tz": -540
    },
    "id": "DA0Ffm6d3Y1c",
    "outputId": "680fa486-c669-4699-f5b7-27b9bac8582e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "247"
      ]
     },
     "execution_count": 27,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1228,
     "status": "ok",
     "timestamp": 1547552622367,
     "user": {
      "displayName": "西沢衛",
      "photoUrl": "",
      "userId": "12011220225445512117"
     },
     "user_tz": -540
    },
    "id": "-uGE2a8D3Y_1",
    "outputId": "eff0bf13-c664-44c7-826e-8e71294bb650"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "107"
      ]
     },
     "execution_count": 28,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DaglScMSYM6o"
   },
   "source": [
    "## モデルの定義\n",
    "\n",
    "回帰のモデル構築も前章の分類とほとんど同じ書き方でネットワークを定義することができます。  \n",
    "基本的には、クラスの定義の部分は同じです。異なる部分は１変数の回帰の場合は出力 `n_out` の値が1になる点です。（複数の数値を予測するモデルを構築する際には`1`にならない場合もあります。）  \n",
    "中間層のノードの数は５、活性化関数はrelu関数を使用します。つまり、`13->5->1` の流れになります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_NguhrLZYM65"
   },
   "outputs": [],
   "source": [
    "class NN(chainer.Chain):\n",
    "\n",
    "    # モデルの構造\n",
    "    def __init__(self, n_mid_units=5, n_out=1):\n",
    "        super().__init__()\n",
    "        with self.init_scope():\n",
    "            self.fc1 = L.Linear(None, n_mid_units)\n",
    "            self.fc2 = L.Linear(None, n_out)            \n",
    "            self.bn = L.BatchNormalization(13)\n",
    "            \n",
    "    # 順伝播\n",
    "    def forward(self, x):\n",
    "        h = self.fc1(self.bn(x))\n",
    "        h = F.relu(h)\n",
    "        h = self.fc2(h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e6gqiyQRYM68"
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O3No1WfAYM6-"
   },
   "outputs": [],
   "source": [
    "model = NN()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P9qh40h7YM7A"
   },
   "source": [
    "### 学習に必要な準備\n",
    "\n",
    "Optimizer, Iteratorなどを定義して学習を実行します。  \n",
    "基本的な記述内容は同じです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Swrij_BkYM7C"
   },
   "outputs": [],
   "source": [
    "optimizer = chainer.optimizers.SGD().setup(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Py0mogl3YM7E"
   },
   "outputs": [],
   "source": [
    "batchsize = 10\n",
    "train_iter = chainer.iterators.SerialIterator(train, batchsize)\n",
    "valid_iter  = chainer.iterators.SerialIterator(valid,  batchsize, repeat=False, shuffle=False)\n",
    "test_iter  = chainer.iterators.SerialIterator(test,  batchsize, repeat=False, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gzql6U4cYM7I"
   },
   "outputs": [],
   "source": [
    "epoch = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v0cVjXhDWFeB"
   },
   "outputs": [],
   "source": [
    "from chainer.dataset import concat_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YKYZ_8qXXaZB"
   },
   "source": [
    "### 学習ループの記述\n",
    "\n",
    "前章までは分類の実装だったため、モデルの評価には`Accuracy`（正解率）を採用してきました。  \n",
    "ですが、回帰の場合は正解率は使用せずに`mean_squared_error`（平均2乗誤差）を用いることが基本的です。  \n",
    "また、パラメータの更新のための損失関数にも`mean_squared_error`（平均2乗誤差）を用います。  \n",
    "\n",
    "そのため、`loss`の計算には`F.mean_squared_error`を使用します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 13634
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3080,
     "status": "ok",
     "timestamp": 1547553302228,
     "user": {
      "displayName": "西沢衛",
      "photoUrl": "",
      "userId": "12011220225445512117"
     },
     "user_tz": -540
    },
    "id": "20AF9Xx1WFXx",
    "outputId": "e2a6d4ca-8afc-4a69-a37d-eca25dd7a299"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:00 train_loss:702.5154 \n",
      "epoch:00 train_loss:321.9022 \n",
      "epoch:00 train_loss:361.5048 \n",
      "epoch:00 train_loss:285.1848 \n",
      "epoch:00 train_loss:274.7160 \n",
      "epoch:00 train_loss:129.9252 \n",
      "epoch:00 train_loss:103.9347 \n",
      "epoch:00 train_loss:83.1562 \n",
      "epoch:00 train_loss:74.3521 \n",
      "epoch:00 train_loss:69.2438 \n",
      "epoch:00 train_loss:84.5675 \n",
      "epoch:00 train_loss:257.4215 \n",
      "epoch:00 train_loss:437.5584 \n",
      "epoch:00 train_loss:445.4826 \n",
      "epoch:00 train_loss:470.2078 \n",
      "epoch:00 train_loss:337.9275 \n",
      "epoch:00 train_loss:546.5707 \n",
      "epoch:00 train_loss:93.5810 \n",
      "epoch:00 train_loss:142.9045 \n",
      "epoch:00 train_loss:90.9780 \n",
      "epoch:00 train_loss:286.4223 \n",
      "epoch:00 train_loss:629.7201 \n",
      "epoch:00 train_loss:607.7008 \n",
      "epoch:00 train_loss:529.9941 \n",
      "epoch:01 train_loss:574.2902 \n",
      "valid_loss:228.6444\n",
      "---\n",
      "epoch:01 train_loss:164.0793 \n",
      "epoch:01 train_loss:240.5096 \n",
      "epoch:01 train_loss:77.3323 \n",
      "epoch:01 train_loss:27.1732 \n",
      "epoch:01 train_loss:50.0803 \n",
      "epoch:01 train_loss:90.8763 \n",
      "epoch:01 train_loss:35.0075 \n",
      "epoch:01 train_loss:29.8944 \n",
      "epoch:01 train_loss:243.6478 \n",
      "epoch:01 train_loss:147.4614 \n",
      "epoch:01 train_loss:573.7899 \n",
      "epoch:01 train_loss:230.5089 \n",
      "epoch:01 train_loss:132.1685 \n",
      "epoch:01 train_loss:115.4401 \n",
      "epoch:01 train_loss:229.3742 \n",
      "epoch:01 train_loss:320.9554 \n",
      "epoch:01 train_loss:317.2508 \n",
      "epoch:01 train_loss:228.7888 \n",
      "epoch:01 train_loss:223.4684 \n",
      "epoch:01 train_loss:128.1299 \n",
      "epoch:01 train_loss:53.9944 \n",
      "epoch:01 train_loss:33.0298 \n",
      "epoch:01 train_loss:15.0993 \n",
      "epoch:01 train_loss:22.7072 \n",
      "epoch:02 train_loss:66.1828 \n",
      "valid_loss:30.6048\n",
      "---\n",
      "epoch:02 train_loss:71.5987 \n",
      "epoch:02 train_loss:101.7687 \n",
      "epoch:02 train_loss:70.4097 \n",
      "epoch:02 train_loss:53.3722 \n",
      "epoch:02 train_loss:44.1072 \n",
      "epoch:02 train_loss:30.5643 \n",
      "epoch:02 train_loss:24.8251 \n",
      "epoch:02 train_loss:26.2422 \n",
      "epoch:02 train_loss:79.4096 \n",
      "epoch:02 train_loss:161.0615 \n",
      "epoch:02 train_loss:4.1248 \n",
      "epoch:02 train_loss:26.6672 \n",
      "epoch:02 train_loss:13.6218 \n",
      "epoch:02 train_loss:10.5530 \n",
      "epoch:02 train_loss:120.8174 \n",
      "epoch:02 train_loss:201.0429 \n",
      "epoch:02 train_loss:100.0927 \n",
      "epoch:02 train_loss:44.0443 \n",
      "epoch:02 train_loss:19.4590 \n",
      "epoch:02 train_loss:38.2913 \n",
      "epoch:02 train_loss:25.8927 \n",
      "epoch:02 train_loss:13.3695 \n",
      "epoch:02 train_loss:45.2634 \n",
      "epoch:02 train_loss:50.9992 \n",
      "epoch:03 train_loss:146.3831 \n",
      "valid_loss:18.6090\n",
      "---\n",
      "epoch:03 train_loss:30.9007 \n",
      "epoch:03 train_loss:100.4474 \n",
      "epoch:03 train_loss:18.5571 \n",
      "epoch:03 train_loss:18.3246 \n",
      "epoch:03 train_loss:21.9604 \n",
      "epoch:03 train_loss:40.8023 \n",
      "epoch:03 train_loss:65.6783 \n",
      "epoch:03 train_loss:30.8283 \n",
      "epoch:03 train_loss:25.2014 \n",
      "epoch:03 train_loss:19.4837 \n",
      "epoch:03 train_loss:17.4477 \n",
      "epoch:03 train_loss:107.8909 \n",
      "epoch:03 train_loss:62.7628 \n",
      "epoch:03 train_loss:83.7716 \n",
      "epoch:03 train_loss:13.1204 \n",
      "epoch:03 train_loss:55.6433 \n",
      "epoch:03 train_loss:106.8876 \n",
      "epoch:03 train_loss:188.9415 \n",
      "epoch:03 train_loss:12.1480 \n",
      "epoch:03 train_loss:128.1008 \n",
      "epoch:03 train_loss:26.8451 \n",
      "epoch:03 train_loss:29.9409 \n",
      "epoch:03 train_loss:55.0783 \n",
      "epoch:04 train_loss:17.2867 \n",
      "valid_loss:32.8074\n",
      "---\n",
      "epoch:04 train_loss:35.9245 \n",
      "epoch:04 train_loss:94.0166 \n",
      "epoch:04 train_loss:39.8584 \n",
      "epoch:04 train_loss:39.0791 \n",
      "epoch:04 train_loss:39.5951 \n",
      "epoch:04 train_loss:26.0455 \n",
      "epoch:04 train_loss:10.4221 \n",
      "epoch:04 train_loss:26.5420 \n",
      "epoch:04 train_loss:33.7994 \n",
      "epoch:04 train_loss:55.9397 \n",
      "epoch:04 train_loss:69.6634 \n",
      "epoch:04 train_loss:102.2277 \n",
      "epoch:04 train_loss:17.9605 \n",
      "epoch:04 train_loss:45.4190 \n",
      "epoch:04 train_loss:17.3052 \n",
      "epoch:04 train_loss:30.5194 \n",
      "epoch:04 train_loss:48.9396 \n",
      "epoch:04 train_loss:10.2461 \n",
      "epoch:04 train_loss:9.9045 \n",
      "epoch:04 train_loss:20.0027 \n",
      "epoch:04 train_loss:30.7583 \n",
      "epoch:04 train_loss:33.0922 \n",
      "epoch:04 train_loss:7.9024 \n",
      "epoch:04 train_loss:30.8317 \n",
      "epoch:05 train_loss:47.3268 \n",
      "valid_loss:34.9136\n",
      "---\n",
      "epoch:05 train_loss:22.2820 \n",
      "epoch:05 train_loss:15.0932 \n",
      "epoch:05 train_loss:97.2015 \n",
      "epoch:05 train_loss:20.4670 \n",
      "epoch:05 train_loss:11.8028 \n",
      "epoch:05 train_loss:48.0749 \n",
      "epoch:05 train_loss:38.0962 \n",
      "epoch:05 train_loss:18.3771 \n",
      "epoch:05 train_loss:16.6475 \n",
      "epoch:05 train_loss:11.9081 \n",
      "epoch:05 train_loss:58.6053 \n",
      "epoch:05 train_loss:84.6214 \n",
      "epoch:05 train_loss:228.9996 \n",
      "epoch:05 train_loss:63.2911 \n",
      "epoch:05 train_loss:35.8347 \n",
      "epoch:05 train_loss:27.3264 \n",
      "epoch:05 train_loss:17.7892 \n",
      "epoch:05 train_loss:12.0203 \n",
      "epoch:05 train_loss:22.4370 \n",
      "epoch:05 train_loss:31.2197 \n",
      "epoch:05 train_loss:12.0203 \n",
      "epoch:05 train_loss:18.0849 \n",
      "epoch:05 train_loss:60.7741 \n",
      "epoch:05 train_loss:39.7785 \n",
      "epoch:06 train_loss:18.6553 \n",
      "valid_loss:22.4568\n",
      "---\n",
      "epoch:06 train_loss:13.8517 \n",
      "epoch:06 train_loss:18.1015 \n",
      "epoch:06 train_loss:6.7983 \n",
      "epoch:06 train_loss:2.8485 \n",
      "epoch:06 train_loss:120.4209 \n",
      "epoch:06 train_loss:105.1286 \n",
      "epoch:06 train_loss:27.2604 \n",
      "epoch:06 train_loss:36.3514 \n",
      "epoch:06 train_loss:10.2474 \n",
      "epoch:06 train_loss:27.2693 \n",
      "epoch:06 train_loss:17.2962 \n",
      "epoch:06 train_loss:64.3966 \n",
      "epoch:06 train_loss:158.7853 \n",
      "epoch:06 train_loss:18.9416 \n",
      "epoch:06 train_loss:22.3041 \n",
      "epoch:06 train_loss:21.1818 \n",
      "epoch:06 train_loss:52.2933 \n",
      "epoch:06 train_loss:36.1334 \n",
      "epoch:06 train_loss:53.9578 \n",
      "epoch:06 train_loss:12.3089 \n",
      "epoch:06 train_loss:22.0914 \n",
      "epoch:06 train_loss:111.4316 \n",
      "epoch:06 train_loss:22.9796 \n",
      "epoch:07 train_loss:90.5962 \n",
      "valid_loss:44.2436\n",
      "---\n",
      "epoch:07 train_loss:38.9267 \n",
      "epoch:07 train_loss:22.3028 \n",
      "epoch:07 train_loss:13.6166 \n",
      "epoch:07 train_loss:86.7768 \n",
      "epoch:07 train_loss:62.0867 \n",
      "epoch:07 train_loss:4.0002 \n",
      "epoch:07 train_loss:172.7430 \n",
      "epoch:07 train_loss:37.5848 \n",
      "epoch:07 train_loss:9.7716 \n",
      "epoch:07 train_loss:8.6884 \n",
      "epoch:07 train_loss:14.7948 \n",
      "epoch:07 train_loss:12.6772 \n",
      "epoch:07 train_loss:6.2672 \n",
      "epoch:07 train_loss:19.3209 \n",
      "epoch:07 train_loss:19.2051 \n",
      "epoch:07 train_loss:56.7426 \n",
      "epoch:07 train_loss:34.7448 \n",
      "epoch:07 train_loss:8.3751 \n",
      "epoch:07 train_loss:32.7972 \n",
      "epoch:07 train_loss:30.5244 \n",
      "epoch:07 train_loss:7.6431 \n",
      "epoch:07 train_loss:30.5962 \n",
      "epoch:07 train_loss:10.6479 \n",
      "epoch:07 train_loss:15.5468 \n",
      "epoch:08 train_loss:30.7954 \n",
      "valid_loss:18.9654\n",
      "---\n",
      "epoch:08 train_loss:27.9900 \n",
      "epoch:08 train_loss:10.9558 \n",
      "epoch:08 train_loss:60.3928 \n",
      "epoch:08 train_loss:63.7630 \n",
      "epoch:08 train_loss:63.5495 \n",
      "epoch:08 train_loss:32.7735 \n",
      "epoch:08 train_loss:19.4934 \n",
      "epoch:08 train_loss:9.6518 \n",
      "epoch:08 train_loss:75.0670 \n",
      "epoch:08 train_loss:32.0827 \n",
      "epoch:08 train_loss:82.7640 \n",
      "epoch:08 train_loss:36.9808 \n",
      "epoch:08 train_loss:30.2197 \n",
      "epoch:08 train_loss:16.6981 \n",
      "epoch:08 train_loss:14.3540 \n",
      "epoch:08 train_loss:98.6283 \n",
      "epoch:08 train_loss:49.4766 \n",
      "epoch:08 train_loss:25.0329 \n",
      "epoch:08 train_loss:20.6209 \n",
      "epoch:08 train_loss:23.1398 \n",
      "epoch:08 train_loss:35.7693 \n",
      "epoch:08 train_loss:21.3659 \n",
      "epoch:08 train_loss:36.2394 \n",
      "epoch:08 train_loss:9.4406 \n",
      "epoch:09 train_loss:16.3929 \n",
      "valid_loss:29.3484\n",
      "---\n",
      "epoch:09 train_loss:36.8180 \n",
      "epoch:09 train_loss:24.0332 \n",
      "epoch:09 train_loss:37.2466 \n",
      "epoch:09 train_loss:46.3045 \n",
      "epoch:09 train_loss:58.0206 \n",
      "epoch:09 train_loss:32.0395 \n",
      "epoch:09 train_loss:55.4150 \n",
      "epoch:09 train_loss:32.7107 \n",
      "epoch:09 train_loss:6.9144 \n",
      "epoch:09 train_loss:20.6584 \n",
      "epoch:09 train_loss:124.1248 \n",
      "epoch:09 train_loss:50.8503 \n",
      "epoch:09 train_loss:14.0927 \n",
      "epoch:09 train_loss:52.8031 \n",
      "epoch:09 train_loss:17.9666 \n",
      "epoch:09 train_loss:14.0640 \n",
      "epoch:09 train_loss:87.4313 \n",
      "epoch:09 train_loss:32.5490 \n",
      "epoch:09 train_loss:60.0214 \n",
      "epoch:09 train_loss:51.6893 \n",
      "epoch:09 train_loss:36.4253 \n",
      "epoch:09 train_loss:25.5709 \n",
      "epoch:09 train_loss:14.0665 \n",
      "epoch:10 train_loss:21.6264 \n",
      "valid_loss:54.6520\n",
      "---\n",
      "epoch:10 train_loss:19.5443 \n",
      "epoch:10 train_loss:40.5919 \n",
      "epoch:10 train_loss:14.2636 \n",
      "epoch:10 train_loss:23.2354 \n",
      "epoch:10 train_loss:26.0897 \n",
      "epoch:10 train_loss:12.7513 \n",
      "epoch:10 train_loss:10.6397 \n",
      "epoch:10 train_loss:22.6432 \n",
      "epoch:10 train_loss:23.2207 \n",
      "epoch:10 train_loss:40.9775 \n",
      "epoch:10 train_loss:27.2240 \n",
      "epoch:10 train_loss:37.0775 \n",
      "epoch:10 train_loss:21.7293 \n",
      "epoch:10 train_loss:12.9382 \n",
      "epoch:10 train_loss:20.9397 \n",
      "epoch:10 train_loss:11.6704 \n",
      "epoch:10 train_loss:77.3644 \n",
      "epoch:10 train_loss:54.5159 \n",
      "epoch:10 train_loss:15.7016 \n",
      "epoch:10 train_loss:38.0593 \n",
      "epoch:10 train_loss:42.3492 \n",
      "epoch:10 train_loss:91.7433 \n",
      "epoch:10 train_loss:24.7185 \n",
      "epoch:10 train_loss:32.5576 \n",
      "epoch:11 train_loss:9.0289 \n",
      "valid_loss:39.4299\n",
      "---\n",
      "epoch:11 train_loss:44.9746 \n",
      "epoch:11 train_loss:18.7202 \n",
      "epoch:11 train_loss:33.3583 \n",
      "epoch:11 train_loss:27.4048 \n",
      "epoch:11 train_loss:7.5442 \n",
      "epoch:11 train_loss:17.1718 \n",
      "epoch:11 train_loss:131.1121 \n",
      "epoch:11 train_loss:29.3383 \n",
      "epoch:11 train_loss:7.9091 \n",
      "epoch:11 train_loss:5.3153 \n",
      "epoch:11 train_loss:37.4883 \n",
      "epoch:11 train_loss:16.7259 \n",
      "epoch:11 train_loss:12.9455 \n",
      "epoch:11 train_loss:14.7821 \n",
      "epoch:11 train_loss:10.1358 \n",
      "epoch:11 train_loss:48.6657 \n",
      "epoch:11 train_loss:17.6525 \n",
      "epoch:11 train_loss:22.0754 \n",
      "epoch:11 train_loss:12.1459 \n",
      "epoch:11 train_loss:11.3221 \n",
      "epoch:11 train_loss:25.5663 \n",
      "epoch:11 train_loss:30.2665 \n",
      "epoch:11 train_loss:21.8095 \n",
      "epoch:11 train_loss:18.8754 \n",
      "epoch:12 train_loss:40.4329 \n",
      "valid_loss:41.9813\n",
      "---\n",
      "epoch:12 train_loss:19.6120 \n",
      "epoch:12 train_loss:8.9165 \n",
      "epoch:12 train_loss:19.6507 \n",
      "epoch:12 train_loss:10.6104 \n",
      "epoch:12 train_loss:75.7741 \n",
      "epoch:12 train_loss:84.9155 \n",
      "epoch:12 train_loss:22.4842 \n",
      "epoch:12 train_loss:39.9926 \n",
      "epoch:12 train_loss:25.5535 \n",
      "epoch:12 train_loss:91.8647 \n",
      "epoch:12 train_loss:14.0325 \n",
      "epoch:12 train_loss:23.5924 \n",
      "epoch:12 train_loss:7.4147 \n",
      "epoch:12 train_loss:26.4081 \n",
      "epoch:12 train_loss:54.3909 \n",
      "epoch:12 train_loss:26.0313 \n",
      "epoch:12 train_loss:64.5840 \n",
      "epoch:12 train_loss:12.5435 \n",
      "epoch:12 train_loss:18.3734 \n",
      "epoch:12 train_loss:105.9734 \n",
      "epoch:12 train_loss:316.9657 \n",
      "epoch:12 train_loss:120.1486 \n",
      "epoch:12 train_loss:153.3712 \n",
      "epoch:12 train_loss:62.9517 \n",
      "epoch:13 train_loss:74.9269 \n",
      "valid_loss:68.0794\n",
      "---\n",
      "epoch:13 train_loss:176.7036 \n",
      "epoch:13 train_loss:30.2777 \n",
      "epoch:13 train_loss:68.1471 \n",
      "epoch:13 train_loss:81.7602 \n",
      "epoch:13 train_loss:22.0305 \n",
      "epoch:13 train_loss:103.7245 \n",
      "epoch:13 train_loss:11.0236 \n",
      "epoch:13 train_loss:3.0125 \n",
      "epoch:13 train_loss:72.1095 \n",
      "epoch:13 train_loss:34.0381 \n",
      "epoch:13 train_loss:37.9940 \n",
      "epoch:13 train_loss:18.7909 \n",
      "epoch:13 train_loss:43.7701 \n",
      "epoch:13 train_loss:8.5284 \n",
      "epoch:13 train_loss:20.4040 \n",
      "epoch:13 train_loss:15.9190 \n",
      "epoch:13 train_loss:21.8374 \n",
      "epoch:13 train_loss:32.4980 \n",
      "epoch:13 train_loss:6.3925 \n",
      "epoch:13 train_loss:9.2827 \n",
      "epoch:13 train_loss:109.3921 \n",
      "epoch:13 train_loss:132.6092 \n",
      "epoch:13 train_loss:58.6292 \n",
      "epoch:14 train_loss:19.4284 \n",
      "valid_loss:39.1270\n",
      "---\n",
      "epoch:14 train_loss:40.1859 \n",
      "epoch:14 train_loss:57.5370 \n",
      "epoch:14 train_loss:7.7779 \n",
      "epoch:14 train_loss:5.4296 \n",
      "epoch:14 train_loss:6.4193 \n",
      "epoch:14 train_loss:20.7767 \n",
      "epoch:14 train_loss:31.4090 \n",
      "epoch:14 train_loss:17.1486 \n",
      "epoch:14 train_loss:126.6240 \n",
      "epoch:14 train_loss:35.5960 \n",
      "epoch:14 train_loss:27.9360 \n",
      "epoch:14 train_loss:40.4105 \n",
      "epoch:14 train_loss:26.8281 \n",
      "epoch:14 train_loss:22.2545 \n",
      "epoch:14 train_loss:22.1841 \n",
      "epoch:14 train_loss:57.5734 \n",
      "epoch:14 train_loss:66.8927 \n",
      "epoch:14 train_loss:21.5855 \n",
      "epoch:14 train_loss:57.1861 \n",
      "epoch:14 train_loss:267.2909 \n",
      "epoch:14 train_loss:86.7964 \n",
      "epoch:14 train_loss:16.6162 \n",
      "epoch:14 train_loss:33.3614 \n",
      "epoch:14 train_loss:21.8924 \n",
      "epoch:15 train_loss:8.5720 \n",
      "valid_loss:35.4441\n",
      "---\n",
      "epoch:15 train_loss:43.9637 \n",
      "epoch:15 train_loss:50.6993 \n",
      "epoch:15 train_loss:101.1092 \n",
      "epoch:15 train_loss:74.5042 \n",
      "epoch:15 train_loss:11.8066 \n",
      "epoch:15 train_loss:11.6511 \n",
      "epoch:15 train_loss:19.7328 \n",
      "epoch:15 train_loss:14.9322 \n",
      "epoch:15 train_loss:7.8298 \n",
      "epoch:15 train_loss:8.3233 \n",
      "epoch:15 train_loss:27.0142 \n",
      "epoch:15 train_loss:9.0868 \n",
      "epoch:15 train_loss:97.3009 \n",
      "epoch:15 train_loss:18.6584 \n",
      "epoch:15 train_loss:62.9294 \n",
      "epoch:15 train_loss:19.5781 \n",
      "epoch:15 train_loss:17.5464 \n",
      "epoch:15 train_loss:15.9124 \n",
      "epoch:15 train_loss:7.3087 \n",
      "epoch:15 train_loss:19.7573 \n",
      "epoch:15 train_loss:16.7337 \n",
      "epoch:15 train_loss:32.6791 \n",
      "epoch:15 train_loss:105.8015 \n",
      "epoch:15 train_loss:10.8024 \n",
      "epoch:16 train_loss:20.9022 \n",
      "valid_loss:24.6857\n",
      "---\n",
      "epoch:16 train_loss:41.5387 \n",
      "epoch:16 train_loss:75.0230 \n",
      "epoch:16 train_loss:52.6881 \n",
      "epoch:16 train_loss:6.9657 \n",
      "epoch:16 train_loss:6.3267 \n",
      "epoch:16 train_loss:4.9357 \n",
      "epoch:16 train_loss:12.7892 \n",
      "epoch:16 train_loss:21.4142 \n",
      "epoch:16 train_loss:116.4430 \n",
      "epoch:16 train_loss:23.8938 \n",
      "epoch:16 train_loss:60.8316 \n",
      "epoch:16 train_loss:55.3509 \n",
      "epoch:16 train_loss:14.0212 \n",
      "epoch:16 train_loss:18.4019 \n",
      "epoch:16 train_loss:42.6656 \n",
      "epoch:16 train_loss:15.2226 \n",
      "epoch:16 train_loss:16.7738 \n",
      "epoch:16 train_loss:38.5822 \n",
      "epoch:16 train_loss:11.2695 \n",
      "epoch:16 train_loss:28.1892 \n",
      "epoch:16 train_loss:17.2719 \n",
      "epoch:16 train_loss:10.6994 \n",
      "epoch:16 train_loss:36.2133 \n",
      "epoch:17 train_loss:35.6439 \n",
      "valid_loss:13.2583\n",
      "---\n",
      "epoch:17 train_loss:21.6208 \n",
      "epoch:17 train_loss:14.1908 \n",
      "epoch:17 train_loss:13.9969 \n",
      "epoch:17 train_loss:13.7841 \n",
      "epoch:17 train_loss:135.0210 \n",
      "epoch:17 train_loss:177.0882 \n",
      "epoch:17 train_loss:24.7589 \n",
      "epoch:17 train_loss:73.5708 \n",
      "epoch:17 train_loss:24.6243 \n",
      "epoch:17 train_loss:40.3575 \n",
      "epoch:17 train_loss:10.3719 \n",
      "epoch:17 train_loss:16.7753 \n",
      "epoch:17 train_loss:8.2714 \n",
      "epoch:17 train_loss:10.1948 \n",
      "epoch:17 train_loss:23.8340 \n",
      "epoch:17 train_loss:36.6165 \n",
      "epoch:17 train_loss:149.5609 \n",
      "epoch:17 train_loss:54.6476 \n",
      "epoch:17 train_loss:30.2247 \n",
      "epoch:17 train_loss:118.5459 \n",
      "epoch:17 train_loss:73.6814 \n",
      "epoch:17 train_loss:8.7720 \n",
      "epoch:17 train_loss:11.4846 \n",
      "epoch:17 train_loss:43.5063 \n",
      "epoch:18 train_loss:21.0722 \n",
      "valid_loss:25.0454\n",
      "---\n",
      "epoch:18 train_loss:10.8189 \n",
      "epoch:18 train_loss:9.8148 \n",
      "epoch:18 train_loss:6.6761 \n",
      "epoch:18 train_loss:107.8603 \n",
      "epoch:18 train_loss:29.9691 \n",
      "epoch:18 train_loss:16.7573 \n",
      "epoch:18 train_loss:34.4139 \n",
      "epoch:18 train_loss:30.3625 \n",
      "epoch:18 train_loss:21.0194 \n",
      "epoch:18 train_loss:5.8774 \n",
      "epoch:18 train_loss:11.3012 \n",
      "epoch:18 train_loss:54.6642 \n",
      "epoch:18 train_loss:14.5228 \n",
      "epoch:18 train_loss:12.7173 \n",
      "epoch:18 train_loss:17.7510 \n",
      "epoch:18 train_loss:26.6148 \n",
      "epoch:18 train_loss:19.5764 \n",
      "epoch:18 train_loss:142.3591 \n",
      "epoch:18 train_loss:10.8367 \n",
      "epoch:18 train_loss:42.6531 \n",
      "epoch:18 train_loss:59.5204 \n",
      "epoch:18 train_loss:38.0888 \n",
      "epoch:18 train_loss:13.9165 \n",
      "epoch:18 train_loss:18.2910 \n",
      "epoch:19 train_loss:23.0162 \n",
      "valid_loss:16.2661\n",
      "---\n",
      "epoch:19 train_loss:14.1494 \n",
      "epoch:19 train_loss:11.5648 \n",
      "epoch:19 train_loss:40.3402 \n",
      "epoch:19 train_loss:14.5162 \n",
      "epoch:19 train_loss:22.6852 \n",
      "epoch:19 train_loss:22.3923 \n",
      "epoch:19 train_loss:11.8443 \n",
      "epoch:19 train_loss:10.1552 \n",
      "epoch:19 train_loss:48.9802 \n",
      "epoch:19 train_loss:23.0915 \n",
      "epoch:19 train_loss:28.2165 \n",
      "epoch:19 train_loss:118.3308 \n",
      "epoch:19 train_loss:11.9646 \n",
      "epoch:19 train_loss:32.4770 \n",
      "epoch:19 train_loss:18.7534 \n",
      "epoch:19 train_loss:14.7281 \n",
      "epoch:19 train_loss:44.6944 \n",
      "epoch:19 train_loss:7.0757 \n",
      "epoch:19 train_loss:50.7517 \n",
      "epoch:19 train_loss:28.5623 \n",
      "epoch:19 train_loss:8.6026 \n",
      "epoch:19 train_loss:72.0584 \n",
      "epoch:19 train_loss:34.2843 \n",
      "epoch:20 train_loss:7.0924 \n",
      "valid_loss:17.9130\n",
      "---\n",
      "epoch:20 train_loss:24.9987 \n",
      "epoch:20 train_loss:110.9411 \n",
      "epoch:20 train_loss:20.8515 \n",
      "epoch:20 train_loss:44.8239 \n",
      "epoch:20 train_loss:43.3761 \n",
      "epoch:20 train_loss:39.0631 \n",
      "epoch:20 train_loss:13.2548 \n",
      "epoch:20 train_loss:41.3904 \n",
      "epoch:20 train_loss:30.0552 \n",
      "epoch:20 train_loss:22.4233 \n",
      "epoch:20 train_loss:11.1551 \n",
      "epoch:20 train_loss:13.2090 \n",
      "epoch:20 train_loss:33.1844 \n",
      "epoch:20 train_loss:9.3407 \n",
      "epoch:20 train_loss:16.9512 \n",
      "epoch:20 train_loss:22.6743 \n",
      "epoch:20 train_loss:13.0283 \n",
      "epoch:20 train_loss:15.7001 \n",
      "epoch:20 train_loss:70.2726 \n",
      "epoch:20 train_loss:65.3807 \n",
      "epoch:20 train_loss:25.1772 \n",
      "epoch:20 train_loss:28.7833 \n",
      "epoch:20 train_loss:26.1645 \n",
      "epoch:20 train_loss:95.5191 \n",
      "epoch:21 train_loss:11.3903 \n",
      "valid_loss:25.3340\n",
      "---\n",
      "epoch:21 train_loss:49.0208 \n",
      "epoch:21 train_loss:59.8751 \n",
      "epoch:21 train_loss:6.7375 \n",
      "epoch:21 train_loss:83.8974 \n",
      "epoch:21 train_loss:21.9530 \n",
      "epoch:21 train_loss:21.7642 \n",
      "epoch:21 train_loss:28.1550 \n",
      "epoch:21 train_loss:63.3799 \n",
      "epoch:21 train_loss:12.3033 \n",
      "epoch:21 train_loss:22.6522 \n",
      "epoch:21 train_loss:23.1995 \n",
      "epoch:21 train_loss:12.9943 \n",
      "epoch:21 train_loss:28.0468 \n",
      "epoch:21 train_loss:18.1159 \n",
      "epoch:21 train_loss:10.5178 \n",
      "epoch:21 train_loss:43.5212 \n",
      "epoch:21 train_loss:25.4106 \n",
      "epoch:21 train_loss:36.6439 \n",
      "epoch:21 train_loss:108.1523 \n",
      "epoch:21 train_loss:55.0473 \n",
      "epoch:21 train_loss:27.7823 \n",
      "epoch:21 train_loss:37.4028 \n",
      "epoch:21 train_loss:9.1888 \n",
      "epoch:21 train_loss:25.6183 \n",
      "epoch:22 train_loss:14.7225 \n",
      "valid_loss:17.7829\n",
      "---\n",
      "epoch:22 train_loss:16.7250 \n",
      "epoch:22 train_loss:37.0499 \n",
      "epoch:22 train_loss:54.2221 \n",
      "epoch:22 train_loss:140.6745 \n",
      "epoch:22 train_loss:91.1786 \n",
      "epoch:22 train_loss:27.6765 \n",
      "epoch:22 train_loss:88.1799 \n",
      "epoch:22 train_loss:29.2943 \n",
      "epoch:22 train_loss:8.3552 \n",
      "epoch:22 train_loss:26.1697 \n",
      "epoch:22 train_loss:11.8849 \n",
      "epoch:22 train_loss:10.6755 \n",
      "epoch:22 train_loss:68.9342 \n",
      "epoch:22 train_loss:34.9179 \n",
      "epoch:22 train_loss:78.1351 \n",
      "epoch:22 train_loss:24.6190 \n",
      "epoch:22 train_loss:108.6920 \n",
      "epoch:22 train_loss:13.8049 \n",
      "epoch:22 train_loss:40.2868 \n",
      "epoch:22 train_loss:12.6733 \n",
      "epoch:22 train_loss:101.1876 \n",
      "epoch:22 train_loss:49.1489 \n",
      "epoch:22 train_loss:60.3947 \n",
      "epoch:22 train_loss:69.4046 \n",
      "epoch:23 train_loss:38.6473 \n",
      "valid_loss:18.7851\n",
      "---\n",
      "epoch:23 train_loss:32.4266 \n",
      "epoch:23 train_loss:17.9999 \n",
      "epoch:23 train_loss:10.1886 \n",
      "epoch:23 train_loss:26.4222 \n",
      "epoch:23 train_loss:15.0040 \n",
      "epoch:23 train_loss:11.3543 \n",
      "epoch:23 train_loss:30.1782 \n",
      "epoch:23 train_loss:15.0472 \n",
      "epoch:23 train_loss:22.4980 \n",
      "epoch:23 train_loss:3.3486 \n",
      "epoch:23 train_loss:28.6975 \n",
      "epoch:23 train_loss:42.1371 \n",
      "epoch:23 train_loss:5.1142 \n",
      "epoch:23 train_loss:14.7255 \n",
      "epoch:23 train_loss:35.1887 \n",
      "epoch:23 train_loss:31.3581 \n",
      "epoch:23 train_loss:127.8271 \n",
      "epoch:23 train_loss:35.4832 \n",
      "epoch:23 train_loss:14.9229 \n",
      "epoch:23 train_loss:62.4239 \n",
      "epoch:23 train_loss:48.4665 \n",
      "epoch:23 train_loss:10.9586 \n",
      "epoch:23 train_loss:42.0933 \n",
      "epoch:24 train_loss:42.4733 \n",
      "valid_loss:13.7352\n",
      "---\n",
      "epoch:24 train_loss:36.8135 \n",
      "epoch:24 train_loss:24.5914 \n",
      "epoch:24 train_loss:26.1169 \n",
      "epoch:24 train_loss:9.9987 \n",
      "epoch:24 train_loss:65.4490 \n",
      "epoch:24 train_loss:16.9043 \n",
      "epoch:24 train_loss:39.3196 \n",
      "epoch:24 train_loss:19.8350 \n",
      "epoch:24 train_loss:24.1159 \n",
      "epoch:24 train_loss:15.4308 \n",
      "epoch:24 train_loss:24.8415 \n",
      "epoch:24 train_loss:13.8203 \n",
      "epoch:24 train_loss:7.1237 \n",
      "epoch:24 train_loss:18.0495 \n",
      "epoch:24 train_loss:6.7192 \n",
      "epoch:24 train_loss:42.7067 \n",
      "epoch:24 train_loss:11.9545 \n",
      "epoch:24 train_loss:10.2516 \n",
      "epoch:24 train_loss:7.2068 \n",
      "epoch:24 train_loss:13.7613 \n",
      "epoch:24 train_loss:13.4949 \n",
      "epoch:24 train_loss:35.6537 \n",
      "epoch:24 train_loss:72.8376 \n",
      "epoch:24 train_loss:26.7272 \n",
      "epoch:25 train_loss:15.2818 \n",
      "valid_loss:13.0708\n",
      "---\n",
      "epoch:25 train_loss:66.2893 \n",
      "epoch:25 train_loss:57.0534 \n",
      "epoch:25 train_loss:40.5496 \n",
      "epoch:25 train_loss:10.1211 \n",
      "epoch:25 train_loss:13.9290 \n",
      "epoch:25 train_loss:13.3056 \n",
      "epoch:25 train_loss:9.6460 \n",
      "epoch:25 train_loss:39.9771 \n",
      "epoch:25 train_loss:9.6557 \n",
      "epoch:25 train_loss:54.8988 \n",
      "epoch:25 train_loss:37.8444 \n",
      "epoch:25 train_loss:19.7139 \n",
      "epoch:25 train_loss:137.3094 \n",
      "epoch:25 train_loss:35.1320 \n",
      "epoch:25 train_loss:13.1068 \n",
      "epoch:25 train_loss:11.8133 \n",
      "epoch:25 train_loss:21.1394 \n",
      "epoch:25 train_loss:13.2581 \n",
      "epoch:25 train_loss:12.8393 \n",
      "epoch:25 train_loss:8.0404 \n",
      "epoch:25 train_loss:7.3564 \n",
      "epoch:25 train_loss:20.5784 \n",
      "epoch:25 train_loss:30.7428 \n",
      "epoch:25 train_loss:49.7162 \n",
      "epoch:26 train_loss:17.2346 \n",
      "valid_loss:20.2850\n",
      "---\n",
      "epoch:26 train_loss:15.0614 \n",
      "epoch:26 train_loss:11.9187 \n",
      "epoch:26 train_loss:19.5362 \n",
      "epoch:26 train_loss:7.5572 \n",
      "epoch:26 train_loss:27.9235 \n",
      "epoch:26 train_loss:33.6436 \n",
      "epoch:26 train_loss:17.5227 \n",
      "epoch:26 train_loss:41.7955 \n",
      "epoch:26 train_loss:8.0801 \n",
      "epoch:26 train_loss:15.4390 \n",
      "epoch:26 train_loss:114.9494 \n",
      "epoch:26 train_loss:203.2613 \n",
      "epoch:26 train_loss:23.1301 \n",
      "epoch:26 train_loss:27.6284 \n",
      "epoch:26 train_loss:17.6357 \n",
      "epoch:26 train_loss:34.4708 \n",
      "epoch:26 train_loss:8.4267 \n",
      "epoch:26 train_loss:35.4314 \n",
      "epoch:26 train_loss:11.3407 \n",
      "epoch:26 train_loss:22.9916 \n",
      "epoch:26 train_loss:4.0924 \n",
      "epoch:26 train_loss:144.8463 \n",
      "epoch:26 train_loss:16.7230 \n",
      "epoch:27 train_loss:56.2354 \n",
      "valid_loss:26.8756\n",
      "---\n",
      "epoch:27 train_loss:56.1715 \n",
      "epoch:27 train_loss:71.0842 \n",
      "epoch:27 train_loss:21.0020 \n",
      "epoch:27 train_loss:74.3345 \n",
      "epoch:27 train_loss:36.9033 \n",
      "epoch:27 train_loss:47.7555 \n",
      "epoch:27 train_loss:22.7944 \n",
      "epoch:27 train_loss:87.4224 \n",
      "epoch:27 train_loss:21.6396 \n",
      "epoch:27 train_loss:7.0169 \n",
      "epoch:27 train_loss:11.9886 \n",
      "epoch:27 train_loss:16.1863 \n",
      "epoch:27 train_loss:38.4879 \n",
      "epoch:27 train_loss:57.3675 \n",
      "epoch:27 train_loss:14.1022 \n",
      "epoch:27 train_loss:44.0222 \n",
      "epoch:27 train_loss:10.9062 \n",
      "epoch:27 train_loss:10.2994 \n",
      "epoch:27 train_loss:6.0381 \n",
      "epoch:27 train_loss:8.6119 \n",
      "epoch:27 train_loss:66.2738 \n",
      "epoch:27 train_loss:58.3182 \n",
      "epoch:27 train_loss:23.4434 \n",
      "epoch:27 train_loss:33.9158 \n",
      "epoch:28 train_loss:47.2509 \n",
      "valid_loss:21.6579\n",
      "---\n",
      "epoch:28 train_loss:74.6118 \n",
      "epoch:28 train_loss:59.5738 \n",
      "epoch:28 train_loss:10.5987 \n",
      "epoch:28 train_loss:10.6933 \n",
      "epoch:28 train_loss:15.9938 \n",
      "epoch:28 train_loss:14.6909 \n",
      "epoch:28 train_loss:58.3671 \n",
      "epoch:28 train_loss:12.1722 \n",
      "epoch:28 train_loss:19.4334 \n",
      "epoch:28 train_loss:35.1148 \n",
      "epoch:28 train_loss:55.2005 \n",
      "epoch:28 train_loss:29.9888 \n",
      "epoch:28 train_loss:17.8146 \n",
      "epoch:28 train_loss:8.7257 \n",
      "epoch:28 train_loss:24.6970 \n",
      "epoch:28 train_loss:57.3172 \n",
      "epoch:28 train_loss:99.4100 \n",
      "epoch:28 train_loss:8.1297 \n",
      "epoch:28 train_loss:10.7493 \n",
      "epoch:28 train_loss:37.8709 \n",
      "epoch:28 train_loss:36.8590 \n",
      "epoch:28 train_loss:49.3805 \n",
      "epoch:28 train_loss:22.5546 \n",
      "epoch:28 train_loss:36.6333 \n",
      "epoch:29 train_loss:35.2662 \n",
      "valid_loss:17.8871\n",
      "---\n",
      "epoch:29 train_loss:82.3617 \n",
      "epoch:29 train_loss:54.6300 \n",
      "epoch:29 train_loss:67.6304 \n",
      "epoch:29 train_loss:31.3708 \n",
      "epoch:29 train_loss:30.3348 \n",
      "epoch:29 train_loss:15.7326 \n",
      "epoch:29 train_loss:28.6963 \n",
      "epoch:29 train_loss:41.7467 \n",
      "epoch:29 train_loss:23.9349 \n",
      "epoch:29 train_loss:17.3796 \n",
      "epoch:29 train_loss:9.6489 \n",
      "epoch:29 train_loss:28.5174 \n",
      "epoch:29 train_loss:39.1421 \n",
      "epoch:29 train_loss:42.2559 \n",
      "epoch:29 train_loss:35.7176 \n",
      "epoch:29 train_loss:40.9159 \n",
      "epoch:29 train_loss:49.3429 \n",
      "epoch:29 train_loss:44.2598 \n",
      "epoch:29 train_loss:19.9285 \n",
      "epoch:29 train_loss:16.6732 \n",
      "epoch:29 train_loss:19.0458 \n",
      "epoch:29 train_loss:24.7909 \n",
      "epoch:29 train_loss:12.4511 \n",
      "epoch:30 train_loss:38.4611 \n",
      "valid_loss:14.1377\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "train_iter.reset()\n",
    "valid_iter.reset()\n",
    "\n",
    "while train_iter.epoch < epoch: \n",
    "  \n",
    "\n",
    "  #　------------  学習の1イテレーション  ------------\n",
    "  \n",
    "  # データの取得\n",
    "  train_batch = train_iter.next()\n",
    "  x_train, t_train = concat_examples(train_batch)\n",
    "\n",
    "\n",
    "  # 予測値の計算\n",
    "  y_train = model(x_train)\n",
    "\n",
    "  # ロスの計算\n",
    "  loss_train = F.mean_squared_error(y_train, t_train)\n",
    "\n",
    "  # 勾配の計算\n",
    "  model.cleargrads()\n",
    "  loss_train.backward()\n",
    "\n",
    "  # パラメータの更新\n",
    "  optimizer.update()\n",
    "\n",
    "\n",
    "  \n",
    "  print('epoch:{:02d} train_loss:{:.04f} '.format(train_iter.epoch, loss_train.data, end=''))\n",
    "  \n",
    "#   ----------------  ここまで  ----------------   \n",
    "\n",
    "\n",
    "  if train_iter.is_new_epoch: # 新しいエポックに入った時のみ計算\n",
    "\n",
    "    while True:\n",
    "      # 検証データの取得   \n",
    "      valid_batch = valid_iter.next() # 追加\n",
    "      x_valid, t_valid = concat_examples(valid_batch)\n",
    "\n",
    "     # 検証用データで順伝播の計算を実行\n",
    "      with chainer.using_config('train', False), chainer.using_config('enable_backprop', False):\n",
    "        y_valid = model(x_valid)\n",
    "\n",
    "\n",
    "      # 検証データで損失関数を計算\n",
    "      loss_valid = F.mean_squared_error(y_valid, t_valid)\n",
    "\n",
    "      if valid_iter.is_new_epoch: # 追加：1エポック計算し終わると、イテレーターをリセット\n",
    "        valid_iter.reset()\n",
    "        break\n",
    "\n",
    "     # 結果を表示  \n",
    "    print('valid_loss:{:.04f}'.format(loss_valid.data))\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "afNSEtxbYXkZ"
   },
   "source": [
    "## 回帰の学習結果の確認\n",
    "\n",
    "単純に`loss`の値がどれほど減少しているかで、学習がどのように行われているか考察することが可能です。  \n",
    "\n",
    "正確にどれほど、モデルがうまく学習できているのかを理解するには、平均2乗誤差で算出された値に対して、平方し2乗を取り除いた値から確認することができます。  \n",
    "\n",
    "Numpyの`sqrt()`関数を使用すれば簡単に算出することが可能です。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1692,
     "status": "ok",
     "timestamp": 1547553986482,
     "user": {
      "displayName": "西沢衛",
      "photoUrl": "",
      "userId": "12011220225445512117"
     },
     "user_tz": -540
    },
    "id": "XgZV6_lKYSuq",
    "outputId": "1c69667a-037a-4675-a509-6e0ea06cfcfd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.7600132978488254"
      ]
     },
     "execution_count": 57,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(14.1377)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iTtQLgm4YM7b"
   },
   "source": [
    "学習済みモデルの保存やロード、学習済みモデルを使用した推論方法は分類とほとんど同じになります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XU9ijWA2nwik"
   },
   "source": [
    "次章では、学習をよりシンプルに実装し、学習結果を容易に可視化することを可能にする`Trainer`について解説します。"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "8_Chainer(回帰).ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
