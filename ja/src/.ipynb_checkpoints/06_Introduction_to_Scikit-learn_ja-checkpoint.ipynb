{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scikit-learn 入門\n",
    "\n",
    "本チュートリアルの主題であるディープラーニングの前に、一般的な機械学習アルゴリズムの実装方法を学びます。\n",
    "モデル構築の手順や細かい調整など先に俯瞰して学んでおくことで、Chainer を使い始めた際の理解度が上がります。\n",
    "\n",
    "Python で機械学習を扱う際には、一般的に **scikit-learn** がよく使用されるため、本章ではこの scikit-learn の使い方を含めて、機械学習によるモデル構築の過程を学んでいきましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scikit-learn（基礎編）\n",
    "\n",
    "scikit-learn は Python のオープンソース機械学習ライブラリであり、分類や回帰などの様々なアルゴリズムが容易に実装できます。\n",
    "すでに学んでいる NumPy の ndarray でデータのやり取りを行うことができ、これまで学んできたライブラリとの連携もしやすくなっています。\n",
    "\n",
    "### scikit-learnが用意しているもの\n",
    "\n",
    "本章は大きく以下の 6 つのセクションに分けられています。\n",
    "\n",
    "- 回帰 (Regression)\n",
    "- 分類 (Classification)\n",
    "- クラスタリング (Clustering)\n",
    "- 次元削減 (Dimensionality reduction)\n",
    "- モデル選択 (Model selection)\n",
    "- 前処理 (Preprocessing)\n",
    "\n",
    "実装されている機械学習アルゴリズムや、その各種パラメータについては[公式のドキュメント](https://scikit-learn.org/stable/)に記載せれており、公式の情報はこちらで確認することができます。\n",
    "\n",
    "\n",
    "### データセットの準備\n",
    "\n",
    "前章で NumPy を用いて実装した、重回帰分析を scikit-learn を用いて実装していきます。  \n",
    "データセットは前章で使用したものを再度使用します。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 3]\n",
      " [2 5]\n",
      " [3 4]\n",
      " [5 9]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# xの定義\n",
    "x = np.array([\n",
    "    [2, 3],\n",
    "    [2, 5],\n",
    "    [3, 4],\n",
    "    [5, 9],\n",
    "])\n",
    "\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "切片を重みベクトルに含めて扱うため、デザイン行列の 0 列目に 1 という値を付け加えます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 2. 3.]\n",
      " [1. 2. 5.]\n",
      " [1. 3. 4.]\n",
      " [1. 5. 9.]]\n"
     ]
    }
   ],
   "source": [
    "# データ数（X.shape[0]) と同じ数だけ 1 が並んだ配列\n",
    "ones = np.ones((x.shape[0], 1))\n",
    "\n",
    "# concatenate を使い、1 次元目に 1 を付け加えていく\n",
    "x = np.concatenate((np.ones((4, 1)), x), axis=1)\n",
    "\n",
    "# 先頭に 1 が付け加わったデザイン行列\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "目標値として下記を使用します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 5 6 8]\n"
     ]
    }
   ],
   "source": [
    "# t の定義\n",
    "t = np.array([1, 5, 6, 8])\n",
    "\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scikit-learnを用いた重回帰分析\n",
    "\n",
    "scikit-learn が提供している重回帰分析を行うためのクラスを読み込み、今回用いるデータに適用していきます。  \n",
    "詳細な使用方法は[公式のドキュメント](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression)を確認してください。  \n",
    "\n",
    "scikit-learn がインストール済みであれば、`sklearn` という名前で `import` することができます。\n",
    "Google Colaboratory では標準でインストールされているため、早速始めていきましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scikit-learn では重回帰分析を行うためのクラスが `LinearRegression` という名前で定義されています。\n",
    "scikit-learn ででこのクラスが定義されているモジュールは以下のような階層構造を持っています。\n",
    "\n",
    "```\n",
    "sklearn\n",
    "├── linear_model\n",
    "│   ├── LinearRegression\n",
    "│   ├── ...\n",
    "```\n",
    "\n",
    "そこで、このような階層構造の下にあるクラスを呼び出す方法を 3 つ紹介します。\n",
    "\n",
    "1 つ目は、`sklearn.linear_model` のように親モジュールを読み込むことです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sklearn.linear_model.LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 つ目は `from` と`import` の 2 つを使って、親モジュールを読み込む方法です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = linear_model.LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 つ目は、2 つ目と同じく  `from` と`import` の 2 つを使って読み込む方法ですが、親モジュールを読み込むのではなく、対象のクラスを直接 `import` する方法です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上記 3 つの方法のうち、どれを用いても構いませんが、コード中で `LinearRegression` クラスを参照するために書かなければならない文字列の長さが変わるため、状況に合わせて使い分けましょう。\n",
    "今回はコードが最も短くなる 3 つ目の書き方を採用します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p8-Fwbao2zp7"
   },
   "source": [
    "初学者は、はじめに下記の 3 つのステップを覚えていきましょう。\n",
    "\n",
    "1. モデルの定義：機械学習アルゴリズム選択し、そのアルゴリズムが実装されたクラスをインスタンス化します。\n",
    "2. モデルの訓練：インスタンス内の属性としてもつパラメータを調整します。\n",
    "3. 精度の検証：訓練済みモデルに対する精度の検証を行います。\n",
    "\n",
    "重回帰分析のアルゴリズムは scikit-learn の中にクラスで定義されており、はじめにインスタンス化を行います。\n",
    "クラスで定義した機能を利用するには、インスタンス化という操作が必要でした。今回はインスタンス化の際に呼び出されるイニシャライザ（`__init__`）は引数を取らないため、クラスの名前に続いて空の丸かっこ `()` を書いてインスタンス化を行っています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QLxsQ11I3OG-"
   },
   "outputs": [],
   "source": [
    "# モデルの定義\n",
    "model = LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fWcEwctR36Dj"
   },
   "source": [
    "一行でしたが、重回帰分析のモデルを定義することができました。\n",
    "\n",
    "それでは、次のステップとして、モデルの訓練を行いましょう。\n",
    "scikit-learn のクラスは訓練の際に `fit()` という関数名でインターフェースが統一されているので覚えておきましょう。\n",
    "インターフェースが統一されているとは、重回帰分析以外の機械学習の手法でも同じ名前の関数名が使われているということです。\n",
    "引数には、入力変数 `x` と教師データ `t` を与え、引数に使用する `x` や `t` は `numpy.ndarray` の形式が標準です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "DJbiWfeg3V1L",
    "outputId": "e0bcfda1-71fa-43b2-852a-bb1b39c9c3d3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/base.py:485: RuntimeWarning: internal gelsd driver lwork query error, required iwork dimension not returned. This is likely the result of LAPACK bug 0038, fixed in LAPACK 3.2.2 (released July 21, 2010). Falling back to 'gelss' driver.\n",
      "  linalg.lstsq(X, y)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,\n",
       "         normalize=False)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# モデルの訓練\n",
    "model.fit(x, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4h-Isref4kCB"
   },
   "source": [
    "訓練に関しても一行で書けてしまうため、実感が湧きにくいですが、これで完了です。\n",
    "\n",
    "モデルの訓練が完了し、パラメータの値が調整されています。\n",
    "それでは、パラメータを確認してみましょう。\n",
    "重回帰分析では、重み `w` とバイアス `b` の２つがパラメータでした。\n",
    "数式を使って説明する場合は、`b` を `w` で包括するように記述しましたが、scikit-learn では重みとバイアスが別の変数に格納されています。\n",
    "重み `w` の確認には `model.coef_`、バイアス `b` の確認には `model.intercept_`をそれぞれ見てみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Q_-9poc-3cft",
    "outputId": "8476bac7-23fc-400f-d8ff-c2d6a6e807b0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.71428571, 0.57142857])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 訓練後のパラメータ w\n",
    "model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.14285714285714057"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 訓練後のバイアス b\n",
    "model.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今回は以前の章で紹介した NumPy での実装を参考にしたため、左の列を `1` で埋めていましたが、scikit-learn の `LinearRegression` では、変数としてバイアス `b`  が別で用意されているため、左の列を`1`で埋める必要がありません。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xの定義\n",
    "x = np.array([\n",
    "    [2, 3],\n",
    "    [2, 5],\n",
    "    [3, 4],\n",
    "    [5, 9],\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルの定義\n",
    "model = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,\n",
       "         normalize=False)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# モデルの訓練\n",
    "model.fit(x, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.71428571, 0.57142857])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 訓練後のパラメータ w\n",
    "model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.14285714285714235"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 訓練後のバイアス b\n",
    "model.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このように、scikit-learn の `LinearRegression` を使用する限りは左の列をバイアスのために `1` で埋めるといった操作は不要であり、考慮すべき負担がひとつ減りました。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hQXB57Gr4xd_"
   },
   "source": [
    "モデルの訓練が完了したら、精度の検証を行います。\n",
    "検証も訓練と同じく `score` という関数名でインターフェースが統一されています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "6uNksu9X3pMF",
    "outputId": "47cff76c-60e6-4a85-ed5d-ebbaddaa0cb8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6923076923076923"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 精度の検証\n",
    "model.score(x, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "さて、検証の結果、数値が返って来ましたが、この結果から善し悪しを測ることができなければ使えません。\n",
    "\n",
    "scikit-learn のモデルが持つ `score` で返ってくる値（デフォルトの設定）の算出方法には、回帰と分類で異なる指標が使われています。\n",
    "\n",
    "回帰の場合、**決定係数**と呼ばれる指標であり、計算式は以下のとおりです。\n",
    "\n",
    "$$\n",
    "R^{2} = 1 - \\dfrac{\\sum_{n=1}^{N_v}\\left( t_{n} - y_{n} \\right)^{2}}{\\sum_{n=1}^{N_v}\\left( t_{n} - \\bar{t} \\right)^{2}}\n",
    "$$\n",
    "\n",
    "ここで、検証に使うデータのサンプル数を $N_v$, $n$  番目のサンプルに対応する予測値を $y_{n}$, 教師データの平均値を $\\bar{t}$ としています。\n",
    "\n",
    "決定係数の最大値は 1 であり、もしこの値が 1 であれば（テストデータに対しては）完璧な予測ができていると言えます。\n",
    "また、0 に近づくと予測としては良い結果とは言えません。\n",
    "0 から 1 の間で判定することが一般的ですが、決定係数の定義としては負の値を取ることもあります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 訓練データとテストデータ\n",
    "\n",
    "モデルの訓練と精度の検証に関する一連の流れを紹介しましたが、ここでひとつ問題があります。\n",
    "その問題点は、今回モデルの訓練と精度の検証に用いるデータセットが同一であったことです。\n",
    "\n",
    "ここで、ひとつ例を挙げて考えてみます。\n",
    "例えば、受験の際に 10 年分の過去問を購入したとしましょう。\n",
    "その手持ちの問題集の中で自分自身の実力を試したい場合に、以下のどちらが適切でしょうか。\n",
    "\n",
    "- 10 年分の過去問を一通り学び、もう一度同じ 10 年分の過去問を実力テスト用に使用する\n",
    "- 5 年分の過去問で学び、残りの答えを知らない 5 年分の過去問を実力テスト用に使用する\n",
    "\n",
    "答えは後者です。\n",
    "前者のように、答えを知っている問題を使用して実力テストを行ったとしても、本当の実力を測ることができません。\n",
    "\n",
    "これは機械学習モデルの訓練と検証でも同じことが当てはまります。\n",
    "実力をつけるために勉強する用の訓練データと、実力を測るためのテストデータは分けるべきであるということです。\n",
    "しかし、上述した一連の流れでは、訓練と検証に同じデータセットを使ってしまっていました。\n",
    "\n",
    "そこで、訓練データとテストデータを分けていきましょう。\n",
    "訓練データとテストデータを分けることを**ホールドアウト法**と呼びます。\n",
    "scikit-learn では、モデルの訓練や検証だけでなく、データセットの分割などを行う関数も用意されており、この関数を利用していきましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データセットを分割するモジュールの読み込み\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 訓練データとテストデータの分割\n",
    "x_train, x_test, t_train, t_test = train_test_split(x, t, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここで、`train_test_split` の引数に `test_size=0.3` を与えています。\n",
    "これはテストデータを全体の 30% と指定することを意味しています。\n",
    "自動的に訓練データは残りの 70% となります。\n",
    "また、`random_state` という引数が指定されていることにも注意が必要です。\n",
    "\n",
    "`train_test_split` では前半  70% と後半 30% のようにデータセットを分割するのではなく、全サンプルの中からランダムに 70% を訓練データとして抽出し、残った 30% をテストデータとして用います。\n",
    "例えば、データセット中のサンプルが、目標値が 1 のサンプルが 10 個、2 のサンプルが 8 個、3 のサンプルが 12個…というように、カテゴリごとにまとめられて並んでいることがあります。\n",
    "その際に、例えばこのデータセットの先頭から 18 個目のところで訓練データとテストデータに分割すると、訓練データには目標値が 3 のデータが 1 つも含まれなくなってしまいます。\n",
    "\n",
    "そこで、ランダムにデータセットを分割する方法が採用されています。\n",
    "NumPy の章でも紹介した通り、乱数を扱う際には再現性が重要であり、その再現性を確保するために、乱数のシードを `random_state` という引数で指定し、固定しています。\n",
    "\n",
    "それでは、分割後の訓練データを用いてモデルの訓練、精度の検証を行いましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,\n",
       "         normalize=False)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# モデルの訓練（訓練データ）\n",
    "model.fit(x_train, t_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-15.999999999999982"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 精度の検証（テストデータ）\n",
    "model.score(x_test, t_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今回はサンプル数が非常に少ないため、テストデータでの予測精度が低く、決定係数の値が負の値を取るような悪い結果となっています。\n",
    "つまり、訓練データで構築したモデルがテストデータに対してうまく当てはまっていないことがわかります。\n",
    "\n",
    "ここで、精度の検証では、訓練データに対する結果も確認しておくことをおすすめします。\n",
    "選択した機械学習アルゴリズムのモデルが訓練データに対してうまく当てはまっていないか、訓練データ自体にはモデルがうまく当てはまっていますが、テストデータに対して訓練済みモデルへの当てはまりが悪いかを判断することができます。\n",
    "この原因を切り分けることで、それぞれ必要な対応を変えることができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 精度の検証（訓練データ）\n",
    "model.score(x_train, t_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "訓練データに対する決定係数は 1 となっており、精度が非常に高く予測できていると言えます。\n",
    "一方で、テストデータに対して精度が低く、このような状況を**過学習 (overfitting)** と呼ばれます。\n",
    "この過学習への対策は scikit-learn の応用編にて解説します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ndGdp3kB4ZPZ"
   },
   "source": [
    "### 推論\n",
    "\n",
    "訓練済みモデルに新たな入力変数を与えることによって、予測値を求めます。\n",
    "scikit-learn では `predict` として関数が定義されています。\n",
    "\n",
    "ひとつ注意すべき点として、推論を行う際の入力となるデータは、訓練時と同様に行列の形である `(サンプル, 入力変数)` を格納して渡す必要があります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e84j6bbh51Wg"
   },
   "outputs": [],
   "source": [
    "# 新たな入力x_newの定義\n",
    "x_new = np.array([[2, 3]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここで、`[2, 3]` とすると、行列の形が `(入力変数)` だけとなり、上述の `(サンプル, 入力変数)` という行列の形になりません。\n",
    "そこで、`[[2,3]]` とすることで 1 サンプルであることを明記できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "zSiL_fmE56W4",
    "outputId": "338cf7e8-98b4-4400-9dba-6dad525f0289"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 予測値の計算\n",
    "y = model.predict(x_new)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zTcwDRhP5913"
   },
   "source": [
    "## scikit-learn（応用編）\n",
    "\n",
    "この節では scikit-learn を用いて、下記の内容について学びます。  \n",
    "\n",
    "- scikit-learn で用意されているデータセットを使用\n",
    "- 重回帰分析以外のアルゴリズムでの実装 \n",
    "- 前処理\n",
    "- パイプライン化\n",
    "- ハイパーパラメータの調整\n",
    "\n",
    "ここで、**前処理**とは、モデルの訓練を行う前に、データセットに対して外れ値除去などの処理を施すことです。\n",
    "外れ値除去以外にも、入力変数を必要なものだけ使用する**変数選択**や、入力変数に対して $\\log$ などの変換を施す**変数変換**、入力変数のスケーリングを行う**正規化**などが代表的です。\n",
    "\n",
    "また、もうひとつ初出である**ハイパーパラメータ**は機械学習では頻出する単語です。\n",
    "重回帰分析では、重み `w` やバイアス `b` のような訓練データによって最適化を行う変数を**パラメータ**と呼んでいました。\n",
    "それに対して、訓練データを用いて最適化を行わないけれど、調整が必要な変数のことを**ハイパーパラメータ** と呼びます。\n",
    "各機械学習アルゴリズムに固有のハイパーパラメータがあります。\n",
    "重回帰分析はハイパーパラメータをアルゴリズム内で持っていませんが、たとえば前処理としてスケーリングを行うか否かは一種のハイパーパラメータと言えます。\n",
    "この節の後半で、具体的なハイパーパラメータとその調整法について紹介していきます。\n",
    "\n",
    "### scikit-learn で用意されているデータセットを使用\n",
    "\n",
    "#### データセットの読み込み\n",
    "\n",
    "まずはじめに、例題で使用するデータセットに関してその内容を紹介します。\n",
    "scikit-learn では、デモ用としていくつかのデータセットが準備されています。\n",
    "今回はその中から、米国ボストン市郊外における地域別の物件価格のデータセットを使用することとします。\n",
    "\n",
    "このデータセットには 506 件のサンプルが存在し、各サンプルには対象地域の平均物件価格と、それに紐づく情報として対象地域の平均的な物件情報、人口統計情報、生活環境に関する情報などが含まれています。  \n",
    "このデータセットを用いて、物件や人口統計などの情報を入力変数として、目標値である平均物件価格を予測するモデルを構築します。\n",
    "入力変数は全部で 13 種類あり、詳細は以下の通りです。\n",
    "\n",
    "- CRIM : 人口 1 人あたりの犯罪発生率\n",
    "- ZN : 25,000 平方フィート以上の住宅区画が占める割合\n",
    "- INDUS : 非小売業が占める面積の割合\n",
    "- CHAS : チャールズ川に関するダミー変数 (1 : 川沿い，0 : それ以外)\n",
    "- NOX : 窒素酸化物の濃度\n",
    "- RM : 住居あたりの平均部屋数\n",
    "- AGE : 1940 年以前に建てられた物件の割合\n",
    "- DIS : 5 つのボストン雇用施設からの重み付き距離\n",
    "- RAD : 都心部の幹線道路へのアクセス指数\n",
    "- TAX : $ 10,000 あたりの固定資産税の割合\n",
    "- PTRATIO : 教師 1 人あたりの生徒数\n",
    "- B : 黒人の比率を表す指数\n",
    "- LSTAT : 低所得者の割合\n",
    "\n",
    "ここで、**ダミー変数**とは、数値ではないデータを数値に変換することで、`CHAS` では川沿いに位置するか否かを `0` もしくは `1` の数値に置き換えています。\n",
    "\n",
    "それでは、あらかじめ scikit-learn で用意されている関数を用いて、データセットを読み込みましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NfPsOvPi_ieo"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9zGvI5vSA0JN"
   },
   "outputs": [],
   "source": [
    "dataset = load_boston()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "変数 `dataset` の `data` という属性に入力変数、`target` という属性に目標値が格納されています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = dataset.data\n",
    "t = dataset.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "bZUbMBbcA7v3",
    "outputId": "c23a7614-6fd1-4d3b-acdd-b560724499d2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.3200e-03, 1.8000e+01, 2.3100e+00, 0.0000e+00, 5.3800e-01,\n",
       "        6.5750e+00, 6.5200e+01, 4.0900e+00, 1.0000e+00, 2.9600e+02,\n",
       "        1.5300e+01, 3.9690e+02, 4.9800e+00],\n",
       "       [2.7310e-02, 0.0000e+00, 7.0700e+00, 0.0000e+00, 4.6900e-01,\n",
       "        6.4210e+00, 7.8900e+01, 4.9671e+00, 2.0000e+00, 2.4200e+02,\n",
       "        1.7800e+01, 3.9690e+02, 9.1400e+00],\n",
       "       [2.7290e-02, 0.0000e+00, 7.0700e+00, 0.0000e+00, 4.6900e-01,\n",
       "        7.1850e+00, 6.1100e+01, 4.9671e+00, 2.0000e+00, 2.4200e+02,\n",
       "        1.7800e+01, 3.9283e+02, 4.0300e+00]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 入力変数xの確認（先頭の３件）\n",
    "x[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "WtvKs6p9BdIC",
    "outputId": "94ad2f68-99f7-4cb5-f7db-3e5c46988061"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(506, 13)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# xの形の確認（行, 列）\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "4K5l3E3WBDdJ",
    "outputId": "280c2579-527a-443c-cf1f-f5e06d0b1af3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([24. , 21.6, 34.7])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 目的値tの確認（先頭の３件）\n",
    "t[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "xSmuD0vkBf1q",
    "outputId": "b9595b93-7ead-4009-ab30-118ff9ed8fcc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(506,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tの形の確認\n",
    "t.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jRkdOI5eBLX_"
   },
   "source": [
    "#### 訓練データ（Train）、テストデータ（Test）に分割\n",
    "\n",
    "前述したホールドアウト法を使用して検証を行うため、訓練データとテストデータに分割していきます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6pshMYGuBocn"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z_AL1uzGBotE"
   },
   "outputs": [],
   "source": [
    "x_train, x_test, t_train, t_test = train_test_split(x, t, train_size=0.7, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "02XnVLgVCYwu"
   },
   "source": [
    "#### 重回帰分析\n",
    "\n",
    "新しいアルゴリズムを試す前に、本章の前半で学んだ重回帰分析をまず適用してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BgmsizPKCY3V"
   },
   "outputs": [],
   "source": [
    "# モデルの定義\n",
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "crtijJ3kCY8l",
    "outputId": "34ba8395-8215-45ca-d344-1ad7d3445f78"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,\n",
       "         normalize=False)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# モデルの訓練\n",
    "model.fit(x_train, t_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sSoaPFq2DheR"
   },
   "source": [
    "ホールドアウト法では訓練データによりモデルの訓練を行うことに再度、気をつけましょう。\n",
    "\n",
    "訓練データとテストデータに対して、精度の検証を行います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "AvyBKGLQCZB2",
    "outputId": "2726973d-f13b-467d-ed2b-d4fd0877631a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7905724291908702"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# モデルの検証（訓練データ）\n",
    "model.score(x_train, t_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "887mZKOwCZG5",
    "outputId": "1a705174-dcbd-46b7-baf0-7d5ab3ce2627"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6671893494370862"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# モデルの検証（テストデータ）\n",
    "model.score(x_test, t_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "crnFPaziCZMk"
   },
   "source": [
    "この結果より、今回は特に大きなオーバーフィッティングは起こしていないことがわかります。\n",
    "\n",
    "補足として、モデルが訓練データに対してすら良い精度で予測できない状態を**未学習 (underfitting)** といいます。  \n",
    "アンダーフィッティングが起きている場合、現状の機械学習アルゴリズムがデータの特徴を捉えるには不十分である可能性があります。\n",
    "その場合は、アルゴリズムの変更や、入力データの特徴をより適切に表現できるような変換を導入するなどして、改善を試みます。  \n",
    "\n",
    "逆にオーバーフィッティング（過学習）の場合、アルゴリズムでデータの特徴をある程度捉えられていることは確認できているので、モデルが過学習しないように対策していきます。  \n",
    "代表的な方法としては、前述した**ハイパーパラメータ**を調整していくことで解決できる場合があります。  \n",
    "具体的な調整についてはこれから紹介します。\n",
    "\n",
    "このように、望ましい結果が得られないといっても、それぞれの状況を把握することで次に打つべき対策が変わってくるため、訓練データとテストデータの両方に対する検証を行うことは重要であることが分かります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6IWqtYWwCZR9"
   },
   "source": [
    "### 重回帰分析以外のアルゴリズムでの実装\n",
    "\n",
    "**サポートベクターマシン (SVM: Support Vector Machine)** は実務でもよく使われる機械学習アルゴリズムのひとつです。\n",
    "背景の数学の詳細な解説はここでは省略しますが、下図のように入力変数と目標値の間の関係が**線形**でないことがあります。\n",
    "線形という新しい用語が登場し、厳密性には欠けますが、ひとまず入出力間の関係が直線で表現できることを線形と言うのだと捉えてください。\n",
    "単回帰分析を含めた重回帰分析では、入出力間の関係が線形な場合しかうまく関係性を捉えることができません。\n",
    "特徴量として $x^{2}$ や $\\sin {x}$ など非線形な関数を採用することも考えられますが、複数の特徴量に対しての組み合わせは無限に存在し、その背景にある物理的な現象を正しく理解できている状況以外、現実的な選択とは言えません。\n",
    "\n",
    "そこで、SVM では、**特徴関数** $\\phi$ という**特徴空間へ変換**する関数を用います。\n",
    "入力変数 $x_{i}$ を特徴空間へ非線形変換 $\\phi(x_{i})$し、変換後の特徴空間において線形回帰を行うという工夫を行います。\n",
    "実際には入力変数を変換する特徴関数を明示的に決定せずに、特徴空間での内積（2 つの入力変数の特徴空間上での距離に相当）を直接求めることができる**カーネル関数** $k$ を用いるため、このアプローチは**カーネルトリック**と呼ばれます。\n",
    "\n",
    "![SVRイメージ](images/06/06_01.png)\n",
    "\n",
    "SVM は数学的に重回帰分析よりも遥かに難易度が上がりますが、scikit-learn では重回帰分析と同様に、あらかじめ用意された関数を使っていくだけでとても簡単に実装できます。\n",
    "scikit-learn での SVM の詳細についてはこちらの[公式ドキュメント](https://scikit-learn.org/stable/modules/svm.html#svr)が便利です。\n",
    "\n",
    "SVM は回帰と分類の両方に対応しており、回帰の場合は **SVR (Support Vector Regression)**, 分類の場合は **SVC (Support Vector Classification)** という名前で scikit-learn 内では定義されています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HYunZTN5Frh3"
   },
   "outputs": [],
   "source": [
    "# モデルの定義\n",
    "from sklearn.svm import SVR\n",
    "model = SVR() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "iJtAk1o5Frh5",
    "outputId": "8ebc6db8-2a91-4cb3-dc70-18508fa766dc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1,\n",
       "  gamma='auto_deprecated', kernel='rbf', max_iter=-1, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# モデルの訓練\n",
    "model.fit(x_train, t_train) # 訓練データを使ってモデルの学習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここで、ひとつ注意したいのは、ここでは `SVR` を引数なしでインスタンス化しているため、すべての引数に対して `C=1.0` や `epsilon=0.1` のようなデフォルトの値が設定されていることです。\n",
    "最初はこれらの値を指定することなく進めますが、当然これらの値にも意味が存在しています。\n",
    "\n",
    "最適化手法を使用して調整する変数を**パラメータ**と呼び、上述の `C` のように機械学習アルゴリズムの挙動を制御するパラメータのことを**ハイパーパラメータ**と呼びます。\n",
    "`fit` の関数ではパラメータを調整することができますが、その挙動を制御するハイパーパラメータの調整まで行うことはできません。\n",
    "ハイパーパラメータの調整に関しては後述します。\n",
    "\n",
    "まずはデフォルトのハイパーパラメータを用いて、パラメータの最適化を行った場合の精度を検証してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "bjtcTTkiFrh8",
    "outputId": "f9f866f5-6d57-4721-b1d3-577cce709d54"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1228199964193637"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 精度の検証（訓練データ）\n",
    "model.score(x_train, t_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Ryii-vvbFrh_",
    "outputId": "f304dc45-c6ad-4a53-a948-31b1ba56cc80"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0003209859005506299"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 精度の検証（テストデータ）\n",
    "model.score(x_test, t_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dwpv2FxdFSYi"
   },
   "source": [
    "この結果より、訓練データとテストデータともに重回帰分析よりも精度が低い結果となりました。\n",
    "本来、線形回帰の手法の問題点を解決しているはずの SVM でしたが、望ましい精度が得られていません。\n",
    "\n",
    "この問題点として、以下の 2 つをよく考えます。\n",
    "\n",
    "- **前処理**を適切に行えているか\n",
    "- **ハイパーパラメータ**が適切に調整できているか\n",
    "\n",
    "### 前処理\n",
    "\n",
    "**前処理 (preprocessing)** とは、欠損値や外れ値の除去・補完から、変数変換、特徴量選択、正規化といった処理を施すことを指します。\n",
    "それぞれのアルゴリズムに合わせた前処理が必要となり、そのためにアルゴリズムの特性を知っておく必要があります。\n",
    "\n",
    "それでは、SVR に対する精度向上のための前処理を考えていきましょう。\n",
    "SVR ではカーネルトリックの際に**距離**を使用することが一般的であり、基礎数学の章で解説したとおり、入力変数間のスケールが統一されていない場合には大きなスケールの変数に影響されてしまいます。\n",
    "**距離**をアルゴリズム内で使用する場合、**正規化**と呼ばれるスケールを統一する処理を施すことがこの問題の対策として考えれます。\n",
    "ディープラーニングでも同様に正規化を施す場合があります。[<sup>*2</sup>](#fn2)  。\n",
    "\n",
    "<span id=\"fn2\"><sup>*2</sup>：<small>ディープラーニングでよく用いられる正規化は[Batch Normalization](Batch Normalizationのリンクの挿入)があります。</small></span>\n",
    "\n",
    "前処理に関しても、scikit-learn にいくつかの関数があらかじめ用意されています。\n",
    "正規化にも、平均 0, 標準偏差 1 に変換する `StandardScaler` と、最小値 0, 最大値 1 に変換する `MinMaxScaler` があります。\n",
    "今回は `StandardScaler` を使用していきましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GLQNMom0CZXA"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "モデルの定義と同様に、インスタンス化を行います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "この `scaler` では、正規化を行うために、平均と標準偏差の値が必要となります。\n",
    "この値を算出するために、モデルの場合と同様に `fit` を用います。\n",
    "このときに注意すべき点として、すべてのサンプルではなく、訓練データを用いてこれらの値を算出することです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "YROpwqRhCZca",
    "outputId": "f1899b3f-7d31-42cf-ae66-eb7c17fd4bad"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StandardScaler(copy=True, with_mean=True, with_std=True)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler.fit(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`fit` によって算出された値が scaler の属性として格納されていることが確認できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.27599073e+00, 1.20231788e+01, 1.10447682e+01, 6.62251656e-02,\n",
       "       5.56298013e-01, 6.33063576e+00, 6.96927152e+01, 3.66313642e+00,\n",
       "       9.83443709e+00, 4.10609272e+02, 1.84635762e+01, 3.65675828e+02,\n",
       "       1.27405298e+01])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 平均\n",
    "scaler.mean_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.98109942e+01, 5.74266019e+02, 5.40302117e+01, 6.18393930e-02,\n",
       "       1.35768756e-02, 5.04099265e-01, 8.20035775e+02, 3.58008203e+00,\n",
       "       8.16480856e+01, 3.33497480e+04, 4.80112363e+00, 6.30709364e+03,\n",
       "       5.53191295e+01])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 分散\n",
    "scaler.var_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "標準偏差ではなく分散が属性として格納されていますが、これは分散が 0 であった場合の対応などを変換する際の関数で定義しているためですが、そこまでの詳細は現時点で気にする必要はありません（[参考: github](https://github.com/automl/paramsklearn/blob/master/ParamSklearn/implementations/StandardScaler.py)）。\n",
    "\n",
    "正規化を施す際には `transform` を使用します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1sz88ORaLw3_"
   },
   "outputs": [],
   "source": [
    "x_train_scaled = scaler.transform(x_train)\n",
    "x_test_scaled  = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "kdGJ0lNnLxJY",
    "outputId": "222d4a2f-8236-44e4-ddba-569183888a9f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1,\n",
       "  gamma='auto_deprecated', kernel='rbf', max_iter=-1, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# モデルの訓練\n",
    "model.fit(x_train_scaled, t_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "70X7rqb3LxW3",
    "outputId": "ff397a9b-c5e6-44a5-c91b-b09c20976d93"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5297457196085098"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 精度の検証（訓練データ）\n",
    "model.score(x_train_scaled, t_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5041470644131514"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 精度の検証（テストデータ）\n",
    "model.score(x_test_scaled, t_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "この結果より、正規化を事前に施すことにより、精度を大幅に向上させることができました。\n",
    "重回帰分析の方が精度が高かったため、まだもう一段階工夫が必要となりますが、前処理の有効性が示せました。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### パイプライン化\n",
    "\n",
    "前処理用の `scaler` と SVR の `model` をそれぞれに訓練していましたが、scikit-learn にはパイプラインと呼ばれる一連の処理を統合できる機能があります。\n",
    "前述の処理をまとめていきましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# パイプラインの作成 (scaler -> svr)\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('svr', SVR())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('svr', SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1,\n",
       "  gamma='auto_deprecated', kernel='rbf', max_iter=-1, shrinking=True,\n",
       "  tol=0.001, verbose=False))])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 一連の流れでモデルの訓練\n",
    "pipeline.fit(x_train, t_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5297457196085098"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# モデルの検証（訓練データ）\n",
    "pipeline.score(x_train, t_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5041470644131514"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# モデルの検証（テストデータ）\n",
    "pipeline.score(x_test, t_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このようにパイプライン化させることで、`x_train_scaled` のような間の変数を挟むことが必要なくなりました。\n",
    "テストデータの正規化を忘れてしまうことがよくあるため、人的ミスを防ぐためにもパイプライン化は有効な手段といえます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ハイパーパラメータの調整\n",
    "\n",
    "前述したとおり、SVM などアルゴリズムを制御するためのパラメータをハイパーパラメータと呼びます。\n",
    "ハイパーパラメータは `fit` の関数では訓練することはできません。\n",
    "\n",
    "そこで、**グリッドサーチ**と呼ばれる方法では、下図のようにハイパーパラメータ (ex. $C$, $\\gamma$) の候補を格子状に切り、それぞれの値で確かめていく方法が有効です。\n",
    "この方法以外にも、**ランダムサーチ**や**ベイズ最適化**を用いた方法もあるため、興味のある人は調べてみてください。\n",
    "今回は、グリッドサーチによる実装を紹介していきます。\n",
    "\n",
    "![](images/06/06_02.png)\n",
    "\n",
    "訓練データは目的関数を最適化するようにパラメータを調整しますが、ハイパーパラメータを調整するためのデータではありません。\n",
    "そのため、ハイパーパラメータを調整するためのデータも別で必要となり、これを**検証データ (validation data)** と呼びます。\n",
    "つまり、ハイパーパラメータを持つアルゴリズムの場合、訓練データ・検証データ・テストデータの 3 つをそれぞれ用意します。\n",
    "\n",
    "特に、ハイパーパラメータの調整を行う場合には **交差検証 (Cross Validation)** と呼ばれる、下記の図のようにデータを交差させる方法がよく用いられ、**$K$-分割交差検証** と呼ばれます。\n",
    "下図は $K=5$ の場合であり、もともと訓練データであったものを 5 分割し、4 つは訓練データとしてパラメータを調整し、残りの1 つを検証データとして目的関数の値を算出します。\n",
    "これをすべての組み合わせで行い、最終的にその算出された目的関数の平均を検証結果の値とします。\n",
    "訓練データの全サンプルを検証用にも使用できるため、検証用データが恣意的にもしくは恣意的でなくても偏ってしまう問題を解決できます。\n",
    "\n",
    "また、データセットのサンプル数が少ない場合にも有効です。\n",
    "訓練データとテストデータを分割して訓練データが減ってしまい、さらに訓練データを検証データと分割すると、パラメータの調整に使用できるサンプル数が極端に少ない状況が考えられます。\n",
    "$K$-分割交差検証であれば、すべての訓練データを訓練と検証に用いることができます。\n",
    "また、検証データのサンプル数を 1 とすることもあり、訓練データのサンプル数を $N_t$ とすると、$N_{t} -1$ サンプルでモデルを訓練し、1 サンプルで検証を行います。\n",
    "これを $N_t$ 回繰り返し、その平均を算出します。\n",
    "このように、検証データのサンプル数を 1 とする場合は、**Leave-one-out 交差検証 (LOOCV)** と呼ばれます。\n",
    "\n",
    "![](images/06/06_01.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "交差検証で求めた目的関数の値をグリッドサーチの指標として利用することで、最良なハイパーパラメータの組み合わせを見つけることができます。\n",
    "これを scikit-learn では `GridSearchCV` として準備されており、手軽にハイパーパラメータの調整を行うことができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# グリッドサーチを行うハイパーパラメータの候補を定義\n",
    "# Pipelineを使う場合は <処理の名前>__<パラメータ名>: [...]\n",
    "# Pipelineを使わない場合は <パラメータ名>: [...]\n",
    "params = [\n",
    "    {'svr__C': [1, 10, 100, 1000], 'svr__gamma': [0.001, 0.01, 0.1, 1]}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "モデルとしては、前回パイプライン化を行った前処理を含めた SVR である `pipeline` を使用し、`estimator` という引数に指定します。\n",
    "ハイパーパラメータの候補は `param_grid` に、分割の数 $K$ は 5 として、`cv` という引数に指定します。\n",
    "また、回帰の際に見落としがちですが、目的関数を指定する `scoring` を `neg_mean_squared_error` としておきましょう。\n",
    "`neg` は negative のことであり、本来は `mean_squared_error` を使用するところですが、scikit-learn の `GridSearchCV` では最大化を行うようにアルゴリズムが実装されています。\n",
    "そのため、最大化を行うアルゴリズムで最小化を行うために、`neg` が付いた目的関数を使用しています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 交差検証 + グリッドサーチを行うモデルを定義\n",
    "model_grid = GridSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_grid=params,\n",
    "    cv=5,\n",
    "    scoring='neg_mean_squared_error'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('svr', SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1,\n",
       "  gamma='auto_deprecated', kernel='rbf', max_iter=-1, shrinking=True,\n",
       "  tol=0.001, verbose=False))]),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid=[{'svr__C': [1, 10, 100, 1000], 'svr__gamma': [0.001, 0.01, 0.1, 1]}],\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='neg_mean_squared_error', verbose=0)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# モデルの訓練\n",
    "model_grid.fit(x_train, t_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "グリッドサーチも含めた訓練が終わると、最終的に最も良かったパラメータの組み合わせを `best_params_` で確認することができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'svr__C': 1000, 'svr__gamma': 0.01}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_grid.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "そして、最良なハイパーパラメータの組み合わせを持つモデルが `best_estimator` で取り出すことができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_grid.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最後に、このハイパーパラメータも調整も行ったモデルでの精度も検証しましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9560597770245561"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 精度の検証（訓練データ）\n",
    "model.score(x_train, t_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7618184466251714"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 精度の検証（テストデータ）\n",
    "model.score(x_test, t_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-bQPeupGLxj6"
   },
   "source": [
    "この結果より、ハイパーパラメータの調整によりさらに予測精度を向上させることができ、その調整による有効性を示せました。\n",
    "データの前処理とハイパーパラメータの調整は基本的にいつも行うため、覚えておきましょう。\n",
    "ハイパーパラメータの名前や値の候補の与え方には数学的な背景の理解や経験が必要となります。\n",
    "scikit-learn の公式ページに[グリッドサーチによるハイパーパラメータの調整](https://scikit-learn.org/stable/modules/grid_search.html)に関する例が紹介されているほか、最近では [Optuna](https://github.com/pfnet/optuna/tree/master/examples) というハイパーパラメータを最適化するためのフレームワークも登場しています。\n",
    "\n",
    "scikit-learn ではその他にも**決定木 (decision tree)** や **主成分分析 (principal component analysis)** など様々な手法が既に実装されています。\n",
    "使い方は今回の一連の手順とほとんど同じであるため、色々な方法を試してみましょう。\n",
    "用意されているアルゴリズムは[公式ページ](https://scikit-learn.org/stable/)から確認することができます。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
