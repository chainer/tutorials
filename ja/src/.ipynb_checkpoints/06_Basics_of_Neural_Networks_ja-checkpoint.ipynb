{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Da9LzFk5G38W"
   },
   "source": [
    "# ニューラルネットワークの基礎\n",
    "\n",
    "本章では、**ニューラルネットワーク（Neural Network）**の基本的な仕組みを説明します。\n",
    "\n",
    "まず、**全結合型（fully-connected）**と呼ばれるニューラルネットワークの構造を説明したあと、入力データと望ましい出力の組が複数与えられたとき、どうやってニューラルネットワークに新しい入力に対して望ましい出力を予測させるよう訓練を行えばよいのか（**教師あり学習（supervised learning）**の仕組み）について説明します。\n",
    "\n",
    "ニューラルネットワークは、任意の連続関数を任意の精度で近似できることが証明されています[<sup>*1</sup>](#fn1)。しかし、理論的には近似できるはずだからといって、目的の関数を近似するニューラルネットワークのパラメータを見つけることが容易であるとは限りません。本章では、複雑な関数を近似するニューラルネットワークのパラメータを現実的な時間内で求めるための最適化手法の一つである**誤差逆伝播法（backpropagation）**と呼ばれるアルゴリズムについても説明します。\n",
    "\n",
    "<span id=\"#fn1\"><sup>*1</sup></span>: <small>これを**普遍性定理（universal approximation theorem）**\n",
    "と呼びます。Michael Nielsen氏によって書かれ、村主崇行氏によって翻訳された普遍性定理の視覚的で分かりやすい解説資料がこちらにあります。：[ニューラルネットワークが任意の関数を表現できることの視覚的証明](https://nnadl-ja.github.io/nnadl_site_ja/chap4.html)</small>興味のある方はぜひ参照してください。\n",
    "\n",
    "はじめに、**ニューラルネットワークをブラックボックスとして扱うのではなく**、一つ一つ内部で行われる計算を丁寧に調べます。そして、パラメータで特徴づけられた関数で表される線形変換と、それに続く非線形変換を組み合わせて、**全体として微分可能な一つの関数を表している**ことを理解しましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LICMyO4rKUX9"
   },
   "source": [
    "## ニューラルネットワークの構造\n",
    "\n",
    "まずはニューラルネットワークの構造を表した下の図を見てください。\n",
    "重回帰分析では、従属変数は 1 つの連続値でした。\n",
    "下の図では、ニューラルネットワークが出力する値は 2 個あります。\n",
    "それぞれ、「白ワイン」と「赤ワイン」という説明が付与されています。\n",
    "ここでやろうとしていることは、重回帰分析における独立変数に対応するようないくつかのワインについての情報（年数、アルコール度数、色合い、匂い）を**入力変数（input variable）**として、そのワインが赤ワインであるか、白ワインであるかを予測するような**分類問題（classification problem）**を解こうとしています。\n",
    "このとき、「赤ワイン」「白ワイン」のような、入力変数が分類される対象となる概念を**クラス（class）**と言います。\n",
    "\n",
    "![ニューラルネットワークの基本構造](images/6/01.png)\n",
    "\n",
    "この図に登場する丸い円を **ノード（node）** もしくは **ユニット（unit）** と呼び、その縦方向の集まりを**層（layer）**と呼びます。\n",
    "そして、一番初めの層を**入力層（input layer）**、最後の層を**出力層（output layer）**、その間を**中間層（intermediate layer）**もしくは**隠れ層（hidden layer）**と呼びます。\n",
    "上の図のニューラルネットワークは、入力層、中間層、出力層の 3 層を持つ構造となっていますが、中間層の数を増やすことでさらに多層のニューラルネットワークを考えることができます。\n",
    "この例では各層間の全ノードが互いに全て結合しているため、**全結合型のニューラルネットワーク（fully-connected neural network）**とも呼び、ニューラルネットワークの最も基本的な構造です。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "meyqKCr-NUDs"
   },
   "source": [
    "![ニューラルネットワークの出力値](images/6/02.png)\n",
    "\n",
    "まず、最終層にどのような値が入るのか、具体例を見てみましょう。\n",
    "例えば、年数が 3 年物でアルコール度数が 14 度、色合いが 0.2、匂いが 0.8 で表されるワインがあるとします。\n",
    "内部の計算は後述するとして、このようなデータをニューラルネットワークに与えたときに結果として得られる値に注目します。\n",
    "上図では、$y_1 = 0.15$、$y_2= 0.85$ となっています。\n",
    "このとき、出力値の中で最も大きな値となっている変数に対応するもの、すなわち今回の例では「赤ワイン」をこの分類問題におけるこのニューラルネットワークの**予測結果（prediction）**とすることができます。\n",
    "\n",
    "ここで出力層のすべての値を合計してみると、$1$ になっていることに気づきます。これは偶然ではなく、そうなるように出力層の値を計算しているためです[<sup>*1</sup>](#fn1)。つまり、出力層のそれぞれのノードが持つ数値は、入力変数の値によって特徴づけられるようなワインが、赤ワイン・白ワインというクラスのそれぞれに属している確率を表していたのでした。そのため、クラス数と同じ数のノードが出力層にあります。各クラスごとの予測値をまとめて出力層を一つのベクトルと見た場合、それを出力ベクトルと呼ぶことがあります。また、入力変数を並べてできるベクトルを入力ベクトルと呼ぶことがあります。\n",
    "\n",
    "<span id=\"fn1\"><sup>*1</sup>：<small>具体的には、**ソフトマックス関数（softmax function）**という活性化関数（後述します）をニューラルネットワークの出力ベクトルに適用することで、出力層における全ノードの値の合計が1になるようにします。</small></span>\n",
    "\n",
    "それでは、ここからニューラルネットワークの内部で行われる計算を詳しく見ていきましょう。ニューラルネットワークの各層は、前の層の値に線形変換と非線形変換を順番に施すことで計算されます。まずは、ここで言う線形変換とは何を表すのか、から説明します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X9-RYTlXNzNQ"
   },
   "source": [
    "### 線形変換\n",
    "\n",
    "ここでは、ニューラルネットワークの各層で行われる線形変換について説明します。\n",
    "\n",
    "<img src=\"images/6/03.png\" width=\"400px\" />\n",
    "\n",
    "線形変換[<sup>*2</sup>](#fn2)とは、入力ベクトルを ${\\bf h}$ としたとき、重み行列 ${\\bf W}$ と、バイアスベクトル ${\\bf b}$ と呼ばれる 2 つのパラメータを使って以下のような変換を行うことを言います。\n",
    "\n",
    "$$\n",
    "{\\bf y} = {\\bf W}{\\bf x} + {\\bf b}\n",
    "$$\n",
    "\n",
    "<span id=\"fn2\"><sup>*2</sup>：<small>通常数学では線形変換とは ${\\bf W}{\\bf h}$ のことを指し、この変換は厳密には「アファイン変換（もしくは アフィン変換）」と呼ばれるものです。しかし、深層学習の文脈ではこの変換も線形変換と呼ばれることが多いです。</small></span>\n",
    "\n",
    "ここで、入力ベクトルが $h$ という文字で表されていますが、これは隠れ層 (hidden layer) の頭文字である h から来ています。\n",
    "入力ベクトルを 0 層目の隠れ層、と考えることもできるためです。\n",
    "これによって表記を一般化しやすくなります。\n",
    "ここでは、入力層の値（上図における $x_1, x_2, x_3, x_4$）を $h_{01}, h_{02}, h_{03}, h_{04}$ と表記します。\n",
    "重み行列 ${\\bf W}$ は $3 \\times 4$ 行列で、i 行 j 列の要素を $w_{ij}$ と表すことにします。\n",
    "バイアスベクトル ${\\bf b}$ は 3 次元ベクトルで、${\\bf b} = [b_1, b_2, b_3]^{\\rm T}$ です。\n",
    "このとき、出力層の各ノードの値（$u_{11}, u_{12}, u_{13}$）は、以下のように計算されます。\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "u_{11} &= w_{11}h_{01} + w_{12}h_{02} + w_{13}h_{03} + w_{14}h_{04}+b_{1} \\\\\n",
    "u_{12} &= w_{21}h_{01} + w_{22}h_{02} + w_{23}h_{03} + w_{24}h_{04}+b_{2} \\\\\n",
    "u_{13} &= w_{31}h_{01} + w_{32}h_{02} + w_{33}h_{03} + w_{34}h_{04}+b_{3}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "バイアス（$b_1, b_2, b_3$）は上図では省略されていることに注意してください。\n",
    "以上の4つの式は、ベクトルと行列の計算として以下のように書き直すことができます。\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\begin{bmatrix}\n",
    "u_{11} \\\\\n",
    "u_{12} \\\\\n",
    "u_{13}\n",
    "\\end{bmatrix}&=\\begin{bmatrix}\n",
    "w_{11} & w_{12} & w_{13} & w_{14} \\\\\n",
    "w_{21} & w_{22} & w_{23} & w_{24} \\\\\n",
    "w_{31} & w_{32} & w_{33} & w_{34}\n",
    "\\end{bmatrix}\\begin{bmatrix}\n",
    "h_{01} \\\\\n",
    "h_{02} \\\\\n",
    "h_{03} \\\\\n",
    "h_{04}\n",
    "\\end{bmatrix}+\\begin{bmatrix}\n",
    "b_{1} \\\\\n",
    "b_{2} \\\\\n",
    "b_{3}\n",
    "\\end{bmatrix}\\\\\n",
    "{\\bf u}_{1}&={\\bf W}{\\bf h}_{0}+{\\bf b}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "本来は ${\\bf W}$ や ${\\bf b}$ にも、どの層とどの層の間の計算に用いられるものなのかを表す添え字をつけるべき（例えば 0 層目（入力層）と 1 層目の間の線形変換に用いられる重み行列を ${\\bf W}_{10}$ と書く、など）ですがここでは簡単のため省略しています。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JmlTx1YF4xMD"
   },
   "source": [
    "### 非線形変換\n",
    "\n",
    "次に、非線形変換について説明します。\n",
    "線形変換のみでは、下の図の右のグラフのように入力と出力の間が非線形な関係である場合、両者の間の関係を適切に表現することができません。\n",
    "\n",
    "![入力と出力の関係](images/6/04.png)\n",
    "\n",
    "そこで、ニューラルネットワークでは各層で線形変換に引き続いて非線形変換を施すことで、全体の関数が非線形性を持つことができるようにしています。\n",
    "この非線形変換を行う関数を、ニューラルネットワークの文脈では **活性化関数（activation function）** と呼びます。\n",
    "\n",
    "上の図の線形変換の結果 $u_{11}, u_{12}, u_{13}$ に活性化関数を使って非線形変換を行った結果を $h_{11}, h_{12}, h_{13}$ と書き、これらを**活性値（activation）**と呼びます（下図参照）。\n",
    "これが次の層への入力となります。\n",
    "\n",
    "<img src=\"images/6/05.png\" width=\"400px\" />\n",
    "\n",
    "活性化関数の具体例としては、下図に示す **ロジスティックシグモイド関数（logistic sigmoid function）**（以下シグモイド関数）\n",
    "\n",
    "![シグモイド関数](images/6/06.png)\n",
    "\n",
    "が従来、よく用いられてきました。\n",
    "しかし近年、層数が多いニューラルネットワークではシグモイド関数は活性化関数としてほとんど用いられません。\n",
    "その理由の一つは、シグモイド関数を活性化関数に採用すると**勾配消失（vanishing gradient）**という現象が起きやすくなり、学習が進行しなくなる問題が発生することがあるためです。\n",
    "これは後で詳述します。\n",
    "この問題を回避するために、**正規化線形関数（rectified linear unit, ReLU）**がよく用いられています。\n",
    "これは、以下のような形をした関数です。\n",
    "\n",
    "![ReLU関数](images/6/07.png)\n",
    "\n",
    "ここで、 $\\max(0, u)$ は、$0$ と $u$ を比べて大きな方を返す関数です。\n",
    "すなわち、ReLU は入力が負の値の場合には出力は 0 で一定であり、正の値の場合は入力をそのまま出力するという関数です。\n",
    "シグモイド関数では、入力が 0 から離れた値をとると、勾配がどんどん小さくなってしまうだろうということが上の図からも見て取れます。\n",
    "それに対し、ReLU 関数は入力の値がいくら大きくなっても、一定の勾配が発生します。\n",
    "これが後ほど紹介する勾配消失という問題に有効に働きます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s-eFDAuJ-Cse"
   },
   "source": [
    "### 数値を見ながら計算の流れを確認\n",
    "\n",
    "ここで、下の図に書き込まれた具体的な数値を使って、入力 $x_1, x_2, x_3$ から出力 $y$ が計算される過程を確認してみましょう。\n",
    "今は計算を簡略化するためバイアス ${\\bf b}$ の計算は省略します（バイアスが全て 0 であるとします）。\n",
    "数値例として、${\\bf x} = \\begin{bmatrix} 2 & 3 & 1 \\end{bmatrix}^T$ が与えられた時の出力 $y$ の計算手順を一つ一つ追いかけてみましょう。\n",
    "\n",
    "<img src=\"images/6/08.png\" width=\"400px\" />\n",
    "\n",
    "別の章で解説した重回帰分析では、目的関数のパラメータについての導関数を 0 とおいて解析的に最適なパラメータを解くことができました。\n",
    "しかし、ニューラルネットワークでは一般的に、解析的にパラメータを解くことはできません。\n",
    "その代わり、目的関数のパラメータに関する導関数を求めておき、各入力値における勾配を利用して逐次的にパラメータを最適化していく方法を用います。\n",
    "\n",
    "そのため、ニューラルネットワークでは、**まずパラメータを乱数で初期化し、ひとまずデータを入力して目的関数の値を計算することができるようにします**。その後、その時点の関数の勾配を計算し、これを利用してパラメータを更新し、その更新後の新しいパラメータを使って再度別の入力データを処理して目的関数の値を計算し…というプロセスを繰り返します。\n",
    "\n",
    "今、パラメータを乱数で初期化した結果、上の図のノード間を繋ぐ線（エッジ）に与えられているような数値が重み行列にセットされた状態を考えます。\n",
    "この値を用いて、入力層の値に線形変換を施すところまでの計算を行ってみましょう。\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "u_{11} &= 3 \\times 2 + 1 \\times 3 + 2 \\times 1 = 11 \\\\\n",
    "u_{12} &= -2 \\times 2 - 3 \\times 3 - 1 \\times 1 = -11\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "次に、非線形変換を行う活性化関数として ReLU 関数を採用し、以下のように中間層の値を計算してみましょう。\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "h_{11} &= \\max(0, 11) = 11 \\\\\n",
    "h_{12} &= \\max(0, -11)  = 0\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "同様に、出力層の $y$ の値までを計算すると、\n",
    "\n",
    "$$\n",
    "y = 3 \\times 11 + 2 \\times 0 = 33\n",
    "$$\n",
    "\n",
    "となります。\n",
    "\n",
    "次節からは、出力層の値を計算したあと、その結果を使ってどのようにパラメータを更新していくかを説明します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ivVppUOibSHy"
   },
   "source": [
    "## 目的関数\n",
    "\n",
    "パラメータの更新方法を考えるためには、そのパラメータがどういう値になっていることが望ましいのか、を定める必要があります。\n",
    "基本的には、ある目的関数を定義し、その値を最小にするようなパラメータが望ましいものであると定めます。\n",
    "ここでは、目的関数として利用できるいくつかの関数を紹介します。\n",
    "ニューラルネットワークでは、微分可能でさえあれば解きたいタスクに合わせて様々な目的関数を利用することができます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BCGZbTKcbUpe"
   },
   "source": [
    "### 平均二乗誤差（mean squared error）\n",
    "\n",
    "出力層に $N$ 個の値を持つニューラルネットワークで回帰問題を解く場合を考えてみましょう。\n",
    "$N$ 個の出力それぞれ（$y_n \\ (n=1, 2, \\dots, N)$）に対して望ましい出力（$t_n \\ (n=1, 2, \\dots, N)$）が与えられたとき、目的関数をそれぞれの出力（$y_n$）と対応する正解（$t_n$）の間の**平均二乗誤差（mean squared error）**とすることで、回帰問題を解くことができます。\n",
    "これは回帰問題を解説した章で用いた二乗和誤差と似ていますが、最後にデータ数で割ることで平均を計算している点が異なります。\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = \\frac{1}{N} \\sum_{n=1}^N (t_n - y_n)^2\n",
    "$$\n",
    "\n",
    "ニューラルネットワークで回帰問題を解く一つの方法は、この目的関数を最小にするようにニューラルネットワーク中のパラメータを決定することとなります。\n",
    "例えば、上の図の例で正解として $t = 20$ が与えられたときの目的関数の値は、\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = \\frac{1}{1} (20 - 33)^2 = 169\n",
    "$$\n",
    "\n",
    "です。これを小さくするような重み行列の値を探せばよいということです。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WqFc0jYJFLYz"
   },
   "source": [
    "### 交差エントロピー\n",
    "\n",
    "一方、分類問題の場合はしばしば**交差エントロピー（cross entropy）**が目的関数として利用されます。\n",
    "\n",
    "例として、$N$ クラスの分類問題を考えてみましょう。\n",
    "ある入力 $x$ が与えられたとき、ニューラルネットワークの出力層に $N$ 個のノードがあり、それぞれがこの入力が $n$ 番目のクラスに属する確率 $y_n = p(y=n|x)$ を表しているとします。\n",
    "これは、入力 $x$ が与えられたという**条件のもとで**、予測クラスを意味する $y$ が $n$ であるような確率、を表す条件付き確率です。\n",
    "（条件付き確率については、「基礎的な数学（微分、線形代数、統計）」で解説しました。）\n",
    "\n",
    "ここで、$x$ が所属するクラスについての正解が、${\\bf t} = \\begin{bmatrix} t_1 & t_2 & \\dots & t_N \\end{bmatrix}^T$ というベクトルで与えられているとします。\n",
    "ただし、このベクトルは $t_n \\ (n=1, 2, \\dots, N)$ のいずれか 1 つだけが 1 であり、それ以外は 0 であるようなベクトルであるとします。 \n",
    "これを**ワンホットベクトル（1-hot vector）**と呼びます。\n",
    "そして、この 1 つだけ値が 1 となっている要素は、その要素のインデックスに対応したクラスが正解であることを意味します。\n",
    "例えば、$t_3 = 1$ であれば 3 というインデックスに対応するクラスが正解であるということになります。\n",
    "\n",
    "以上に基づいて、交差エントロピーは以下のように計算されます。\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = - \\frac{1}{N} \\sum_{n=1}^{N}t_{n}\\log y_{n}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kxqfo4Yrekaw"
   },
   "source": [
    "#### 補足：交差エントロピーについて\n",
    "\n",
    "以下は、交差エントロピーについてより正確に知りたい方だけ参考にしてください。\n",
    "情報理論などで交差エントロピーの定義を知っている方は上の式で表されるものが交差エントロピーとは違うように見えるかもしれません。\n",
    "しかし、これは、以下のように説明できます。\n",
    "今、$p(y|x)$ を観測されたデータ $x$ が与えられたとき、その所属クラスが $y$ である条件付き確率とします。\n",
    "次に、$q(y|x)$ をニューラルネットワークによってその条件付き確率をモデル化したものとします。\n",
    "ここで、真の $p(y|x)$ は実際には未知であるため、代わりに学習データの経験分布\n",
    "\n",
    "$$\n",
    "\\hat{p}(y|x) = \\frac{1}{N} \\sum_{n=1}^N I(x = x_n, y = y_n)\n",
    "$$\n",
    "\n",
    "を用いることとします。\n",
    "$N$ は学習データセット中のデータ数を表します。\n",
    "また $I$ はディラック関数とよばれ、その等号が成立するとき値が $\\infty$、それ以外では $0$ であるような関数で、その定義域全体（あり得る $x$ と $y$ の値の組み合わせ全て）にわたる和が 1 になるものです。\n",
    "\n",
    "この時，確率分布 $\\hat{p}(y | x)$ と $q(y | x)$ の間に、**カルバック・ライブラー情報量（Kullback-Leibler divergence）**（以下、KLダイバージェンス）と呼ばれる、確率分布間の差異を測る尺度が定義できます。\n",
    "これは、2 つの確率分布が一致する時、またその時にのみ 0 となり、それ以外のときは正の値をとる値となります。\n",
    "KLダイバージェンスは、以下のように定義されます。\n",
    "\n",
    "$$\n",
    "{\\rm KL}(p || q) = \\int_{x, y} \\hat{p}(y | x) \\log \\frac{\\hat{p}(y | x)}{q(y | x)} dxdy\n",
    "$$\n",
    "\n",
    "ここでディラックのデルタ関数の定義を用い、$\\hat{p}$ だけに依存する項を除くと、先程の交差エントロピーの目的関数が導出されます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Drh35j_lCYiy"
   },
   "source": [
    "## ニューラルネットワークの最適化\n",
    "\n",
    "目的関数の値を最小にするようなパラメータの値を決定することが、ニューラルネットワークを訓練する目的です。\n",
    "では、どのようにしてそのようなパラメータを探し当てればよいのでしょうか。\n",
    "ある目的関数が与えられたもとで、できるだけその目的関数が望ましい値をとるようなニューラルネットワークのパラメータを求めることを、ニューラルネットワークの**最適化（optimization）**といいます。\n",
    "\n",
    "まず最適化の方法を考える前に、最適化の対象が何であったか、再度確認しましょう。\n",
    "「ニューラルネットワークを最適化する」とは、すなわち「ニューラルネットワークが内部で用いている全てのパラメータの値を適切に決定する」という意味です。\n",
    "では、ニューラルネットワークのパラメータとは、何だったでしょうか。\n",
    "それは、ここまで紹介したシンプルな全結合型ニューラルネットワークの場合、各層の線形変換に用いられていた ${\\bf W}$ と ${\\bf b}$ のことを指します。\n",
    "\n",
    "ニューラルネットワークの各パラメータを、目的関数に対する勾配を 0 とおいて解析的に解くことは、一般的には困難です。\n",
    "しかし、実データをニューラルネットワークに入力すれば、その入力の値における目的関数の勾配を数値的に求めることは可能です。\n",
    "この値が分かれば、パラメータをどのように変化させれば目的関数の値を小さくすることができるのかが分かります。\n",
    "そこで、この勾配を使ってパラメータを繰り返し少しずつ更新していくことで、ニューラルネットワークの最適化を行うことができます。\n",
    "この方法について順を追って考えていきましょう。\n",
    "\n",
    "まず、以下の図を見てください。\n",
    "図中の点線は、パラメータ $w$ を変化させた際の目的関数 $\\mathcal{L}$ の値を表しています。\n",
    "この例では簡単のため二次関数の形になっていますが、ニューラルネットワークの目的関数は実際には多次元で、かつもっと複雑な形をしていることがほとんどです。\n",
    "しかし、ここでは説明のためこのようにシンプルな形を考えます。\n",
    "さて、この目的関数が最小値を与えるような $w$ は、どのようにして発見できるでしょうか。\n",
    "\n",
    "![パラメータと目的関数の関係（イメージ）](images/6/09.png)\n",
    "\n",
    "前節で説明したように、ニューラルネットワークのパラメータはまず乱数で初期化されます。\n",
    "ここでは、例として $w=3$ という初期化が行われたと考えてみましょう。\n",
    "そうすると、$w=3$ における $\\mathcal{L}$ の勾配 $\\frac{\\partial \\mathcal{L}}{\\partial w}$ が求まります。\n",
    "ニューラルネットワークの目的関数は、全てのパラメータについて微分可能である[<sup>*3</sup>](#fn3)ように設計されることに注意してください。\n",
    "さて、ここでは仮に $w=3$ における $\\frac{\\partial \\mathcal{L}}{\\partial w}$ が $3$ であったとしましょう。\n",
    "このことを $\\frac{\\partial \\mathcal{L}}{\\partial w} |_{w=3} = 3$と書きます。\n",
    "すると、以下の図のように、この $3$ という値は $w=3$ における $\\mathcal{L}(w)$ という関数の接線の傾き（勾配（gradient）とも言う）を表しています。\n",
    "\n",
    "<span id=\"fn3\"><sup>*3</sup>：<small>厳密には目的関数に微分不可能な点が存在する可能性はあります。例えば ReLU は $x = 0$ で微分不可能なため、ReLU を含んだニューラルネットワークには微分不可能な点が存在することになります。このような場合、**劣微分（subderivative もしくは subdifferential）**という考え方を導入し、全ての点で勾配が求まるようにすることなどが行われます。</small></span>\n",
    "\n",
    "![目的関数の接線の傾き](images/6/10.png)\n",
    "\n",
    "傾きとは、$w$ を増加させた際に $\\mathcal{L}$ が増加する方向を意味しているので、今は $\\mathcal{L}$ の値を小さくしたいわけですから、この傾きの逆方向へ $w$ を変化させる、すなわち $w$ **から** $\\partial \\mathcal{L} / \\partial w$ **を引けばよい** ことになります。\n",
    "\n",
    "これが**ニューラルネットワークのパラメータを目的関数の勾配を用いて更新していく**際の基本的な考え方です。\n",
    "このときの $w$ の一度の更新量（ステップサイズ）のスケールを調整するために、勾配に**学習率（learning rate）**と呼ばれる値を乗じるのが一般的です。\n",
    "\n",
    "例えば、学習率を $0.5$ に設定してみます。\n",
    "そうすると、$w$ の更新量は**学習率** $\\times$ **勾配**で決まるので、$0.5 \\times 3 = 1.5$ となります。\n",
    "現在 $w=3$ なので、**この値を引いて** $w \\leftarrow w - 1.5$ と更新した後は、 $w=1.5$ となります。\n",
    "上の図は、この 1 度の更新を行ったあとの状態を表しています。\n",
    "\n",
    "1度目の更新を行って、$w$ が $w = 1.5$ の位置に移動しました。\n",
    "そこで、再度この点においても勾配を求めてみます。\n",
    "今度は $-1$ になっていたとしましょう。\n",
    "すると**学習率** $\\times$ **勾配**は $0.5 \\times -1 = -0.5$ となります。\n",
    "これを再び用いて、$w \\leftarrow w - (-0.5)$ と2度目の更新を行うと、今度は $w = 2$ の位置にくるでしょう。\n",
    "このようにして、2 回更新したあとは、以下の図のようになります。\n",
    "\n",
    "![パラメータの更新](images/6/11.png)\n",
    "\n",
    "徐々に $\\mathcal{L}$ が最小値をとるときの $w$ の値に近づいていることが見て取れます。\n",
    "\n",
    "こうして、**学習率** $\\times$ **勾配**を更新量としてパラメータを変化させていくと、パラメータ $w$ を求めたい $\\mathcal{L}$ の最小値を与える $w$ に徐々に近づけていくことができます。\n",
    "このような勾配を用いた目的関数の最小化手法を**勾配降下法**と呼びます。\n",
    "ニューラルネットワークは、基本的に**微分可能な関数のみを層間をつなぐ関数として用いて**設計されるため、登場する関数はすべて微分可能であり、学習データセットを用いて勾配降下法によってパラメータを最適化することができます。\n",
    "\n",
    "ただし、通常ニューラルネットワークを勾配降下法で最適化する場合は、データを一つ一つ用いてパラメータを更新するのではなく、いくつかのデータをまとめて入力し、それぞれの勾配を計算したあと、その勾配の平均値を用いてパラメータの更新を行う方法がよく行われます。\n",
    "これを**ミニバッチ学習**と呼びます。\n",
    "これは、学習データセットから一様ランダムに $k (>0)$ 個のデータを抽出し、その $k$ 個のデータに対する目的関数の平均の値を小さくするようパラメータを更新することを、異なる $k$ 個のデータの組み合わせに対して繰り返し行う方法です。\n",
    "結果的にはデータセットに含まれる全てのデータを使用していきますが、1 度の更新に用いるデータは $k$ 個ずつということになります。\n",
    "実際の実装では、データセット内のサンプルのインデックスをまずランダムにシャッフルして並べた配列を作り、その配列の先頭から $k$ 個ずつインデックスを取り出し、対応するデータを使ってミニバッチを構成します。\n",
    "こうして、全てのインデックスを使い切ること、すなわちデータセット内のデータを 1 度ずつ全て、パラメータ更新に用い終えることを **1 エポック**の学習を終える、と言います。\n",
    "そして、この $k$ をバッチサイズもしくはミニバッチサイズと呼び、このような学習方法は、**確率的勾配降下法（stocastic gradient descent, SGD）**と呼ばれます。\n",
    "現在多くのニューラルネットワークの最適化手法はこのSGDをベースとした手法となっています。\n",
    "SGDを用いると、全体の計算時間が劇的に少なくできるだけでなく、下図のように目的関数が凸関数でなかったとしても、適当な条件のもとで“ほとんど確実に”**局所最適解**[<sup>*4</sup>](#fn4)に収束することが知られています。\n",
    "\n",
    "![局所最適解と大域最適解](images/6/12.png)\n",
    "\n",
    "<span id=\"fn4\"><sup>*4</sup>：<small>この局所解が大域的最適解と一致する条件については、現在活発に研究が行われています。参考：[\"A Convergence Theory for Deep Learning via Over-Parameterization\"](https://arxiv.org/abs/1811.03962)</small></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gtJOsYWkKPDj"
   },
   "source": [
    "### パラメータ更新量の算出\n",
    "\n",
    "それでは今、下図のような3層の全結合型ニューラルネットワークを考え、1 層目と 2 層目の間の線形変換が ${\\bf w}_1, {\\bf b}_1$、2 層目と 3 層目の間の線形変換が ${\\bf w}_2, {\\bf b}_2$ というパラメータによって表されているとします（図ではバイアス ${\\bf b}_1, {\\bf b}_2$ は省略されています）。\n",
    "また、これらをまとめて $\\boldsymbol{\\Theta}$ と表すことにします。\n",
    "\n",
    "![パラメータ更新の例](images/6/13.png)\n",
    "\n",
    "入力ベクトルは ${\\bf x}$、ニューラルネットワークの出力は ${\\bf y} \\in \\mathbb{R}^N$（$N$ 次元実数ベクトルという意味）とし、入力 ${\\bf x}$ に対応した“望ましい出力”（教師ベクトルとも言う）を ${\\bf t}$ とします。\n",
    "ここで、目的関数には前述の平均二乗誤差関数を用いることとします。\n",
    "\n",
    "さて、パラメータをそれぞれ適当な乱数で初期化したあと、入力 ${\\bf x}$ が与えられたときの目的関数の各パラメータについての勾配を計算して、それぞれのパラメータについて更新量を算出してみましょう。\n",
    "\n",
    "まず、目的関数を改めてベクトル表記を用いて書き直すと、以下のようになります。\n",
    "\n",
    "$$\n",
    "\\mathcal{L}({\\bf y}, {\\bf t}) = \\frac{1}{N} || {\\bf t} - {\\bf y} ||_2^2\n",
    "$$\n",
    "\n",
    "$|| {\\bf t} - {\\bf y} ||_2^2$ はここでは $({\\bf t} - {\\bf y})^T({\\bf t} - {\\bf y})$ と同等の意味となります。\n",
    "さらに、ニューラルネットワーク全体を $f$ と書くことにすると、出力 ${\\bf y}$ は\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "{\\bf y} &= f({\\bf x}; \\boldsymbol{\\Theta}) \\\\\n",
    "&= a_2 ( {\\bf w}_2 a_1({\\bf w}_1 {\\bf x} + {\\bf b}_1) + {\\bf b}_2 )\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "と書くことができます。\n",
    "ここで、$a_1, a_2$ はそれぞれ、1 層目と 2 層目の、および 2 層目と 3 層目の間で線形変換のあとに施される非線形変換（活性化関数）を意味しています。\n",
    "以下、簡単のために、各層間で行われた線形変換の結果を ${\\bf u}_1, {\\bf u}_2$ とし、中間層の値、すなわち ${\\bf u}_1$ に活性化関数を適用した結果を ${\\bf h}_1$ と書きます。\n",
    "ただし、${\\bf u}_2$ に活性化関数を適用した結果は ${\\bf y}$ と表記します。\n",
    "すると、これらの関係は以下のように整理することができます。\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "{\\bf y} &= a_2({\\bf u}_2) \\\\\n",
    "{\\bf u}_2 &= {\\bf w}_2 {\\bf h}_1 + {\\bf b}_2 \\\\\n",
    "{\\bf h}_1 &= a_1({\\bf u}_1) \\\\\n",
    "{\\bf u}_1 &= {\\bf w}_1 {\\bf x} + {\\bf b}_1\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k1L6xptUs3l4"
   },
   "source": [
    "#### パラメータ ${\\bf w}_2$ の更新量\n",
    "\n",
    "それではまず、出力層に近い方のパラメータ、${\\bf w}_2$ についての $\\mathcal{L}$ の勾配を求めてみましょう。\n",
    "これは、合成関数の偏微分なので、**連鎖律（chain rule）**を用いて以下のように展開できます。\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial {\\bf w}_2}\n",
    "&= \\frac{\\partial \\mathcal{L}}{\\partial {\\bf y}} \\frac{\\partial {\\bf y}}{\\partial {\\bf w}_2} \\\\\n",
    "&= \\frac{\\partial \\mathcal{L}}{\\partial {\\bf y}} \\frac{\\partial {\\bf y}}{\\partial {\\bf u}_2} \\frac{\\partial {\\bf u}_2}{\\partial {\\bf w}_2}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "この 3 つの偏微分はそれぞれ、\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial {\\bf y}}\n",
    "&= -\\frac{2}{N} ({\\bf t} - {\\bf y}) \\\\\n",
    "\\frac{\\partial {\\bf y}}{\\partial {\\bf u}_2}\n",
    "&= \\frac{\\partial a_2}{\\partial {\\bf u}_2} \\\\\n",
    "\\frac{\\partial {\\bf u}_2}{\\partial {\\bf w}_2} \n",
    "&= {\\bf h}_1\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "と求まります。\n",
    "ここで、活性化関数の入力に関する出力の勾配\n",
    "\n",
    "$$\n",
    "\\frac{\\partial a_2}{\\partial {\\bf u}_2}\n",
    "$$\n",
    "\n",
    "が登場しました。\n",
    "これは、例えば活性化関数にシグモイド関数を用いる場合は、\n",
    "\n",
    "$$\n",
    "a_2({\\bf u}_2) = \\frac{1}{1 + \\exp(-{\\bf u}_2)}\n",
    "$$\n",
    "\n",
    "の微分ですから、すなわち\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial a_2({\\bf u}_2)}{\\partial {\\bf u}_2}\n",
    "&= -\\frac{-(\\exp(-{\\bf u}_2))}{(1 + \\exp(-{\\bf u}_2))^2} \\\\\n",
    "&= \\frac{1}{1 + \\exp(-{\\bf u}_2)} \\cdot \\frac{\\exp(-{\\bf u}_2)}{1 + \\exp(-{\\bf u}_2)} \\\\\n",
    "&= \\frac{1}{1 + \\exp(-{\\bf u}_2)} \\cdot \\frac{1 + \\exp(-{\\bf u}_2) - 1}{1 + \\exp(-{\\bf u}_2)} \\\\\n",
    "&= \\frac{1}{1 + \\exp(-{\\bf u}_2)} (1 - \\frac{1}{1 + \\exp(-{\\bf u}_2)}) \\\\\n",
    "&= a_2({\\bf u}_2)(1 - a_2({\\bf u}_2))\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "となります。\n",
    "シグモイド関数の勾配は、このようにシグモイド関数の出力値を使って簡単に計算することができます。\n",
    "\n",
    "これで ${\\bf w}_2$ の勾配を計算するのに必要な値は全て出揃いました。\n",
    "では実際に NumPy を使ってこれらを計算してみましょう。\n",
    "ここでは簡単のために、バイアスベクトルはすべて 0 で初期化されているとします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fD0MA510V1f-"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 入力\n",
    "x = np.array([2, 3, 1])\n",
    "\n",
    "# 正解\n",
    "t = np.array([20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VOznWs5PV3pQ"
   },
   "source": [
    "まず、NumPy モジュールを読み込んでから、入力の配列を定義します。\n",
    "ここでは、上図と同じになるように `2, 3, 1` の3つの値を持つ3次元ベクトルを定義しています。\n",
    "また、正解として仮に `20` を与えることにしました。\n",
    "次に、パラメータを定義します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cBuj19ptXu1f"
   },
   "outputs": [],
   "source": [
    "# 1-2層間のパラメータ\n",
    "w1 = np.array([[3, 1, 2], [-2, -3, -1]])\n",
    "b1 = np.array([0, 0])\n",
    "\n",
    "# 2-3層間のパラメータ\n",
    "w2 = np.array([[3, 2]])\n",
    "b2 = np.array([0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2xT4AnmBXwIb"
   },
   "source": [
    "ここでは、以下の4つのパラメータを定義しました。\n",
    "\n",
    "**1 層目と 2 層目の間の線形変換のパラメータ**\n",
    "\n",
    "${\\bf w}_1 \\in \\mathbb{R}^{2 \\times 3}$ : 3 次元ベクトルを 2 次元ベクトルに変換する行列\n",
    "\n",
    "${\\bf b}_1 \\in \\mathbb{R}^2$ : 2 次元バイアスベクトル\n",
    "\n",
    "**2 層目と 3 層目の間の線形変換のパラメータ**\n",
    "\n",
    "${\\bf w}_2 \\in \\mathbb{R}^{1 \\times 2}$ : 2 次元ベクトルを 1 次元ベクトルに変換する行列\n",
    "\n",
    "${\\bf b}_2 \\in \\mathbb{R}^1$ : 1 次元バイアスベクトル\n",
    "\n",
    "それでは、各層の計算を実際に実行してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "d-ELmfkiKQfd",
    "outputId": "ef26b9e1-ecac-4e5c-f69b-d7443a2338ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.95257194]\n"
     ]
    }
   ],
   "source": [
    "# 中間層の計算\n",
    "u1 = w1.dot(x) + b1\n",
    "h1 = 1. / (1 + np.exp(-u1))\n",
    "\n",
    "# 出力の計算\n",
    "u2 = w2.dot(h1) + b2\n",
    "y = 1. / (1 + np.exp(-u2))\n",
    "\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8t-YjO8BZ6kU"
   },
   "source": [
    "出力は $0.95257194$ と求まりました。\n",
    "つまり、$f([2, 3, 1]^T) = 0.95257194$ ということになります。\n",
    "次に、上で求めた\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial {\\bf w}_2}\n",
    "= \\frac{\\partial \\mathcal{L}}{\\partial {\\bf y}} \\frac{\\partial {\\bf y}}{\\partial {\\bf u}_2} \\frac{\\partial {\\bf u}_2}{\\partial {\\bf w}_2}\n",
    "$$\n",
    "\n",
    "の右辺の 3 つの偏微分をそれぞれ計算してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8RkBjaaaZ7U8"
   },
   "outputs": [],
   "source": [
    "# dL / dy\n",
    "g_Ly = -2 / 1 * (t - y)\n",
    "\n",
    "# dy / du_2\n",
    "g_yu2 = y * (1 - y)\n",
    "\n",
    "# du_2 / dw_2\n",
    "g_u2w2 = h1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uPmRfu3_aXJG"
   },
   "source": [
    "これらを掛け合わせれば、求めたかったパラメータ ${\\bf w}_2$ についての勾配を得ることができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ZrKfXtzfaWk4",
    "outputId": "cc2060bc-b039-465c-b156-cb4b45264c5a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.72104507e+00 -1.43112111e-06]\n"
     ]
    }
   ],
   "source": [
    "# dL / dw_2: 求めたい勾配\n",
    "g_Lw_2 = g_Ly * g_yu2 * g_u2w2\n",
    "\n",
    "print(g_Lw_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XN293R5NU1zP"
   },
   "source": [
    "勾配が求まりました。\n",
    "これが $\\frac{\\partial \\mathcal{L}}{\\partial {\\bf w}_2}$ の値です。\n",
    "これを学習率でスケールさせたものを使えば、パラメータ ${\\bf w}_2$ を更新することができます。\n",
    "更新式は、具体的には以下のようになります。\n",
    "\n",
    "$$\n",
    "{\\bf w}_2 \\leftarrow {\\bf w}_2 - \\eta \\frac{\\partial \\mathcal{L}}{\\partial {\\bf w}_2}\n",
    "$$\n",
    "\n",
    "ここでは学習率を $\\eta$ で表記しました。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HLAwYypPshj8"
   },
   "source": [
    "#### 学習率（learning rate）について\n",
    "\n",
    "学習率が大きすぎると、繰り返しパラメータ更新を行っていく中で目的関数の値が振動したり、発散したりしてしまいます。\n",
    "逆に小さすぎると、収束に時間がかかってしまいます。\n",
    "そのため、この学習率を適切に決定することがニューラルネットワークの学習においては非常に重要となります。\n",
    "多くの場合、学習がきちんと進むもっとも大きな値を経験的に探すということが行われます。\n",
    "シンプルな画像認識のタスクなどでは大抵、$0.1$ から $0.01$ 程度の値が最初に試される場合が比較的多く見られます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SC0RoRpucZh9"
   },
   "source": [
    "#### パラメータ ${\\bf w}_1$ の更新量\n",
    "\n",
    "次に、${\\bf w}_1$ の更新量も求めてみましょう。\n",
    "そのためには、${\\bf w}_1$ で目的関数 $\\mathcal{L}$ を偏微分した値が必要です。\n",
    "これは以下のように計算できます。\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial {\\bf w}_1}\n",
    "&= \\frac{\\partial \\mathcal{L}}{\\partial {\\bf y}} \\frac{\\partial {\\bf y}}{\\partial {\\bf w}_1} \\\\\n",
    "&=\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial {\\bf y}}\n",
    "\\frac{\\partial {\\bf y}}{\\partial {\\bf u}_2}\n",
    "\\frac{\\partial {\\bf u}_2}{\\partial {\\bf w}_1} \\\\\n",
    "&=\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial {\\bf y}}\n",
    "\\frac{\\partial {\\bf y}}{\\partial {\\bf u}_2}\n",
    "\\frac{\\partial {\\bf u}_2}{\\partial {\\bf h}_1}\n",
    "\\frac{\\partial {\\bf h}_1}{\\partial {\\bf w}_1} \\\\\n",
    "&=\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial {\\bf y}}\n",
    "\\frac{\\partial {\\bf y}}{\\partial {\\bf u}_2}\n",
    "\\frac{\\partial {\\bf u}_2}{\\partial {\\bf h}_1}\n",
    "\\frac{\\partial {\\bf h}_1}{\\partial {\\bf u}_1}\n",
    "\\frac{\\partial {\\bf u}_1}{\\partial {\\bf w}_1}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "この5つの偏微分のうち初めの1つはすでに求めました。\n",
    "残りの3つは、それぞれ、\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial {\\bf y}}{\\partial {\\bf u}_2}\n",
    "&= {\\bf y}(1 - {\\bf y}) \\\\\n",
    "\\frac{\\partial {\\bf u}_2}{\\partial {\\bf h}_1}\n",
    "&= {\\bf w}_2 \\\\\n",
    "\\frac{\\partial {\\bf h}_1}{\\partial {\\bf u}_1}\n",
    "&= {\\bf h}_1(1 - {\\bf h}_1) \\\\\n",
    "\\frac{\\partial {\\bf u}_1}{\\partial {\\bf w}_1}\n",
    "&= {\\bf x}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "と計算できます。\n",
    "では、さっそく実際に NumPy を用いて計算を実行してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "v41tSnDkcbPk",
    "outputId": "d6075fb2-5da9-4981-f1b7-d65eaf219fbe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.72463398e-04 -2.58695098e-04 -8.62316992e-05]\n",
      " [-5.72447970e-06 -8.58671954e-06 -2.86223985e-06]]\n"
     ]
    }
   ],
   "source": [
    "g_yu_2 = y * (1 - y)\n",
    "g_u2h1 = w2\n",
    "g_h1u1 = h1 * (1 - h1)\n",
    "g_u1w1 = x\n",
    "\n",
    "# 上から du1 / dw1 の直前までを一旦計算\n",
    "g_Lu1 = g_Ly * g_yu_2 * g_u2h1 * g_h1u1\n",
    "\n",
    "# g_u1w1は (3,) というshapeなので、g_u1w1[None]として(1, 3)に変形\n",
    "g_u1w1 = g_u1w1[None]\n",
    "\n",
    "# dL / dw_1: 求めたい勾配\n",
    "g_Lw1 = g_Lu1.T.dot(g_u1w1)\n",
    "\n",
    "print(g_Lw1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Al0CmLzTtyvR"
   },
   "source": [
    "これが $\\frac{\\partial \\mathcal{L}}{\\partial {\\bf w}_1}$ の値です。\n",
    "これを用いて、${\\bf w}_2$ と同様に以下のような更新式でパラメータ ${\\bf w}_1$ の更新をすることができます。\n",
    "\n",
    "$$\n",
    "{\\bf w}_1 \\leftarrow {\\bf w}_1 - \\eta \\frac{\\partial \\mathcal{L}}{\\partial {\\bf w}_1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zIv_rP32uH1y"
   },
   "source": [
    "## 誤差逆伝播法（バックプロパゲーション）\n",
    "\n",
    "ここまでで、各パラメータについての目的関数の導関数を手計算により導出して実際に勾配の数値計算を行うということを体験しました。\n",
    "では、もっと層数の多いニューラルネットワークの場合は、どうなるでしょうか。\n",
    "同様に手計算によって導関数を求めることも不可能ではありませんが、ニューラルネットワークが微分可能な関数を繰り返し適用するものであるという性質を用いると、コンピュータによって自動的に勾配を与える関数を導き出すことが可能です。\n",
    "合成関数の偏微分は、連鎖律によって複数の偏微分の積の形に変形できることを思い出しましょう。\n",
    "\n",
    "下図は、ここまでの説明で用いていた 3 層の全結合型ニューラルネットワークの出力を得るための計算と、その値を使って目的関数の値を計算する過程を青い矢印で、そして前節で手計算によって行った各パラメータによる目的関数の偏微分を計算する過程を赤い矢印で表現した動画となっています。\n",
    "\n",
    "![誤差逆伝播法(Backpropagation)の計算過程](images/6/backpropagation.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R8YaTki1Dhhx"
   },
   "source": [
    "まず、目的関数の出力を $l = \\mathcal{L}({\\bf y}, {\\bf t})$ とします。\n",
    "この図の丸いノードは変数を表し、四角いノードは関数を表しています。\n",
    "今、一つの巨大な合成関数として見ることができるニューラルネットワーク全体を $f$ と表し、その中で各層間の線形変換に用いられる関数を $f_1$, $f_2$、非線形変換をそれぞれ $a_1$, $a_2$ と表します。\n",
    "このとき、前節で行った更新量の算出はどのように捉えることができるでしょうか。\n",
    "\n",
    "今、上図の青い矢印で表されるように、新しい入力 ${\\bf x}$ がニューラルネットワークに与えられ、それが順々に出力側に伝わっていき、最終的に目的関数の値 $l$ まで計算が終わったとします。\n",
    "ここまでを**順伝播（forward propagation）**といいます。\n",
    "\n",
    "すると次は、目的関数の出力の値を小さくするような各パラメータの更新量を求めたいということになりますが、このために必要な目的関数の勾配は、各パラメータの丸いノードより先の部分（出力側）にある関数の勾配だけで計算できることが分かります。\n",
    "具体的には、それらを全て掛け合わせたものになっています。\n",
    "つまり、上図の赤い矢印で表されるように、出力側から入力側に向かって、**順伝播とは逆向きに**、各関数における入力についての勾配を求めて掛け合わせていけば、パラメータについての目的関数の勾配が計算できるわけです。\n",
    "\n",
    "このように、微分の連鎖律の仕組みを用いて、ニューラルネットワークを構成する関数が持つパラメータについての目的関数の勾配を、**順伝播で通った経路を逆向きにたどるようにして**途中の関数の勾配の掛け算によって求めるアルゴリズムを **誤差逆伝播法（backpropagation）**と呼びます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uHGp6qIBcKOI"
   },
   "source": [
    "## 勾配消失\n",
    "\n",
    "活性化関数について初めに触れた際、シグモイド関数には勾配消失という現象が起きやすくなるという問題があり、現在はあまり使われていないと説明をしました。\n",
    "その理由についてもう少し詳しく見ていきましょう。\n",
    "\n",
    "上で既に計算した、シグモイド関数の導関数を思い出してみます。\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "f\\left( u\\right) &=\\dfrac {1}{1+e^{-u}} \\\\\n",
    "f'\\left( u\\right) &= f\\left( u\\right) \\left( 1-f\\left( u\\right) \\right)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "さて、この導関数の値を入力変数に関してプロットしてみると、下記のようになります。\n",
    "\n",
    "![シグモイド関数の導関数](images/6/15.png)\n",
    "\n",
    "この図の上 2 つは、導関数を構成する2つの部分 $f(u)$ と $1 - f(u)$ の値を別々にプロットしたもので、中央下の図が実際の導関数の値となります。\n",
    "上図中央下の導関数の形を見ると、入力が原点から遠くなるにつれ勾配の値がどんどん減少し、0に漸近していくことが見て取れます。\n",
    "\n",
    "各パラメータの更新量を求めるには、前節で説明したように、**そのパラメータよりも先のすべての関数の勾配を掛け合わせる**必要がありました。\n",
    "このとき、活性化関数にシグモイド関数を用いていると、勾配は必ず**最大でも0.25**という値にしかなりません。\n",
    "すると、線形変換が計算グラフ中に現れるたびに、目的関数の勾配は、多くとも0.25倍されてしまいます。\n",
    "これは、層数が増えていけばいくほど、この最大でも0.25という値が繰り返し掛け合わされることになるため、入力に近い層に流れていく勾配がどんどん0に近づいていってしまいます。\n",
    "\n",
    "具体例を見てみましょう。\n",
    "今回は 3 層のニューラルネットワークを用いて説明を行っていましたが、4層の場合を考えてみます。\n",
    "すると、一番入力に近い線形変換のパラメータの勾配は、多くとも目的関数の勾配を $0.25 \\times 0.25 = 0.0625$ 倍したものということになります。\n",
    "層数が一つ増えるたびに、指数的に勾配が小さくなるということがよく分かります。\n",
    "\n",
    "ディープラーニングでは、4 層よりもさらに多くの層を積み重ねたニューラルネットワークが用いられます。\n",
    "そうすると、活性化関数としてシグモイド関数を使用した場合、**目的関数の勾配が入力に近い関数が持つパラメータへほぼ全く伝わらなくなってしまいます**。\n",
    "あまりにも小さな勾配しか伝わってこなくなると、パラメータの更新量がほとんど 0 になるため、どんなに目的関数が大きな値になっていても、入力層に近い関数が持つパラメータは変化しなくなります。\n",
    "つまり初期化時からほとんど値が変わらなくなるということになり、学習が行われていないという状態になるわけです。\n",
    "これを**勾配消失（vanishing gradient）**と呼び、長らく深い（十数層を超える）ニューラルネットワークの学習が困難であった一つの要因でした。"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Introduction to Neural Network",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
