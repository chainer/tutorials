{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CE_CacxQ7q7t"
   },
   "source": [
    "# 微分\n",
    "\n",
    "本章以降 3 つの章にわたって、ディープラーニングを含めた機械学習に必要な数学の基礎的なトピックから「微分」「線形代数」「確率統計」について、要点を絞り、簡潔に紹介します。\n",
    "そこでまず、**機械学習 (machine learning)** の考え方について大枠を掴み、そのどの部分でそれぞれの項目が登場するかを把握しておきましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yfsQqK7V7q7v"
   },
   "source": [
    "## 機械学習\n",
    "\n",
    "機械学習とは、与えられたデータから、未知のデータに対しても当てはまる規則やパターンを抽出したり、それらを元に未知のデータを分類したり、予測したりするための手法等を研究する学術領域です。\n",
    "機械学習は様々な技術に応用されており、例えば画像認識、音声認識、文書分類、医療診断、迷惑メール検知、商品推薦など、幅広い分野で重要な役割を果たしています。\n",
    "\n",
    "![訓練と推論](images/03/03_01.png)\n",
    "\n",
    "与えられたデータ（**訓練データ (training data)**）から、未知のデータ（**テストデータ (test data)** ）に対しても当てはまる規則やパターンを抽出したり、抽出されたパターンを使って、データを人間にとって意味のあるカテゴリに分類するための関数を得ることを**訓練**と呼びます。\n",
    "訓練によって獲得される関数（**モデル**とも呼ばれます）は多くの場合**パラメータ**と呼ばれる数値の集合によって特徴づけられています。\n",
    "\n",
    "訓練においては、予測のための要因となる**入力変数 (input variable)** $x$ [<sup>*1</sup>](#fn1) と**教師データ (supervised data)** $t$ [<sup>*2</sup>](#fn2)　が必要となります。\n",
    "その与えられた入力変数と教師データに基づいて、その関係性をうまく表すようなモデルを構築します。\n",
    "そして、訓練後のモデルは**訓練済みモデル**と呼ばれ、この訓練済みモデルを用いた予測値を求めることを**推論 (inference)** と呼びます。\n",
    "本チュートリアルでは、主に訓練について扱いますが、実運用を行う際には推論に対するシステム構築のノウハウも重要となります。\n",
    "\n",
    "単純な例として、直線の方程式を考えてみましょう。\n",
    "これは、傾き $a$ と切点 $b$ の２つのパラメータで \n",
    "\n",
    "$$\n",
    "f(x) = ax + b\n",
    "$$\n",
    "\n",
    "と表されます。\n",
    "さらに厳密には、\n",
    "\n",
    "$$\n",
    "f(x; a, b) = ax + b\n",
    "$$\n",
    "\n",
    "と表記されます。\n",
    "これは、「 $;$ 」の後ろに書かれているパラメータ（ここでは $a, b$ ）によって特徴づけられ、一つの入力 $x$ を与えると一つの出力 $y$ を返す関数 $f$ という意味です。\n",
    "機械学習の目標は、訓練データを用いてこれらのパラメータを最適な値へ決定することです。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u704l9iO7q7w"
   },
   "source": [
    "### 目的関数\n",
    "\n",
    "機械学習では訓練データに基づいてパラメータを最適な値へと調整を行います。\n",
    "さて、最適な値というともっともらしく感じますが、「何」に対する最適なのでしょうか。\n",
    "その答えが**目的関数 (objective function)** です。\n",
    "\n",
    "機械学習では、多くの場合、目的関数と呼ばれるものをモデルとは別に用意し、これを**最小化**（もしくは**最大化**）するようにモデルのパラメータを決定することによって、そのモデルが望ましい働きをするように訓練します。\n",
    "そのため、目的関数はモデルの出力値が望ましい場合には小さな値をとり、そうでない場合は大きな値をとるように設計します。\n",
    "\n",
    "例えば、学習データとして2次元空間上の点を $N$ 個集めたデータセット $D = \\left\\{ (x_1, t_1), (x_2, t_2), \\ldots, (x_N, t_N) \\right\\}$ が与えられたとします。\n",
    "ここで、 $x_{n}$ は $n$ 番目の入力変数、 $t_{n}$ は $n$ 番目の教師データに対応しています。\n",
    "これらの点の近くをできる限り通るような直線 \n",
    "$$y = f(x; a, b) = ax + b$$\n",
    "を見つけたいというとき、どのような目的関数を採用すると良いでしょうか。\n",
    "ここで、今後はパラメータを $\\theta$ でまとめて表記することにします。\n",
    "例えば、今考えている直線フィッティングの例では、$\\theta = (a, b)$ となり、これを用いて表される直線の方程式を $y = f(x; \\theta)$ と表記します。\n",
    "\n",
    "教師データと予測値の差が小さいほど望ましいということを表す目的関数を考えてみましょう。\n",
    "例えば、\n",
    "\n",
    "$$\n",
    "t_{n} - y_{n}\n",
    "$$\n",
    "\n",
    "のような単純な差を利用するのはどうでしょうか。\n",
    "さらに、$n$ 番目のサンプルだけに着目するのではなく、データセット内の $N$ 個のサンプル全ての差を足したものを考えてみます。\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "L(\\theta) &= (t_{1} - y_{1}) + (t_{2} - y_{2}) + \\cdots + (t_{N} - y_{N}) \\\\\n",
    "&= \\sum_{n=1}^{N} (t_{n} - y_{n})\n",
    "\\end{aligned}\n",
    "$$\n",
    "のようにすべてのサンプルの総和とします。\n",
    "ここで、$L (\\theta)$ は目的関数を表し、パラメータ $\\theta$ によって目的関数の値に影響があるため、$\\theta$ が変数となっています。\n",
    "ただし、この場合だと、差分が正の値と負の値の両方が取られるため、各サンプル間で打ち消し合ってしまうことが懸念されます。\n",
    "\n",
    "ここで、$L (\\theta)$ は目的関数を表し、その値がパラメータ $\\theta$ によって影響を受けるため、$\\theta$ を引数として取っています。\n",
    "しかし、この目的関数は実際にはうまく働きません。\n",
    "なぜなら、この目的関数は、教師データがどのような値であれ、予測値を無限に大きくしていくことでもその出力を小さくすることができてしまい、教師データと予測値の差が小さいときに**だけ**値が小さくなるような関数になっていないためです。\n",
    "\n",
    "そこで、各サンプルごとの予測値と教師データの間の**差の二乗**を足し合わせていく\n",
    "\n",
    "$$\n",
    "L(\\theta) = \\sum_{n=1}^N (t_n - y_{n})^2\n",
    "$$\n",
    "\n",
    "を目的関数に採用します。\n",
    "この関数は一般に**二乗和誤差 (squared error)** と呼ばれます。\n",
    "二乗ではなく絶対値でも良いように思いますが、数学的な取扱いやすさの観点で二乗することが多いです。\n",
    "\n",
    "さて、この関数を最小化するとはどういうことかを考えてみましょう。\n",
    "上式は、モデルの予測値 $y_{n}$ と教師データ $t_n$ との差の二乗を各データ点（データセット内のひとつのデータ）についてそれぞれ求め、そのデータセット全体に渡る合計値を計算しています。\n",
    "このため、$L(\\theta)$ の値は全てのデータ点に対する予測が対応する教師データと**一致した場合にだけ** $0$ となり、それ以外は予測のはずれ具合に応じた正の値をとります[<sup>*3</sup>](#fn3)。\n",
    "この目的関数を最小化する最適な $\\theta$ を求めることで、データセット $D$ に含まれる点の $x$ の値 $x_n$ $(n=1, 2, \\dots, N)$ から、その教師データの値 $t_n$ $(n=1, 2, \\dots, N)$ を精度良く予測する関数 $y = f(x; \\theta)$ が得られることになります。\n",
    "\n",
    "そして、目的関数の最小化問題を理解するために、**微分**の知識が役に立ちます。\n",
    "また、機械学習では 1 つの変数だけでなく、上の例でパラメータとして 2 つの値が登場したように、複数の変数を扱う必要がある場合が多く、そのようなときに**線形代数**の知識が役に立ちます。\n",
    "さらに、各入力変数ごとの値のスケールを統一したり、外れ値除去などの機械学習でよく行われるデータに対する操作を理解するには、**統計**の知識が役に立ちます。\n",
    "しかし、大学で学ぶような微分、線形代数、統計に関する全ての知識が必要なわけではありません。\n",
    "ここでは、機械学習を学び始める前に必要となる最低限の知識に絞って、微分や線形代数、確率・統計といった分野から、要点のみを解説します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FxGE-MEa7q7x"
   },
   "source": [
    "## 微分\n",
    "\n",
    "本節では、目的関数の最小化に役に立つと前置きで紹介した微分について、その使い方を具体例を用いて紹介します。\n",
    "\n",
    "まずはじめに、微分では何が求まるのでしょうか。\n",
    "関数の入力値における微分は、その点における**接線の傾き**に相当し、下図のように関数に接する直線として表すことができます。\n",
    "\n",
    "![微分1](images/03/03_02.png)\n",
    "\n",
    "例えば上図では、赤い直線が点 $\\theta_{1}$ における接線を表しており、その傾きは $+3$ です。\n",
    "右肩上がりの直線の傾きは正の値になります。\n",
    "\n",
    "![微分2](images/03/03_03.png)\n",
    "\n",
    "一方、上図の点 $\\theta_{2}$ においては、傾きは $-1$ です。\n",
    "接線は右肩下がりの直線となっています。\n",
    "\n",
    "![微分3](images/03/03_03_02.png)\n",
    "\n",
    "そして、関数の値が最小となる点では、**接線の傾きは 0** となります。\n",
    "ここでは、以下の 2 つを覚えておきましょう。\n",
    "\n",
    "1. 微分を行うと、接線の傾きが求まる\n",
    "2. 関数の値が最小となる点では、接線の傾きは 0 になる\n",
    "\n",
    "再度、微分そのものの説明に戻り、その定義や多変数入力、多変数出力の場合についても詳しくみていきます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1blOPTbc7q7y"
   },
   "source": [
    "### 2 点間を通る直線の傾き\n",
    "\n",
    "中学校で習った数学の復習からはじめましょう。\n",
    "微分の原理を理解していくために、下図に示す2点間を通る直線の傾き $a$ を求めてください。\n",
    "\n",
    "![2点間を通る直線](images/03/03_04.png)\n",
    "\n",
    "この時、傾き $a$ は、\n",
    "\n",
    "$$\n",
    "a = \\dfrac{f(x_{2}) - f(x_{1})}{x_{2}-x_{1}}\n",
    "$$\n",
    "\n",
    "でした。\n",
    "中学校では、「$y$ の増加量 / $x$ の増加量」のように習っていました。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3Th2z7b-7q7z"
   },
   "source": [
    "### 1 点での接線の傾き\n",
    "\n",
    "2 点間の接線の傾きを発展させ、与えられた関数の接線の傾きを求めていきます。\n",
    "接線はある 1 点における傾きであり、この計算を行うためには、**極限**の考えが必要になります。\n",
    "極限では、変数がある値に限りなく近づくとき、その変数によって記述される関数がどのような振る舞いをするか考えます。\n",
    "極限を表すために、 $\\lim$ という記号が一般的に用いられます。\n",
    "\n",
    "例えば、\n",
    "\n",
    "$$\n",
    "\\displaystyle \\lim _{x\\rightarrow 0}3x=3\\times 0=0\n",
    "$$\n",
    "\n",
    "では、 $x$ という変数を $0$ に近づけていったときに式の値がどのような値になるかを表しています。\n",
    "\n",
    "それでは、下図のある点 $x$ における接線の傾き $a$ を求めていきましょう。\n",
    "\n",
    "![1点での接線](images/03/03_05.png)\n",
    "\n",
    "さきほど考えた2点を通る直線と極限を組み合わせて、接線を求めることができます。\n",
    "\n",
    "![1点での接線2](images/03/03_06.png)\n",
    "\n",
    "はじめに、 $x$ から $h$ だけ離れた点 $x + h$ を考え、2点を通る直線の傾きを求めてみます。\n",
    "次に $h$ を $h \\rightarrow 0$ のように小さくしていけば、直線の開始点と終了点の2点が1点に収束し、1点での接線として考えることができます。\n",
    "これを式でみると\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "a\n",
    "&= \\lim_{h \\rightarrow 0} \\frac{f(x + h) - f(x)}{(x + h) - x} \\\\\n",
    "&= \\lim_{h \\rightarrow 0} \\frac{f(x + h) - f(x)}{h} \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "となります。\n",
    "上の式は**導関数**とよび、 $f'(x)$ で表されます。\n",
    "\n",
    "$$\n",
    "f'(x)= \\lim_{h \\rightarrow 0} \\frac{f(x + h) - f(x)}{h}\n",
    "$$\n",
    "\n",
    "また導関数を求めることを**微分する**といいます。\n",
    "また、記号の使い方として、\n",
    "\n",
    "$$\n",
    "(\\cdot)' = \\frac{d}{dx}(\\cdot)\n",
    "$$\n",
    "\n",
    "のように表しても構いません。\n",
    "この $d$ という記号は**微分 (differentiation)** を表しており、 $d(\\cdot)$ が対象の値の変化量、 $dx$ が $x$ の変化量を表し、それらを小さくしていった時の極限を表します。\n",
    "この記法は煩雑ですが、変数が $x$ 、 $y$ など複数ある場合に、 $x$ で微分しているのか、 $y$ で微分しているかが明確になるため、正確な表現をすることができます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qh166PcP7q70"
   },
   "source": [
    "### 微分の公式\n",
    "\n",
    "覚えておくと便利な微分の公式があります。\n",
    "以下に幾つか紹介します。以下では、 $c$ は定数、 $x$ は変数を表します。\n",
    "\n",
    "まずはじめは、以下の 3 つの公式を押さえておきましょう。\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\left( c\\right)^{'} &= 0 \\\\\n",
    "\\left( x\\right)^{'} &= 1\\\\\n",
    "\\left( x^{2}\\right)^{'} &= 2x\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "また、少し発展的なものとして、以下の公式も覚えておくと便利です。\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\left( x^{n} \\right)^{'} &= nx^{n-1} \\\\\n",
    "\\left( e^{ax} \\right)^{'} &= ae^{ax}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "ここで、$e$ は自然対数と呼ばれる定数です。\n",
    "また、$e^{ax}$ は $\\exp(ax)$ のように表記されることがよくあり、その場合\n",
    "\n",
    "$$\n",
    "\\bigl( \\exp(ax) \\bigr)^{'} = a\\exp(ax)\n",
    "$$\n",
    "\n",
    "となります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 線形性\n",
    "\n",
    "微分は**線形性**という性質を持っています。\n",
    "\n",
    "それがどのような性質なのか、具体例を挙げて見ていきましょう。\n",
    "微分には線形性という性質によって、\n",
    "\n",
    "$$\n",
    "(3x)' = 3 \\times (x)'\n",
    "$$\n",
    "\n",
    "のように定数項を微分の演算の外側に出すことができます。\n",
    "また、\n",
    "\n",
    "$$\n",
    "\\left( 3x^{2} + 4x - 5 \\right)' = \\left( 3x^{2} \\right)' + \\left( 4x \\right)' - \\left( 5 \\right)' \n",
    "$$\n",
    "\n",
    "のように、加算や減算はそれぞれ項ごとに独立に微分の演算を行うことができます。\n",
    "この ２ つの特性を合わせて線形性と呼びます。\n",
    "\n",
    "もう少し微分の計算を練習してみましょう。\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\left( 3x^{2} + 4x + 5 \\right)' &= \\left( 3x^{2} \\right)' + \\left( 4x \\right)' - \\left( 5 \\right)' \\\\ \n",
    "&= 3 \\times \\left( x^{2} \\right)' + 4 \\times \\left( x \\right)' - 5 \\times \\left( 1 \\right)' \\\\ \n",
    "&= 3 \\times 2x + 4 \\times 1 - 5 \\times 0  \\\\ \n",
    "&= 6x + 4 \n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "この線形性に関しては、下記のように公式としてまとめることができます。\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\left( cf(x) \\right)^{'} &= c f'(x) \\\\\n",
    "\\left( f(x) + g(x) \\right)^{'} &= f^{'}(x) + g^{'}(x) \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "また、関数の積に関する微分の公式も存在するため、余力があれば覚えておきましょう。\n",
    "\n",
    "$$\n",
    "\\bigl( f(x) g(x) \\bigr)^{'} = f^{'}(x)g(x) + f(x)g^{'}(x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0WB8DMII7q72"
   },
   "source": [
    "### 合成関数の微分\n",
    "\n",
    "後の章で詳しく述べますが、一般的に機械学習においては複雑な **合成関数の微分** を考える必要が出てきます。\n",
    "簡単な例として、\n",
    "\n",
    "$$\n",
    "\\left\\{ (3x + 4)^{2} \\right\\}'\n",
    "$$\n",
    "\n",
    "を計算することを考えます。\n",
    "この式は、 $3x+4$ という内側の部分と $(\\cdot)^{2}$ という外側の部分で構成されています。\n",
    "この式を $(9x^2 + 24x + 16)'$ のように展開してから微分を計算しても良いのですが、3乗や4乗となってくると展開するのも大変になります。\n",
    "ここで役に立つ考え方が合成関数の微分です。\n",
    "合成関数の微分は、内側の微分と外側の微分をそれぞれ行い、その結果をかけ合わせることで求めることができます。\n",
    "外側の微分の際には関数の引数を入力とみなし、その入力についての微分を計算します。\n",
    "\n",
    "それでは、具体的にこの $(3x+4)^2$ という関数の微分を考えてみます。\n",
    "\n",
    "まず内側の関数を $u = (3x+4)$ とおいて、\n",
    "\n",
    "$$\n",
    "\\left\\{ (3x + 4)^{2} \\right\\}' = (u^{2})'\n",
    "$$\n",
    "\n",
    "と見ます。ここで、 $(\\cdot)'$ をもう少し厳密に考える必要が出てきます。\n",
    "今は、 $x$ と $u$ の2つの変数が登場しており、 $(\\cdot)'$ では、 $x$ で微分しているのか $u$ で微分しているのかの区別がつきません。\n",
    "そこで、多少複雑に見えますが、先程紹介した $d$ を使った記法で微分する変数を明示的に記述すると、\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\left\\{ (3x + 4)^{2} \\right\\}' &= \\frac{d}{dx} \\left\\{ (3x + 4)^{2} \\right\\} \\\\\n",
    "&= \\frac{du}{dx} \\frac{d}{du} (u^2) \\\\\n",
    "&= \\frac{d}{du} (u^{2}) \\cdot \\frac{d}{dx} (3x + 4) \\\\ \n",
    "&= 2u \\cdot 3 \\\\ \n",
    "&= 6u = 6(3x + 4) = 18x + 24 \\\\ \n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "となります。\n",
    "\n",
    "合成関数の微分を公式としてまとめると下記のようになります。\n",
    "\n",
    "$$\n",
    "\\bigl\\{ f(g(x)) \\bigr\\}^{'} = \\frac{df(u)}{du}\\frac{du}{dx} \\hspace{1em} \\text{where $u = g(x)$}\n",
    "$$\n",
    "\n",
    "ニューラルネットワークの仕組みを理解する上では、このような合成関数の微分を使用する場面が頻繁に登場するため、この計算方法をしっかりと覚えておきましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sZJDW8537q73"
   },
   "source": [
    "### 偏微分\n",
    "\n",
    "機械学習では、1つの入力変数 $x$ から出力変数 $y$ を予測するケースは稀であり、多くの場合、複数の入力変数 $x_1, x_2, \\dots, x_M$ を用いて $y$ を予測する**多変数関数**が扱われます。\n",
    "例えば、家賃を予測する場合、部屋の広さだけではなく、駅からの距離や周辺の犯罪発生率なども同時に考慮した方がより正確に予測ができると期待されます。\n",
    "複数の入力 $x_1, x_2, \\dots, x_M$ をとる関数 $f(x_1, x_2, \\dots, x_M)$ を多変数関数とよびます。\n",
    "この多変数関数において、ある入力 $x_m$ にのみ注目して微分することを **偏微分** とよび、\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial x_{m}} f(x_1, x_2, \\dots, x_M)\n",
    "$$\n",
    "\n",
    "と表します。微分を意味する記号が、 $d$ から $\\partial$ に変わっています。こうすると、 $\\frac{\\partial}{\\partial x_m}$ は $x_m$ 以外を定数と考え、 $x_m$ にのみ着目して微分を行うという意味となります[<sup>*4</sup>](#fn4)。\n",
    "\n",
    "以下の例で具体的な計算の流れを確認しましょう。\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial}{\\partial x_1}\n",
    "\\left( 3x_1+4x_2 \\right)\n",
    "&= \\frac{\\partial}{\\partial x_1}\n",
    "\\left( 3x_1 \\right) + \\frac{\\partial}{\\partial x_1} \\left( 4x_2 \\right) \\\\\n",
    "&= 3 \\times \\frac{\\partial}{\\partial x_1} \\left( x_1 \\right) + 4 \\times \\frac{\\partial}{\\partial x_1} x_2 \\\\\n",
    "&= 3 \\times 1 + 4 \\times 0 \\\\\n",
    "&= 3\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "偏微分でも微分と同じ公式を適用できます。今回のケースでは、 $x_1$ にだけ着目するため、 $x_2$ は定数として扱われています。このことを把握しておくと上記の計算の流れが理解しやすくなります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr />\n",
    "<span id=\"fn1\"><sup>*1</sup>: <small>*特徴量変換を施した場合、**特徴量** と呼ぶこともありますが、本チュートリアルでは特徴量も含め、入力変数という呼び名で統一することとします。</small></span>\n",
    "\n",
    "<span id=\"fn2\"><sup>*2</sup>: <small>*厳密には **目標値 (target value)** として観測されたデータを教師データと呼びますが、説明をシンプルにするために、すべて教師データで統一して紹介しています。$t$ は target value から由来しています。また、教師データが必要となるケースは機械学習の中でも教師あり学習に限られますが、本チュートリアルでは教師あり学習を中心に扱うため、機械学習の一般的な考え方を教師あり学習を中心に紹介しています。</small></span>\n",
    "\n",
    "<span id=\"fn3\"><sup>\\*3</sup> : <small> 間違えた度合いを測る関数を、特に**損失関数 (loss function)** と呼ぶ場合があります。 </small></span>\n",
    "\n",
    "<span id=\"fn4\"><sup>*4</sup>: <small>入力変数が他の入力変数と独立でない場合は定数と考えることはできません。しかし本資料ではそのようなケースは出てきません。</small></span>\n",
    "\n",
    "<span id=\"fn5\"><sup>*5</sup> : <small> $N \\times M$ 行列、などと言われたときに、$N$ と $M$ のどちらが行で、どちらが列だろう？と迷ったときは、「行列」という言葉を再度思い浮かべて、「行→列」つまり先にくる $N$ が行数で、$M$ が列数だ、と思い出すのがおすすめです。</small></span>\n",
    "\n",
    "<span id=\"fn6\"><sup>*6</sup>: <small>ここでは概念の説明を簡単にするため、この例のように離散的な値を取る確率変数を考え、特に明示しない限り連続値の確率変数は考えないことにします。</small></span>\n",
    "\n",
    "<span id=\"fn7\"><sup>*7</sup>: <small> $x$ は $1, 2, 3, 4, 5, 6$ のいずれか。すなわち $x \\in \\{1, 2, 3, 4, 5, 6\\}$</small></span>\n",
    "\n",
    "<span id=\"fn8\"><sup>*8</sup>: <small> $y$ は 2 つ目のコインが取りうる状態で、この場合、「表」と「裏」という値のいずれか。</small></span>\n",
    "\n",
    "<span id=\"fn9\"><sup>*9</sup>: <small> $x$ は 1 つ目のコインが取りうる状態で、この場合、「表」と「裏」という値のいずれか。</small></span>\n",
    "\n",
    "<span id=\"fn10\"><sup>\\*10</sup>: <small>以降、このことを「データの分布を推定する」と言うことがあります。また、観測されたデータのみから各データの発生確率（頻度とも捉えられる）を求めたものは**経験分布（empirical distribution）**とも呼ばれ、本節で説明しているのは正確にはこの経験分布を確率モデルで近似する方法です。</small></span>\n",
    "\n",
    "<span id=\"fn11\"><sup>\\*11</sup>: <small>この関数には、ベルヌーイ分布いう名前がついています。</small></span>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "03_Basics_of_Math_for_Machine_Learning_ja.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
