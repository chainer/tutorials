{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "_CMCHQhNDfXm"
            },
            "source": [
                "# Chainer の応用\n",
                "\n",
                "前章で説明した Chainer を用いてネットワークを訓練するまでに必要なステップは、以下の 5 つでした。\n",
                "\n",
                "- Step 1：データセットの準備\n",
                "- Step 2：ネットワークの定義\n",
                "- Step 3：目的関数を決める\n",
                "- Step 4：最適化手法の選択\n",
                "- Step 5：ネットワークの訓練\n",
                "\n",
                "本章では、これらの各ステップに対して Chainer の機能を活用した工夫を加え、結果を改善する方法を解説します。"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 1 の改善: データセットの準備の工夫\n",
                "\n",
                "前章で用いた iris データセットは、ndarray のままスライスを用いてミニバッチに分割し、ネットワークに入力したり、誤差の計算に用いたりしていました。\n",
                "ここでは、Chainer が提供するいくつかのデータセットを扱うためのクラスを用いて同様のことを行ってみます。\n",
                "\n",
                "まずはデータセットを scikit-learn を用いて取得しておきます。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.datasets import load_iris\n",
                "\n",
                "x, t = load_iris(return_X_y=True)\n",
                "x = x.astype('float32')\n",
                "t = t.astype('int32')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### TupleDataset\n",
                "\n",
                "Chainer は `chainer.datasets` モジュール以下に色々なデータセットに対して用いられる便利なクラスが用意されています。\n",
                "その一つが `TupleDataset` で、これは上で取得したような入力値を並べた配列と目標値を並べた配列を与えると、1 つ 1 つを取り出して対応するペアを作って返してくれるものです。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "from chainer.datasets import TupleDataset\n",
                "\n",
                "dataset = TupleDataset(x, t)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Chainer が提供する汎用のデータセットクラスは、ndarray をデータセットとして扱う場合と同様にして使えるよう、`[]` を使ったインデックスによる各データ（入力値と目標値のペタ）へのアクセスを可能としています。\n",
                "\n",
                "それでは、1 つ目の入力値と目標値のペアを取り出してみます。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "(array([5.1, 3.5, 1.4, 0.2], dtype=float32), 0)"
                        ]
                    },
                    "execution_count": 3,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "dataset[0]"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "入力値と目標値を 1 つずつ持つタプルが返されました。\n",
                "それでは、2 つのデータを取り出してみます。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "[(array([5.1, 3.5, 1.4, 0.2], dtype=float32), 0),\n",
                            " (array([4.9, 3. , 1.4, 0.2], dtype=float32), 0)]"
                        ]
                    },
                    "execution_count": 4,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "dataset[:2]"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "`(入力値, 目標値)` というタプルを 2 つ持つリストが返されました。\n",
                "このように、Python のリストで用いることができるスライスと同様の表記を Chainer のデータセットオブジェクトに対して用いることができます。\n",
                "\n",
                "例えば、先頭から 1 つおきに 5 つのデータを取り出したい場合、"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "[(array([5.1, 3.5, 1.4, 0.2], dtype=float32), 0),\n",
                            " (array([4.7, 3.2, 1.3, 0.2], dtype=float32), 0),\n",
                            " (array([5. , 3.6, 1.4, 0.2], dtype=float32), 0),\n",
                            " (array([4.6, 3.4, 1.4, 0.3], dtype=float32), 0),\n",
                            " (array([4.4, 2.9, 1.4, 0.2], dtype=float32), 0)]"
                        ]
                    },
                    "execution_count": 5,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "dataset[:10:2]"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "のような記述が可能です。"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### データセットの分割\n",
                "\n",
                "データセットを訓練用・検証用・テスト用に分割するのに便利な関数が Chainer にも用意されています。\n",
                "これを用いて先程読み込んだ Iris データセットを 3 つに分割してみます。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": [
                "from chainer.datasets import split_dataset_random\n",
                "\n",
                "train_val, test = split_dataset_random(dataset, int(len(dataset) * 0.7), seed=0)\n",
                "train, valid = split_dataset_random(train_val, int(len(train_val) * 0.7), seed=0)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "`dataset` の中のデータのうち 70% を `train_val` に、残りを `test` にランダムに振り分け、さらに `train_test` のうち 70% を `train` に、残りを `valid` にランダムに振り分けています。"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### SerialIterator\n",
                "\n",
                "前章では、訓練データセットの順番を毎エポックシャッフルする方法として `np.random.permutation()` 関数が用いられていましたが、Chainer ではこのようなネットワークの訓練に際してよく行われるデータセットへの操作を抽象化した Iterator が提供されています。\n",
                "\n",
                "Iteratorは、データセットオブジェクトを与えると、順番のシャッフルやバッチサイズ個だけデータをまとめて返すなどの操作を自動的に行なってくれるものです。\n",
                "Python のイテレータと似たインターフェースを持ち、`next()` メソッドで順番にミニバッチを返してくれ、1 エポックが終了したら自動的にデータの順番をシャッフルしなおしてくれる便利なものです。\n",
                "\n",
                "ここでは最もシンプルな Iterator である `SerialIterator` を紹介します。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "[(array([4.4, 3. , 1.3, 0.2], dtype=float32), 0),\n",
                            " (array([6.4, 3.1, 5.5, 1.8], dtype=float32), 2),\n",
                            " (array([6.8, 3. , 5.5, 2.1], dtype=float32), 2),\n",
                            " (array([7.2, 3. , 5.8, 1.6], dtype=float32), 2)]"
                        ]
                    },
                    "execution_count": 7,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "from chainer.iterators import SerialIterator\n",
                "\n",
                "train_iter = SerialIterator(train, batch_size=4, repeat=True, shuffle=True)\n",
                "\n",
                "minibatch = train_iter.next()\n",
                "\n",
                "minibatch"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "上のコードは、iris データセットを分割して作成した訓練用データセット train を渡して作成した SerialIterator から、最初のミニバッチを取り出す例です。\n",
                "\n",
                "インスタンス化の際に引数として `batchsize=4` を指定しているため、返ってきたミニバッチには 4 つのデータが格納されていることが分かります。\n",
                "\n",
                "`repeat` 引数には、`next()` を繰り返し実行してデータセット内の全てのデータを取り出し終えたあとに、次の `next()` の呼び出しに対してまたデータセットの先頭からデータを取り出して次のミニバッチを返すかどうかを指定します。\n",
                "1 エポック以上訓練を行う場合は訓練データセット内のデータを複数回用いるので、これを `True` にします。\n",
                "\n",
                "`shuffle` 引数には、データセット内のデータの順番をエポックごとに自動的にシャッフルするかどうかを指定します。"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 2 の改善: ネットワーク構造の工夫\n",
                "\n",
                "前章ではネットワークを Sequential クラスを用いて定義しました。\n",
                "ここでは Chain クラスを継承してネットワークを定義する方法を説明します。"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Chain\n",
                "\n",
                "Link を複数まとめて Chain というまとまりを作ることが出来ます。\n",
                "Chain クラスは Link クラスを継承しており、前章で解説した Link と同様に扱うことができます。\n",
                "ただし、内部に複数の Link を保持しておくことができ、呼び出されたときに実行される巡伝播計算を `forward()` メソッドに記述しておくことで、複数の Link や Function を組み合わせた独自の層を作るのに使うことができます。\n",
                "また、ネットワークの部分構造を記述したり、それ自体で 1 つのネットワークの定義とすることもできます。\n",
                "\n",
                "それでは、前章で定義した Linear 層を 3 つ持つネットワークと同じものを Chain クラスを継承した `Net` というクラスを作って定義してみましょう。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [],
            "source": [
                "import chainer\n",
                "import chainer.links as L\n",
                "import chainer.functions as F\n",
                "\n",
                "\n",
                "class Net(chainer.Chain):\n",
                "    \n",
                "    def __init__(self, n_in=4, n_hidden=3, n_out=3):\n",
                "        super().__init__()\n",
                "        with self.init_scope():\n",
                "            self.l1 = L.Linear(n_in, n_hidden)\n",
                "            self.l2 = L.Linear(n_hidden, n_hidden)\n",
                "            self.l3 = L.Linear(n_hidden, n_out)\n",
                "\n",
                "    def forward(self, x):\n",
                "        h = F.relu(self.l1(x))\n",
                "        h = F.relu(self.l2(h))\n",
                "        h = self.l3(h)\n",
                "        \n",
                "        return h\n",
                "\n",
                "net = Net()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "この Net クラスは 3 層の全結合型ニューラルネットワークを表しています。\n",
                "\n",
                "`__init__()` メソッドでは、`init_scope()` によって作られるコンテキストの中で Linear 層 を 3 つ作成し、それぞれ別々の名前の属性に代入しています。\n",
                "このコンテキストの中で属性に代入された Link が持つパラメータは、Optimizer によるパラメータ更新の対象として登録されます。\n",
                "このコンテキストの外で Link を作成し属性に代入を行っても、その Link が持つパラメータは Optimizer によるパラメータ更新の対象にならないため、注意が必要です。\n",
                "\n",
                "`init_scope()` コンテキストの中で定義されている Linear クラスのインスタンス化の際には `n_in`、`n_hidden`、`n_out` などの Net クラスの `__init__()` メソッドがとる引数が使われています。\n",
                "こうすることで Net クラスをインスタンス化する際に内部の Linear 層のパラメータ数などを変更することができます。\n",
                "例えば、`self.l1` の出力ノード数を 100 にするには、以下のようにします。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [],
            "source": [
                "net = Net(n_hidden=100)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "このように、Chain を継承したクラスを用いてネットワークを定義すると、インスタンス化の際に各層の出力ノード数などを変更することができ、より柔軟性とコードの再利用性が高まります。"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 3 の改善: 目的関数の工夫\n",
                "\n",
                "前章では、分類問題を解くために `F.softmax_cross_entorpy` という目的関数を使用しました。\n",
                "これはネットワークの出力を ${\\bf y}$ とおくと、以下のような式で表される計算を行っています。\n",
                "\n",
                "$$\n",
                "\\begin{align}\n",
                "\\hat{\\bf y} &= {\\rm softmax}({\\bf y}) \\\\\n",
                "L(\\boldsymbol\\Theta) &= \\sum_{k=1}^K \\hat{y}_k \\log t_k\n",
                "\\end{align}\n",
                "$$\n",
                "\n",
                "ただし、$\\hat{y}_k, t_k$ はそれぞれ $\\hat{\\bf y}$ の $k$ 番目の要素と、1-hotベクトル表現されたクラスラベルを表す目的変数 ${\\bf t}$ の $k$ 番目の要素をそれぞれ表しています。\n",
                "また、$\\boldsymbol\\Theta$ はネットワークが持つ全パラメータを指します。\n",
                "\n",
                "ここで、この目的関数に**正則化項 (regularization term)** を追加してみましょう。\n",
                "**正則化 (regularization)** とは、過学習を防ぐために、目的関数に新たな項を追加して、モデルの複雑さに罰則を科したり、パラメータのノルムの大きさに罰則をかけたりすることを指します。\n",
                "そのために目的関数に追加される項が正則化項です。\n",
                "\n",
                "ここでは、**重み減衰 (weight decay)** と呼ばれる正則化を適用してみましょう。\n",
                "これは、**L2正則化 (L2 regularization)** とも呼ばれます。\n",
                "重み減衰を行う場合、最適化する目的関数は以下になります。\n",
                "\n",
                "$$\n",
                "L(\\boldsymbol\\Theta) + \\lambda \\frac{1}{2} \\sum_w || w ||^2\n",
                "$$\n",
                "\n",
                "ここで、$w$ はネットワークのパラメータを指し、上式の第 2 項は全てのパラメータの値を二乗したものの総和を取り、2 で割るという計算を意味します。\n",
                "この項を目的関数に加えると、ネットワークの重みの絶対値が大きくなりすぎないようにする効果があり、過学習を防ぐために役立ちます。\n",
                "$\\lambda$ は正則化の強さをコントロールします。\n",
                "新しい目的関数を用いる場合のパラメータの更新式は一般に以下となります。\n",
                "\n",
                "$$\n",
                "w \\leftarrow w - \\eta \\left( \\frac{\\partial L(\\boldsymbol\\Theta)}{\\partial w} + \\lambda w \\right)\n",
                "$$\n",
                "\n",
                "ここで、$\\eta$ は学習率を表します。\n",
                "この式から、パラメータ更新の際にパラメータ自身の更新前の値に $\\lambda$ を乗じたものを更新量に加えればよいということが分かります。\n",
                "重み減衰はバイアスの更新式には通常適用しないことに注意してください[<sup>*1</sup>](#no_weight_decay_for_bias]。\n",
                "\n",
                "Chainer は、パラメータを更新する際に更新計算をカスタマイズする方法を 2 種類提供しています。\n",
                "\n",
                "1 つは、ネットワークが持つパラメータ全てに対して一様に、更新時にある処理を行いたい場合に使える、Optimizer hook という機能です。\n",
                "これは、Optimizer オブジェクトの `add_hook()` メソッドに更新時に全パラメータに対して行いたい処理を記述した関数を渡して使用します。\n",
                "\n",
                "もう 1 つは、パラメータごとに別々に処理を行いたい場合に使える方法で、ネットワークのパラメータが持っている UpdateRule というオブジェクトにフック関数を追加します。\n",
                "全ての訓練可能なパラメータは、Optimizer オブジェクトの `setup()` メソッドに渡された際に `update_rule` という属性に UpdateRule オブジェクトがセットされます。\n",
                "この UpdateRule オブジェクトは、最適化手法によって異なる更新ルールが記述されたもので、`add_hook()` メソッドを持ち、ここに Optimizer hook または任意の関数を追加することができます。\n",
                "これを用いると、更新時にパラメータごとに別の関数を読んで更新計算をカスタマイズすることができます。\n",
                "重み減衰は、前述のようにバイアスには適用しないため、今回は UpdateRule に対してフック関数を追加します。\n",
                "\n",
                "Chainer では、`chainer.optimizer_hooks` モジュール以下に数種類の正則化手法が定義されており、重み減衰は `WeightDecay` というクラスとして定義されています。それでは、これを用いて前章と同様に最適化手法として SGD を採用しつつ、新たに重み減衰を適用するような `optimizer` を定義しましょう。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [],
            "source": [
                "from chainer import optimizers\n",
                "from chainer.optimizer_hooks import WeightDecay\n",
                "\n",
                "optimizer = optimizers.SGD(lr=0.001)  # 学習率を 0.01 に設定\n",
                "optimizer.setup(net)\n",
                "\n",
                "for param in net.params():\n",
                "    if param.name != 'b':  # バイアス以外だったら\n",
                "        param.update_rule.add_hook(WeightDecay(0.0001))  # 重み減衰を適用"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "ネットワークの持つパラメータは、Chain クラスの `params()` メソッドを使って取得することができます。\n",
                "また、Linear クラスの中でバイアスは `b` という名前で定義されているため、各パラメータの名前をチェックして `b` という名前**でない**場合にだけ WeightDecay を UpdateRule にフック関数として追加しています。\n",
                "WeightDecay クラスはインスタンス化時に引数を 1 つとり、前述の $\\lambda$ に対応する係数を指定することができます。\n",
                "ここでは、`0.0001` を指定しています。"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 4 の改善: 最適化手法の工夫\n",
                "\n",
                "前章では、Chainer が提供している最もシンプルな最適化手法の一つである SGD を用いていました。\n",
                "\n",
                "Chainer は SGD の他にも多くの最適化手法を提供しています。\n",
                "ここでは、その中でも代表的な手法の一つである MomentumSGD という手法を用いるように変更を加えてみましょう。\n",
                "\n",
                "MomentumSGD は SGD の改良版で、パラメータ更新の際に**前回の更新量**を使って更新方向がスムーズになるように工夫するもので、更新式は以下になります。\n",
                "\n",
                "$$\n",
                "\\Delta w_t = \\frac{\\partial L(\\boldsymbol\\Theta)}{\\partial w^{(t)}} \\\\\n",
                "w \\leftarrow w - \\eta \\Delta x_{t} + \\mu \\Delta x_{t - 1}\n",
                "$$\n",
                "\n",
                "ここで、$t$ 回目の更新量を $\\Delta x_t$ とおいています。\n",
                "また、$\\mu$ は前回更新量に掛ける係数で、多くの場合 $0.9$ 程度が用いられます。\n",
                "これを前節で解説した重み減衰と合わせて用いると、更新式は以下のようになります。\n",
                "\n",
                "$$\n",
                "w \\leftarrow w - \\eta \\Delta x_{t} + \\mu \\Delta x_{t - 1} - \\eta \\lambda w\n",
                "$$\n",
                "\n",
                "それでは、MomentumSGD に加えて重み減衰を用いる Optimizer を定義してみましょう。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [],
            "source": [
                "from chainer import optimizers\n",
                "from chainer.optimizer_hooks import WeightDecay\n",
                "\n",
                "optimizer = optimizers.MomentumSGD(lr=0.001, momentum=0.9)\n",
                "optimizer.setup(net)\n",
                "\n",
                "for param in net.params():\n",
                "    if param.name != 'b':  # バイアス以外だったら\n",
                "        param.update_rule.add_hook(WeightDecay(0.0001))  # 重み減衰を適用"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 5 の改善：ネットワークの訓練における工夫\n",
                "\n",
                "最後に、ネットワークの訓練を高速化するために GPU を用いる方法を紹介します。\n",
                "\n",
                "Iris データセットはデータ数が少なく、各データの次元数も小さい上に、前章では訓練したニューラルネットワークのパラメータ数も多くありませんでした。\n",
                "そのため、CPU を使ってもそこまで長く待たされることなく訓練が終了しました。\n",
                "\n",
                "しかし、より大きなデータセットを使い、より多くの層を持つニューラルネットワークを訓練しようとすると、CPU だけでは膨大な時間がかかってしまいます。\n",
                "そこで、GPU を使って計算を高速化する必要が出てきます。\n",
                "\n",
                "まず、GPU が使える環境であることを確認しましょう。\n",
                "\n",
                "Google Colaboratory ではメニュー項目の「ランタイム」から「ランタイムのタイプを変更」を選択し、「ハードウェアアクセラレータ」と書かれたプルダウンメニューからから GPU を指定することができます。 さっそく GPU を有効にしてください。\n",
                "\n",
                "また、一台のマシンに複数の GPU を挿して用いる場合もあるため、GPU を用いる際はデバイス ID というものを意識する必要がありますが、Google Colaboratory では利用できる GPU が 1 枚だけなので、このデバイス ID としては基本的に 0 を指定すれば良いことになります。\n",
                "\n",
                "GPU を利用したい場合、ネットワークの訓練を開始する前に気をつけることは主に以下の 2 つです。\n",
                "\n",
                "- ネットワークを `to_gpu()` を用いて GPU メモリ上に転送しておく\n",
                "- ネットワークに入力するデータを CuPy の ndarray に変換しておく\n",
                "\n",
                "それでは、訓練ループを GPU を使用する形に変更してみましょう。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {
                "scrolled": true
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "epoch: 0, iteration: 20, loss (train): 0.6595, loss (valid): 0.6838acc (train): 0.7500, acc (valid): 0.6562\n",
                        "epoch: 1, iteration: 38, loss (train): 0.2920, loss (valid): 0.9503acc (train): 1.0000, acc (valid): 0.6562\n",
                        "epoch: 2, iteration: 56, loss (train): 0.0972, loss (valid): 0.6568acc (train): 1.0000, acc (valid): 0.6562\n",
                        "epoch: 3, iteration: 74, loss (train): 0.2779, loss (valid): 0.3604acc (train): 1.0000, acc (valid): 0.9688\n",
                        "epoch: 4, iteration: 93, loss (train): 0.3347, loss (valid): 0.3295acc (train): 1.0000, acc (valid): 0.9688\n",
                        "epoch: 5, iteration: 111, loss (train): 0.1136, loss (valid): 0.3016acc (train): 1.0000, acc (valid): 0.9688\n",
                        "epoch: 6, iteration: 129, loss (train): 0.4425, loss (valid): 0.3211acc (train): 0.5000, acc (valid): 0.8438\n",
                        "epoch: 7, iteration: 147, loss (train): 0.0825, loss (valid): 0.3410acc (train): 1.0000, acc (valid): 0.8125\n",
                        "epoch: 8, iteration: 166, loss (train): 0.3209, loss (valid): 0.3126acc (train): 0.7500, acc (valid): 0.8438\n",
                        "epoch: 9, iteration: 184, loss (train): 0.3123, loss (valid): 0.2467acc (train): 1.0000, acc (valid): 0.9062\n",
                        "epoch: 10, iteration: 202, loss (train): 0.1243, loss (valid): 0.2129acc (train): 1.0000, acc (valid): 0.9688\n",
                        "epoch: 11, iteration: 220, loss (train): 0.2259, loss (valid): 0.2021acc (train): 1.0000, acc (valid): 0.9688\n",
                        "epoch: 12, iteration: 239, loss (train): 0.0555, loss (valid): 0.2403acc (train): 1.0000, acc (valid): 0.9062\n",
                        "epoch: 13, iteration: 257, loss (train): 0.1268, loss (valid): 0.2379acc (train): 1.0000, acc (valid): 0.9062\n",
                        "epoch: 14, iteration: 275, loss (train): 0.2880, loss (valid): 0.1877acc (train): 1.0000, acc (valid): 0.9688\n",
                        "epoch: 15, iteration: 293, loss (train): 0.1954, loss (valid): 0.2516acc (train): 1.0000, acc (valid): 0.8750\n",
                        "epoch: 16, iteration: 312, loss (train): 0.0538, loss (valid): 0.2650acc (train): 1.0000, acc (valid): 0.8438\n",
                        "epoch: 17, iteration: 330, loss (train): 0.0271, loss (valid): 0.3496acc (train): 1.0000, acc (valid): 0.8438\n",
                        "epoch: 18, iteration: 348, loss (train): 0.2700, loss (valid): 0.3569acc (train): 1.0000, acc (valid): 0.8438\n",
                        "epoch: 19, iteration: 366, loss (train): 0.1411, loss (valid): 0.1664acc (train): 1.0000, acc (valid): 0.9688\n",
                        "epoch: 20, iteration: 385, loss (train): 0.3122, loss (valid): 0.2194acc (train): 0.7500, acc (valid): 0.9062\n",
                        "epoch: 21, iteration: 403, loss (train): 0.1310, loss (valid): 0.1811acc (train): 1.0000, acc (valid): 0.9375\n",
                        "epoch: 22, iteration: 421, loss (train): 0.0201, loss (valid): 0.2310acc (train): 1.0000, acc (valid): 0.9062\n",
                        "epoch: 23, iteration: 439, loss (train): 0.0586, loss (valid): 0.1298acc (train): 1.0000, acc (valid): 0.9688\n",
                        "epoch: 24, iteration: 458, loss (train): 0.0809, loss (valid): 0.1313acc (train): 1.0000, acc (valid): 0.9688\n",
                        "epoch: 25, iteration: 476, loss (train): 0.2808, loss (valid): 0.1391acc (train): 0.7500, acc (valid): 0.9375\n",
                        "epoch: 26, iteration: 494, loss (train): 0.2512, loss (valid): 0.1253acc (train): 1.0000, acc (valid): 0.9688\n",
                        "epoch: 27, iteration: 512, loss (train): 1.3128, loss (valid): 0.6164acc (train): 0.2500, acc (valid): 0.7812\n",
                        "epoch: 28, iteration: 531, loss (train): 1.2856, loss (valid): 0.2101acc (train): 0.2500, acc (valid): 0.9375\n",
                        "epoch: 29, iteration: 549, loss (train): 0.0320, loss (valid): 0.1248acc (train): 1.0000, acc (valid): 0.9688\n",
                        "epoch: 30, iteration: 567, loss (train): 0.0077, loss (valid): 0.1204acc (train): 1.0000, acc (valid): 0.9688\n",
                        "epoch: 31, iteration: 585, loss (train): 0.1045, loss (valid): 0.1607acc (train): 1.0000, acc (valid): 0.9688\n",
                        "epoch: 32, iteration: 604, loss (train): 1.1598, loss (valid): 0.5646acc (train): 0.2500, acc (valid): 0.8125\n",
                        "epoch: 33, iteration: 622, loss (train): 0.5136, loss (valid): 0.2271acc (train): 0.7500, acc (valid): 0.9062\n",
                        "epoch: 34, iteration: 640, loss (train): 0.0549, loss (valid): 0.1714acc (train): 1.0000, acc (valid): 0.9688\n",
                        "epoch: 35, iteration: 658, loss (train): 0.1485, loss (valid): 0.1486acc (train): 1.0000, acc (valid): 0.9688\n",
                        "epoch: 36, iteration: 677, loss (train): 0.0492, loss (valid): 0.2425acc (train): 1.0000, acc (valid): 0.9062\n",
                        "epoch: 37, iteration: 695, loss (train): 0.2139, loss (valid): 0.1577acc (train): 0.7500, acc (valid): 0.9688\n",
                        "epoch: 38, iteration: 713, loss (train): 0.0582, loss (valid): 0.1873acc (train): 1.0000, acc (valid): 0.9375\n",
                        "epoch: 39, iteration: 731, loss (train): 0.6252, loss (valid): 0.2553acc (train): 0.7500, acc (valid): 0.9062\n",
                        "epoch: 40, iteration: 750, loss (train): 0.0443, loss (valid): 0.2230acc (train): 1.0000, acc (valid): 0.9375\n",
                        "epoch: 41, iteration: 768, loss (train): 0.0534, loss (valid): 0.1531acc (train): 1.0000, acc (valid): 0.9688\n",
                        "epoch: 42, iteration: 786, loss (train): 0.0030, loss (valid): 0.2319acc (train): 1.0000, acc (valid): 0.9062\n",
                        "epoch: 43, iteration: 804, loss (train): 0.0722, loss (valid): 0.1405acc (train): 1.0000, acc (valid): 0.9688\n",
                        "epoch: 44, iteration: 823, loss (train): 0.1533, loss (valid): 0.1339acc (train): 1.0000, acc (valid): 0.9688\n",
                        "epoch: 45, iteration: 841, loss (train): 0.0328, loss (valid): 0.1249acc (train): 1.0000, acc (valid): 0.9688\n",
                        "epoch: 46, iteration: 859, loss (train): 0.0268, loss (valid): 0.1045acc (train): 1.0000, acc (valid): 0.9688\n",
                        "epoch: 47, iteration: 877, loss (train): 0.0331, loss (valid): 0.1107acc (train): 1.0000, acc (valid): 0.9688\n",
                        "epoch: 48, iteration: 896, loss (train): 0.0341, loss (valid): 0.1257acc (train): 1.0000, acc (valid): 0.9688\n",
                        "epoch: 49, iteration: 914, loss (train): 0.0463, loss (valid): 0.1255acc (train): 1.0000, acc (valid): 0.9688\n"
                    ]
                }
            ],
            "source": [
                "gpu_id = 0  # 使用する GPU 番号\n",
                "n_batch = 64  # バッチサイズ\n",
                "n_epoch = 50  # エポック数\n",
                "\n",
                "# ネットワークを GPU メモリ上に転送\n",
                "net.to_gpu(gpu_id)\n",
                "\n",
                "# ログ\n",
                "results_train, results_valid = {}, {}\n",
                "results_train['loss'], results_train['accuracy'] = [], []\n",
                "results_valid['loss'], results_valid['accuracy'] = [], []\n",
                "\n",
                "train_iter.reset()  # 上で一度 next() が呼ばれているため\n",
                "\n",
                "count = 1\n",
                "\n",
                "for epoch in range(n_epoch):\n",
                "    \n",
                "    while True:\n",
                "        \n",
                "        # ミニバッチの取得\n",
                "        train_batch = train_iter.next()\n",
                "        \n",
                "        # x と t に分割\n",
                "        # データを GPU に転送するために、concat_examples に gpu_id を渡す\n",
                "        x_train, t_train = chainer.dataset.concat_examples(train_batch, gpu_id)\n",
                "\n",
                "        # 予測値と目的関数の計算\n",
                "        y_train = net(x_train)\n",
                "        loss_train = F.softmax_cross_entropy(y_train, t_train)\n",
                "        acc_train = F.accuracy(y_train, t_train)\n",
                "\n",
                "        # 勾配の初期化と勾配の計算\n",
                "        net.cleargrads()\n",
                "        loss_train.backward()\n",
                "\n",
                "        # パラメータの更新\n",
                "        optimizer.update()\n",
                "    \n",
                "        # カウントアップ\n",
                "        count += 1\n",
                "\n",
                "        # 1エポック終えたら、valid データで評価する\n",
                "        if train_iter.is_new_epoch:\n",
                "\n",
                "            # 検証用データに対する結果の確認\n",
                "            with chainer.using_config('train', False), chainer.using_config('enable_backprop', False):\n",
                "                x_valid, t_valid = chainer.dataset.concat_examples(valid, gpu_id)\n",
                "                y_valid = net(x_valid)\n",
                "                loss_valid = F.softmax_cross_entropy(y_valid, t_valid)\n",
                "                acc_valid = F.accuracy(y_valid, t_valid)\n",
                "            \n",
                "            # 注意：GPU で計算した結果はGPU上に存在するため、CPU上に転送します\n",
                "            loss_train.to_cpu()\n",
                "            loss_valid.to_cpu()\n",
                "            acc_train.to_cpu()\n",
                "            acc_valid.to_cpu()\n",
                "\n",
                "            # 結果の表示\n",
                "            print('epoch: {}, iteration: {}, loss (train): {:.4f}, loss (valid): {:.4f}'\n",
                "                  'acc (train): {:.4f}, acc (valid): {:.4f}'.format(\n",
                "                epoch, count, loss_train.array.mean(), loss_valid.array.mean(),\n",
                "                  acc_train.array.mean(), acc_valid.array.mean()))\n",
                "\n",
                "            # 可視化用に保存\n",
                "            results_train['loss'] .append(loss_train.array)\n",
                "            results_train['accuracy'] .append(acc_train.array)\n",
                "            results_valid['loss'].append(loss_valid.array)\n",
                "            results_valid['accuracy'].append(acc_valid.array)\n",
                "            \n",
                "            break"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "GPU を用いて訓練することができました。\n",
                "\n",
                "前章で CPU を用いて訓練を行ったとき同様に、損失と精度が訓練が進むにつれてどのように変化していったかを確認してみましょう。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "<Figure size 640x480 with 1 Axes>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/plain": [
                            "<Figure size 640x480 with 1 Axes>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "import matplotlib.pyplot as plt\n",
                "\n",
                "# 損失 (loss)\n",
                "plt.plot(results_train['loss'], label='train')  # label で凡例の設定\n",
                "plt.plot(results_valid['loss'], label='valid')  # label で凡例の設定\n",
                "plt.legend()  # 凡例の表示\n",
                "plt.show()\n",
                "\n",
                "# 精度 (accuracy)\n",
                "plt.plot(results_train['accuracy'], label='train')  # label で凡例の設定\n",
                "plt.plot(results_valid['accuracy'], label='valid')  # label で凡例の設定\n",
                "plt.legend()  # 凡例の表示\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "テストデータに対する評価も行ってみましょう。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "test loss: 0.0828\n",
                        "test accuracy: 0.9556\n"
                    ]
                }
            ],
            "source": [
                "# テストデータに対する損失と精度を計算\n",
                "x_test, t_test = chainer.dataset.concat_examples(test, device=gpu_id)\n",
                "with chainer.using_config('train', False), chainer.using_config('enable_backprop', False):\n",
                "    y_test = net(x_test)\n",
                "    loss_test = F.softmax_cross_entropy(y_test, t_test)\n",
                "    acc_test = F.accuracy(y_test, t_test)\n",
                "\n",
                "print('test loss: {:.4f}'.format(loss_test.array.get()))\n",
                "print('test accuracy: {:.4f}'.format(acc_test.array.get()))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### GPU を用いて訓練したネットワークの保存\n",
                "\n",
                "訓練に GPU を用いた場合は、`save_npz()` 関数を使ったネットワーク重みの保存の際に、まずネットワークのパラメータを CPU メモリ上に転送することを忘れないでください。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [],
            "source": [
                "from chainer.serializers import save_npz\n",
                "\n",
                "net.to_cpu()\n",
                "\n",
                "save_npz('net.npz', net)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "正常に保存できているかどうか、読み込んで確認してみます。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "l3/b :\t (3,)\n",
                        "l3/W :\t (3, 100)\n",
                        "l1/b :\t (100,)\n",
                        "l1/W :\t (100, 4)\n",
                        "l2/b :\t (100,)\n",
                        "l2/W :\t (100, 100)\n"
                    ]
                }
            ],
            "source": [
                "import numpy as np\n",
                "\n",
                "params = np.load('net.npz')\n",
                "\n",
                "for key, param in params.items():\n",
                "    print(key, ':\\t', param.shape)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<hr />\n",
                "<span id=\"no_weight_decay_for_bias\"><sup>*1</sup> : <small>\n",
                "    重み減衰をバイアスに適用することは、大抵最終的なネットワークの訓練結果に小さな違いしかもたらしません。（参考：[Backpropagation Algorithm](http://deeplearning.stanford.edu/wiki/index.php/Backpropagation_Algorithm)）\n",
                "    </small></span>"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.6.8"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}